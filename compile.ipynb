{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c7c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "# uv pip install torch==2.10.0 torchvision==0.25.0 triton==3.6.0 --index-url https://download.pytorch.org/whl/cu128\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "def compile_model_once(device=\"cuda\", precision=\"fp16\",\n",
    "                       compile_mode=\"reduce-overhead\", fullgraph=False):\n",
    "\n",
    "    model = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Hybrid\")\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    if precision == \"fp16\":\n",
    "        model = model.half()\n",
    "        \n",
    "    if hasattr(model, 'backbone') and hasattr(model.backbone, 'patch_embed'):\n",
    "        stem_layer = model.backbone.patch_embed.backbone.stem\n",
    "    # This tells the compiler: \"When you hit this function, stop compiling, run it normally, and resume compilation afterward.\"\n",
    "# torch._dynamo.disable(model.backbone.patch_embed.backbone.stem.forward)\n",
    "    #decorate the forward pass of the stem to be skipped by Dynamo\n",
    "        stem_layer.forward = torch._dynamo.disable(stem_layer.forward)\n",
    "        print(\"Optimization disabled for ResNet stem to prevent LoweringException.\")\n",
    "        \n",
    "\n",
    "    compiled_model = torch.compile(\n",
    "        model,\n",
    "        backend=\"aot_eager\",\n",
    "        mode=compile_mode,\n",
    "        dynamic=True\n",
    "    )\n",
    "\n",
    "    return compiled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a386f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_input_dtype(model, input_tensor):\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "    return input_tensor.to(dtype=model_dtype)\n",
    "\n",
    "def preprocess_frame(frame, resolution, device):\n",
    "    frame_resized = cv2.resize(frame, (resolution, resolution))\n",
    "    rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "    rgb = rgb / 255.0\n",
    "\n",
    "    tensor = torch.from_numpy(rgb).permute(2,0,1).float().unsqueeze(0)\n",
    "    return tensor.to(device)\n",
    "def infer_image(compiled_model,\n",
    "                image_path,\n",
    "                transform_fn,\n",
    "                device=\"cuda\",\n",
    "                warmup_runs=5,\n",
    "                measure_runs=30,\n",
    "                output_path=\"depth_output.png\"):\n",
    "\n",
    "    frame = cv2.imread(image_path)\n",
    "    if frame is None:\n",
    "        raise ValueError(\"Invalid image path\")\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # input_tensor = transform_fn(img_rgb).to(device)\n",
    "    input_tensor = preprocess_frame(frame, 384, device)\n",
    "    input_tensor = align_input_dtype(compiled_model, input_tensor)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(warmup_runs):\n",
    "        with torch.no_grad():\n",
    "            _ = compiled_model(input_tensor)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    latencies = []\n",
    "\n",
    "    for _ in range(measure_runs):\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            depth = compiled_model(input_tensor)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "\n",
    "        latencies.append((end - start) * 1000)\n",
    "\n",
    "    latencies = np.array(latencies)\n",
    "\n",
    "    mean_latency = latencies.mean()\n",
    "    fps = 1000.0 / mean_latency\n",
    "    peak_mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    # Save output\n",
    "    depth_np = depth.squeeze().detach().cpu().numpy()\n",
    "    depth_norm = (depth_np - depth_np.min()) / (depth_np.max() - depth_np.min() + 1e-6)\n",
    "    depth_uint8 = (depth_norm * 255).astype(np.uint8)\n",
    "    depth_color = cv2.applyColorMap(depth_uint8, cv2.COLORMAP_INFERNO)\n",
    "\n",
    "    cv2.putText(depth_color,\n",
    "                f\"FPS: {fps:.2f}\",\n",
    "                (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1,\n",
    "                (255, 255, 255),\n",
    "                2)\n",
    "\n",
    "    cv2.imwrite(output_path, depth_color)\n",
    "\n",
    "    print(\"------ IMAGE INFERENCE ------\")\n",
    "    print(f\"Mean Latency: {mean_latency:.2f} ms\")\n",
    "    print(f\"P95 Latency: {np.percentile(latencies,95):.2f} ms\")\n",
    "    print(f\"FPS: {fps:.2f}\")\n",
    "    print(f\"Peak Memory: {peak_mem:.2f} MB\")\n",
    "\n",
    "    return {\n",
    "        \"MeanLatency_ms\": float(mean_latency),\n",
    "        \"FPS\": float(fps),\n",
    "        \"PeakMemory_MB\": float(peak_mem)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67cd7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_to_colormap(depth_tensor):\n",
    "    depth = depth_tensor.squeeze().detach().cpu().numpy()\n",
    "    dmin, dmax = float(depth.min()), float(depth.max())\n",
    "    depth_norm = (depth - dmin) / (dmax - dmin + 1e-6)\n",
    "    depth_uint8 = (depth_norm * 255).astype(np.uint8)\n",
    "    depth_color = cv2.applyColorMap(depth_uint8, cv2.COLORMAP_INFERNO)\n",
    "    return depth_color\n",
    "\n",
    "\n",
    "def run_inference(\n",
    "    compiled_model,\n",
    "    source,\n",
    "    transform_fn,\n",
    "    device=\"cuda\",\n",
    "    warmup_runs=5,\n",
    "    measure_runs=30,\n",
    "    save=False,\n",
    "    output_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs inference on compiled model.\n",
    "\n",
    "    Parameters:\n",
    "        compiled_model : torch.compile wrapped model\n",
    "        source         : image path or video path\n",
    "        transform_fn   : MiDaS transform function\n",
    "        warmup_runs    : warmup iterations\n",
    "        measure_runs   : measured iterations\n",
    "    \"\"\"\n",
    "\n",
    "    assert compiled_model is not None, \"Model must be pre-compiled\"\n",
    "    compiled_model.eval()\n",
    "\n",
    "    is_video = source.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\"))\n",
    "\n",
    "    latencies = []\n",
    "    outputs = []\n",
    "\n",
    "    if is_video:\n",
    "        cap = cv2.VideoCapture(source)\n",
    "        orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        if save:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "            writer = cv2.VideoWriter(\n",
    "                output_path,\n",
    "                fourcc,\n",
    "                orig_fps if orig_fps > 0 else 30,\n",
    "                (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "                 int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "            )\n",
    "    else:\n",
    "        frame = cv2.imread(source)\n",
    "        if frame is None:\n",
    "            raise ValueError(\"Invalid image path\")\n",
    "\n",
    "        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        input_tensor = transform_fn(img_rgb).to(device)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Warmup phase\n",
    "    # -----------------------------\n",
    "    if not is_video:\n",
    "        for _ in range(warmup_runs):\n",
    "            with torch.no_grad():\n",
    "                _ = compiled_model(input_tensor)\n",
    "    else:\n",
    "        count = 0\n",
    "        while count < warmup_runs:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            input_tensor = transform_fn(img_rgb).to(device)\n",
    "            if next(compiled_model.parameters()).dtype == torch.float16:\n",
    "                input_tensor = input_tensor.half()\n",
    "            with torch.no_grad():\n",
    "                _ = compiled_model(input_tensor)\n",
    "            count += 1\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Measurement phase\n",
    "    # -----------------------------\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    if not is_video:\n",
    "        for _ in range(measure_runs):\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = compiled_model(input_tensor)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "\n",
    "            latencies.append((end - start) * 1000)\n",
    "            outputs.append(output.detach().cpu())\n",
    "\n",
    "            if save:\n",
    "                depth_color = depth_to_colormap(output)\n",
    "                fps = 1000.0 / latencies[-1]\n",
    "                depth_color = overlay_fps(depth_color, fps)\n",
    "                cv2.imwrite(output_path, depth_color)\n",
    "\n",
    "    else:\n",
    "        count = 0\n",
    "        while count < measure_runs:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            input_tensor = transform_fn(img_rgb).to(device)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = compiled_model(input_tensor)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "\n",
    "            latency_ms = (end - start) * 1000\n",
    "            latencies.append(latency_ms)\n",
    "\n",
    "            if save:\n",
    "                depth_color = depth_to_colormap(output)\n",
    "                fps = 1000.0 / latency_ms\n",
    "                depth_color = overlay_fps(depth_color, fps)\n",
    "                writer.write(depth_color)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        cap.release()\n",
    "        if save:\n",
    "            writer.release()\n",
    "\n",
    "    latencies = np.array(latencies)\n",
    "\n",
    "    results = {\n",
    "        \"MeanLatency_ms\": float(latencies.mean()),\n",
    "        \"P50_ms\": float(np.percentile(latencies, 50)),\n",
    "        \"P95_ms\": float(np.percentile(latencies, 95)),\n",
    "        \"FPS\": float(1000.0 / latencies.mean()),\n",
    "        \"PeakMemory_MB\": float(torch.cuda.max_memory_allocated() / (1024**2))\n",
    "    }\n",
    "\n",
    "    return results, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3538098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_video(compiled_model,\n",
    "                video_path,\n",
    "                transform_fn,\n",
    "                device=\"cuda\",\n",
    "                warmup_frames=5,\n",
    "                measure_frames=100,\n",
    "                output_path=\"depth_video_output.mp4\"):\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(output_path, fourcc,\n",
    "                             orig_fps if orig_fps > 0 else 30,\n",
    "                             (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    latencies = []\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        input_tensor = transform_fn(img_rgb).to(device)\n",
    "        input_tensor = align_input_dtype(compiled_model, input_tensor)\n",
    "\n",
    "        if frame_count < warmup_frames:\n",
    "            with torch.no_grad():\n",
    "                _ = compiled_model(input_tensor)\n",
    "            frame_count += 1\n",
    "            continue\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            depth = compiled_model(input_tensor)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "\n",
    "        latency_ms = (end - start) * 1000\n",
    "        latencies.append(latency_ms)\n",
    "\n",
    "        # Visualization\n",
    "        depth_np = depth.squeeze().detach().cpu().numpy()\n",
    "        depth_norm = (depth_np - depth_np.min()) / (depth_np.max() - depth_np.min() + 1e-6)\n",
    "        depth_uint8 = (depth_norm * 255).astype(np.uint8)\n",
    "        depth_color = cv2.applyColorMap(depth_uint8, cv2.COLORMAP_INFERNO)\n",
    "\n",
    "        fps = 1000.0 / latency_ms\n",
    "\n",
    "        cv2.putText(depth_color,\n",
    "                    f\"FPS: {fps:.2f}\",\n",
    "                    (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1,\n",
    "                    (255, 255, 255),\n",
    "                    2)\n",
    "\n",
    "        writer.write(depth_color)\n",
    "\n",
    "        frame_count += 1\n",
    "        if frame_count >= warmup_frames + measure_frames:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    latencies = np.array(latencies)\n",
    "    mean_latency = latencies.mean()\n",
    "    fps = 1000.0 / mean_latency\n",
    "    peak_mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    print(\"------ VIDEO INFERENCE ------\")\n",
    "    print(f\"Mean Latency: {mean_latency:.2f} ms\")\n",
    "    print(f\"P95 Latency: {np.percentile(latencies,95):.2f} ms\")\n",
    "    print(f\"FPS: {fps:.2f}\")\n",
    "    print(f\"Peak Memory: {peak_mem:.2f} MB\")\n",
    "\n",
    "    return {\n",
    "        \"MeanLatency_ms\": float(mean_latency),\n",
    "        \"FPS\": float(fps),\n",
    "        \"PeakMemory_MB\": float(peak_mem)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee005d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "Using cache found in /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "W0220 14:58:04.777000 2457723 torch/_dynamo/backends/common.py:53] [3/2_1] aot_autograd-based backend ignoring extra kwargs {'mode': 'reduce-overhead'}\n",
      "W0220 14:58:25.433000 2457723 torch/_dynamo/backends/common.py:53] [4/3] aot_autograd-based backend ignoring extra kwargs {'mode': 'reduce-overhead'}\n",
      "W0220 14:59:00.341000 2457723 torch/_dynamo/backends/common.py:53] [4/3_1] aot_autograd-based backend ignoring extra kwargs {'mode': 'reduce-overhead'}\n",
      "W0220 14:59:35.867000 2457723 torch/_dynamo/backends/common.py:53] [5/0] aot_autograd-based backend ignoring extra kwargs {'mode': 'reduce-overhead'}\n",
      "W0220 14:59:37.023000 2457723 torch/_dynamo/backends/common.py:53] [6/0] aot_autograd-based backend ignoring extra kwargs {'mode': 'reduce-overhead'}\n",
      "W0220 14:59:38.381000 2457723 torch/_dynamo/backends/common.py:53] [7/0] aot_autograd-based backend ignoring extra kwargs {'mode': 'reduce-overhead'}\n",
      "W0220 14:59:59.267000 2457723 torch/_dynamo/backends/common.py:53] [4/4] aot_autograd-based backend ignoring extra kwargs {'mode': 'reduce-overhead'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ IMAGE INFERENCE ------\n",
      "Mean Latency: 25.99 ms\n",
      "P95 Latency: 26.77 ms\n",
      "FPS: 38.48\n",
      "Peak Memory: 1127.28 MB\n"
     ]
    }
   ],
   "source": [
    "compiled_model = compile_model_once(\n",
    "    device=\"cuda\"\n",
    ")\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "results_img = infer_image(\n",
    "    compiled_model,\n",
    "    \"Images/people.jpg\",\n",
    "    midas_transforms.dpt_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62a6b326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ IMAGE INFERENCE ------\n",
      "Mean Latency: 25.86 ms\n",
      "P95 Latency: 28.81 ms\n",
      "FPS: 38.68\n",
      "Peak Memory: 1127.28 MB\n"
     ]
    }
   ],
   "source": [
    "results_img = infer_image(\n",
    "    compiled_model,\n",
    "    \"Images/people.jpg\",\n",
    "    midas_transforms.dpt_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9922e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = compile_model_once(\n",
    "    device=\"cuda\",\n",
    "    precision=\"fp16\"\n",
    ")\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "results_img = infer_image(\n",
    "    compiled_model,\n",
    "    \"Images/people.jpg\",\n",
    "    midas_transforms.dpt_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8592a",
   "metadata": {},
   "source": [
    "## Compilation error due to custom padding logic in MiDas architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973e7d8",
   "metadata": {},
   "source": [
    "```\n",
    "InductorError: LoweringException: AssertionError: \n",
    "  target: aten.convolution.default\n",
    "  args[0]: TensorBox(StorageBox(\n",
    "    ComputedBuffer(name='buf3', layout=FixedLayout('cuda:0', torch.float32, size=[1, 3, s53 + Max(0, -s53 + 2*CeilToInt(IntTrueDiv(s53, 2)) + 5), s0 + Max(0, -s0 + 2*CeilToInt(IntTrueDiv(s0, 2)) + 5)], stride=[3*(s0 + Max(0, -s0 + 2*CeilToInt(IntTrueDiv(s0, 2)) + 5))*(s53 + Max(0, -s53 + 2*CeilToInt(IntTrueDiv(s53, 2)) + 5)), (s0 + Max(0, -s0 + 2*CeilToInt(IntTrueDiv(s0, 2)) + 5))*(s53 + Max(0, -s53 + 2*CeilToInt(IntTrueDiv(s53, 2)) + 5)), s0 + Max(0, -s0 + 2*CeilToInt(IntTrueDiv(s0, 2)) + 5), 1]), data=Pointwise(\n",
    "      'cuda',\n",
    "      torch.float32,\n",
    "      def inner_fn(index):\n",
    "          _, i1, i2, i3 = index\n",
    "          tmp0 = ops.index_expr(i2 - ps0, torch.int64)\n",
    "          tmp1 = ops.index_expr(0, torch.int64)\n",
    "          tmp2 = tmp0 >= tmp1\n",
    "          tmp3 = ops.index_expr(i2 - ps0, torch.int64)\n",
    "          tmp4 = ops.index_expr(s53, torch.int64)\n",
    "          tmp5 = tmp3 < tmp4\n",
    "          tmp6 = ops.index_expr(i3 - ps1, torch.int64)\n",
    "          tmp7 = ops.index_expr(0, torch.int64)\n",
    "          tmp8 = tmp6 >= tmp7\n",
    "          tmp9 = ops.index_expr(i3 - ps1, torch.int64)\n",
    "          tmp10 = ops.index_expr(s0, torch.int64)\n",
    "          tmp11 = tmp9 < tmp10\n",
    "          tmp12 = tmp2 & tmp5\n",
    "          tmp13 = tmp12 & tmp8\n",
    "          tmp14 = tmp13 & tmp11\n",
    "          tmp15 = ops.load(arg2_1, i1 + -3 * ps1 + 3 * i3 + 3 * s0 * (i2 - ps0))\n",
    "          tmp16 = ops.masked(tmp14, tmp15, 0.0)\n",
    "          return tmp16\n",
    "      ,\n",
    "      ranges=[1, 3, s53 + Max(0, -s53 + 2*CeilToInt(IntTrueDiv(s53, 2)) + 5), s0 + Max(0, -s0 + 2*CeilToInt(IntTrueDiv(s0, 2)) + 5)],\n",
    "      origin_node=None,\n",
    "      origins=OrderedSet([convolution, constant_pad_nd, view_5, mul...,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcfc390",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fbb217a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/cuda/graphs.py:81: UserWarning: The CUDA Graph is empty. This usually means that the graph was attempted to be captured on wrong device or stream. (Triggered internally at ../aten/src/ATen/cuda/CUDAGraph.cpp:224.)\n",
      "  super().capture_end()\n",
      "/home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/cuda/graphs.py:81: UserWarning: The CUDA Graph is empty. This usually means that the graph was attempted to be captured on wrong device or stream. (Triggered internally at ../aten/src/ATen/cuda/CUDAGraph.cpp:224.)\n",
      "  super().capture_end()\n",
      "/home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/cuda/graphs.py:81: UserWarning: The CUDA Graph is empty. This usually means that the graph was attempted to be captured on wrong device or stream. (Triggered internally at ../aten/src/ATen/cuda/CUDAGraph.cpp:224.)\n",
      "  super().capture_end()\n",
      "STAGE:2026-02-20 11:16:33 2354873:2354873 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2026-02-20 11:16:33 2354873:2354873 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2026-02-20 11:16:33 2354873:2354873 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "STAGE:2026-02-20 11:16:33 2354873:2354873 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2026-02-20 11:16:34 2354873:2354873 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2026-02-20 11:16:34 2354873:2354873 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiler run complete. Printing summary...\n",
      "--------------------------------------------------\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "fmha_cutlassF_f32_aligned_64x64_rf_sm75(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us      10.265ms         9.11%      10.265ms     244.405us            42  \n",
      "                                          ProfilerStep*         0.45%     525.000us        60.39%      70.089ms      23.363ms       0.000us         0.00%      95.069ms      31.690ms             3  \n",
      "                                               aten::to         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     205.000us         0.18%     205.000us       1.000us           205  \n",
      "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us      27.365ms        24.27%      27.365ms     205.752us           133  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.129ms         1.89%       2.129ms      12.166us           175  \n",
      "void at::native::(anonymous namespace)::vectorized_l...         0.00%       0.000us         0.00%       0.000us       0.000us     854.000us         0.76%     854.000us       9.816us            87  \n",
      "                               TorchDynamo Cache Lookup         0.21%     246.000us         0.21%     246.000us       9.111us       0.000us         0.00%       0.000us       0.000us            27  \n",
      "                                  Torch-Compiled Region        17.27%      20.044ms       227.08%     263.562ms      10.982ms       0.000us         0.00%     325.366ms      13.557ms            24  \n",
      "                                            aten::slice         0.09%     106.000us         0.10%     121.000us       1.754us       0.000us         0.00%       0.000us       0.000us            69  \n",
      "                                       aten::as_strided         0.05%      57.000us         0.05%      57.000us       0.097us       0.000us         0.00%       0.000us       0.000us           585  \n",
      "                                           aten::select         0.17%     192.000us         0.17%     195.000us       1.667us       0.000us         0.00%       0.000us       0.000us           117  \n",
      "                                          aten::reshape         0.53%     614.000us         0.91%       1.051ms       1.134us       0.000us         0.00%       0.000us       0.000us           927  \n",
      "                                             aten::view         0.65%     756.000us         0.65%     756.000us       0.376us       0.000us         0.00%       0.000us       0.000us          2013  \n",
      "                                          aten::permute         0.08%      92.000us         0.08%      92.000us       2.190us       0.000us         0.00%       0.000us       0.000us            42  \n",
      "                              aten::upsample_bilinear2d         0.17%     193.000us         0.29%     338.000us      18.778us       1.842ms         1.63%       1.854ms     103.000us            18  \n",
      "                                            aten::copy_         0.22%     252.000us         0.50%     577.000us      16.028us     440.000us         0.39%     440.000us      12.222us            36  \n",
      "                                        cudaMemcpyAsync         0.19%     226.000us         0.19%     226.000us      12.556us       0.000us         0.00%       0.000us       0.000us            18  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     988.000us         0.88%     988.000us      19.760us            50  \n",
      "                                              aten::cat         0.17%     196.000us         0.25%     292.000us      24.333us     130.000us         0.12%     130.000us      10.833us            12  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 116.065ms\n",
      "Self CUDA time total: 112.730ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESOLUTION = 384\n",
    "WARMUP_FRAMES = 20\n",
    "MEASURE_FRAMES = 200\n",
    "dummy_input = torch.randn(1, 3, RESOLUTION, RESOLUTION).to(DEVICE)\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA, # Only include if CUDA is available\n",
    "    ],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    ") as prof:\n",
    "    for i in range(10):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = align_input_dtype(compiled_model, dummy_input)\n",
    "            compiled_model(dummy_input)\n",
    "        prof.step()\n",
    "print(\"Profiler run complete. Printing summary...\")\n",
    "print(\"-\" * 50)\n",
    "print(prof.key_averages().table(sort_by=\"flops\", row_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe37318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f889735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27ce63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Monocular-Depth-ViT-Optimization (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
