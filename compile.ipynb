{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97c7c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "# uv pip install torch==2.10.0 torchvision==0.25.0 triton==3.6.0 --index-url https://download.pytorch.org/whl/cu128\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "def compile_model_once(model=None, precision=\"fp16\",backend=\"inductor\",\n",
    "                       compile_mode=\"reduce-overhead\", fullgraph=False):\n",
    "\n",
    "    if precision == \"fp16\":\n",
    "        model = model.half()\n",
    "        \n",
    "    if hasattr(model, 'backbone') and hasattr(model.backbone, 'patch_embed'):\n",
    "        stem_layer = model.backbone.patch_embed.backbone.stem\n",
    "    # This tells the compiler: \"When you hit this function, stop compiling, run it normally, and resume compilation afterward.\"\n",
    "# torch._dynamo.disable(model.backbone.patch_embed.backbone.stem.forward)\n",
    "    #decorate the forward pass of the stem to be skipped by Dynamo\n",
    "        stem_layer.forward = torch._dynamo.disable(stem_layer.forward)\n",
    "        print(\"Optimization disabled for ResNet stem to prevent LoweringException.\")\n",
    "        \n",
    "\n",
    "    compiled_model = torch.compile(\n",
    "        model,\n",
    "        backend=backend, #aot_eager\n",
    "        mode=compile_mode,\n",
    "        dynamic=False\n",
    "    )\n",
    "\n",
    "    return compiled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a386f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_input_dtype(model, input_tensor):\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "    return input_tensor.to(dtype=model_dtype)\n",
    "\n",
    "def preprocess_frame(frame, resolution, device):\n",
    "    frame_resized = cv2.resize(frame, (resolution, resolution))\n",
    "    rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "    rgb = rgb / 255.0\n",
    "\n",
    "    tensor = torch.from_numpy(rgb).permute(2,0,1).float().unsqueeze(0)\n",
    "    return tensor.to(device)\n",
    "def infer_image(compiled_model,\n",
    "                image_path,\n",
    "                transform_fn,\n",
    "                device=\"cuda\",\n",
    "                warmup_runs=5,\n",
    "                measure_runs=30,\n",
    "                output_path=\"depth_output.png\"):\n",
    "\n",
    "    frame = cv2.imread(image_path)\n",
    "    if frame is None:\n",
    "        raise ValueError(\"Invalid image path\")\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # input_tensor = transform_fn(img_rgb).to(device)\n",
    "    input_tensor = preprocess_frame(frame, 384, device)\n",
    "    input_tensor = align_input_dtype(compiled_model, input_tensor)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(warmup_runs):\n",
    "        with torch.no_grad():\n",
    "            _ = compiled_model(input_tensor)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    latencies = []\n",
    "\n",
    "    for _ in range(measure_runs):\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            depth = compiled_model(input_tensor)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "\n",
    "        latencies.append((end - start) * 1000)\n",
    "\n",
    "    latencies = np.array(latencies)\n",
    "\n",
    "    mean_latency = latencies.mean()\n",
    "    fps = 1000.0 / mean_latency\n",
    "    peak_mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    # Save output\n",
    "    depth_np = depth.squeeze().detach().cpu().numpy()\n",
    "    depth_norm = (depth_np - depth_np.min()) / (depth_np.max() - depth_np.min() + 1e-6)\n",
    "    depth_uint8 = (depth_norm * 255).astype(np.uint8)\n",
    "    depth_color = cv2.applyColorMap(depth_uint8, cv2.COLORMAP_INFERNO)\n",
    "\n",
    "    cv2.putText(depth_color,\n",
    "                f\"FPS: {fps:.2f}\",\n",
    "                (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1,\n",
    "                (255, 255, 255),\n",
    "                2)\n",
    "\n",
    "    cv2.imwrite(output_path, depth_color)\n",
    "\n",
    "    print(\"------ IMAGE INFERENCE ------\")\n",
    "    print(f\"Mean Latency: {mean_latency:.2f} ms\")\n",
    "    print(f\"P95 Latency: {np.percentile(latencies,95):.2f} ms\")\n",
    "    print(f\"FPS: {fps:.2f}\")\n",
    "    print(f\"Peak Memory: {peak_mem:.2f} MB\")\n",
    "\n",
    "    return {\n",
    "        \"MeanLatency_ms\": float(mean_latency),\n",
    "        \"FPS\": float(fps),\n",
    "        \"PeakMemory_MB\": float(peak_mem)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67cd7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_to_colormap(depth_tensor):\n",
    "    depth = depth_tensor.squeeze().detach().cpu().numpy()\n",
    "    dmin, dmax = float(depth.min()), float(depth.max())\n",
    "    depth_norm = (depth - dmin) / (dmax - dmin + 1e-6)\n",
    "    depth_uint8 = (depth_norm * 255).astype(np.uint8)\n",
    "    depth_color = cv2.applyColorMap(depth_uint8, cv2.COLORMAP_INFERNO)\n",
    "    return depth_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3538098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_video(compiled_model,\n",
    "                video_path,\n",
    "                transform_fn,\n",
    "                device=\"cuda\",\n",
    "                warmup_frames=5,\n",
    "                measure_frames=100,\n",
    "                output_path=\"depth_video_output.mp4\"):\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(output_path, fourcc,\n",
    "                             orig_fps if orig_fps > 0 else 30,\n",
    "                             (384, 384))\n",
    "\n",
    "    frame_count = 0\n",
    "    latencies = []\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.resize(frame, (384, 384))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = frame / 255.0\n",
    "        frame = torch.from_numpy(frame).permute(2,0,1).float().unsqueeze(0).to(device)\n",
    "        if next(compiled_model.parameters()).dtype == torch.float16:\n",
    "                frame = frame.half()\n",
    "        if frame_count < warmup_frames:\n",
    "            with torch.no_grad():\n",
    "                _ = compiled_model(frame)\n",
    "            frame_count += 1\n",
    "            continue\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            depth = compiled_model(frame)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "\n",
    "        latency_ms = (end - start) * 1000\n",
    "        latencies.append(latency_ms)\n",
    "\n",
    "        # Visualization\n",
    "        depth_np = depth.squeeze().detach().cpu().numpy()\n",
    "        depth_norm = (depth_np - depth_np.min()) / (depth_np.max() - depth_np.min() + 1e-6)\n",
    "        depth_uint8 = (depth_norm * 255).astype(np.uint8)\n",
    "        depth_color = cv2.applyColorMap(depth_uint8, cv2.COLORMAP_INFERNO)\n",
    "\n",
    "        fps = 1000.0 / latency_ms\n",
    "\n",
    "        cv2.putText(depth_color,\n",
    "                    f\"FPS: {fps:.2f}\",\n",
    "                    (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1,\n",
    "                    (255, 255, 255),\n",
    "                    2)\n",
    "\n",
    "        writer.write(depth_color)\n",
    "\n",
    "        frame_count += 1\n",
    "        if frame_count >= warmup_frames + measure_frames:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    latencies = np.array(latencies)\n",
    "    mean_latency = latencies.mean()\n",
    "    fps = 1000.0 / mean_latency\n",
    "    peak_mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    print(\"------ VIDEO INFERENCE ------\")\n",
    "    print(f\"Mean Latency: {mean_latency:.2f} ms\")\n",
    "    print(f\"P95 Latency: {np.percentile(latencies,95):.2f} ms\")\n",
    "    print(f\"FPS: {fps:.2f}\")\n",
    "    print(f\"Peak Memory: {peak_mem:.2f} MB\")\n",
    "\n",
    "    return {\n",
    "        \"MeanLatency_ms\": float(mean_latency),\n",
    "        \"FPS\": float(fps),\n",
    "        \"PeakMemory_MB\": float(peak_mem)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9922e55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "Using cache found in /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ IMAGE INFERENCE ------\n",
      "Mean Latency: 9.31 ms\n",
      "P95 Latency: 9.65 ms\n",
      "FPS: 107.41\n",
      "Peak Memory: 269.17 MB\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Hybrid\")\n",
    "model = model.to(\"cuda\").eval()\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "compiled_model = compile_model_once(\n",
    "    model = model,\n",
    "    precision=\"fp16\",\n",
    "    backend=\"inductor\", #eager, aot-eager\n",
    "    compile_mode = \"reduce-overhead\" #max-autotune\n",
    ")\n",
    "\n",
    "results_img = infer_image(\n",
    "    compiled_model,\n",
    "    \"Images/people.jpg\",\n",
    "    midas_transforms.dpt_transform,\n",
    "    output_path=\"depth_compile_f16.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c96432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ VIDEO INFERENCE ------\n",
      "Mean Latency: 10.05 ms\n",
      "P95 Latency: 10.41 ms\n",
      "FPS: 99.48\n",
      "Peak Memory: 919.28 MB\n"
     ]
    }
   ],
   "source": [
    "# results_img = infer_video(\n",
    "#     compiled_model,\n",
    "#     \"/home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/test.mov\",\n",
    "#     midas_transforms.dpt_transform,\n",
    "#     output_path=\"depth_compile_f16.avi\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8592a",
   "metadata": {},
   "source": [
    "## Compilation error due to custom padding logic in MiDas architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973e7d8",
   "metadata": {},
   "source": [
    "```\n",
    "InductorError: LoweringException: AssertionError: \n",
    "  target: aten.convolution.default\n",
    "  args[0]: TensorBox(StorageBox(\n",
    "    ComputedBuffer(name='buf3', layout=FixedLayout('cuda:0', torch.float32, size=[1, 3, s53 + Max(0, -s53 + 2*CeilToInt(IntTrueDiv(s53, 2)) + 5), s0 + Max(0, -s0 + 2*CeilToInt(IntTrueDiv(s0, 2)) + 5)], stride=[3*(s0 + Max(0, -s0 + 2*CeilToInt(IntTrueDiv(s0, 2)) + 5))*(s53 + Max(0, -s53 + 2*CeilToInt(IntTrueDiv(s53, 2)) + 5)), (s0 + Max(0, -s0 + 2*CeilToInt(IntTrueDiv(s0, 2)) + 5))*(s53 + Max(0, -s53 + 2*CeilToInt(IntTrueDiv(s53, 2)) + 5)), s0 + Max(0, -s0 + 2*CeilToInt(IntTrueDiv(s0, 2)) + 5), 1]), data=Pointwise(\n",
    "      'cuda',\n",
    "      torch.float32,\n",
    "      def inner_fn(index):\n",
    "          _, i1, i2, i3 = index\n",
    "          tmp0 = ops.index_expr(i2 - ps0, torch.int64)\n",
    "          tmp1 = ops.index_expr(0, torch.int64)\n",
    "          tmp2 = tmp0 >= tmp1\n",
    "          tmp3 = ops.index_expr(i2 - ps0, torch.int64)\n",
    "          tmp4 = ops.index_expr(s53, torch.int64)\n",
    "          tmp5 = tmp3 < tmp4\n",
    "          tmp6 = ops.index_expr(i3 - ps1, torch.int64)\n",
    "          tmp7 = ops.index_expr(0, torch.int64)\n",
    "          tmp8 = tmp6 >= tmp7\n",
    "          tmp9 = ops.index_expr(i3 - ps1, torch.int64)\n",
    "          tmp10 = ops.index_expr(s0, torch.int64)\n",
    "          tmp11 = tmp9 < tmp10\n",
    "          tmp12 = tmp2 & tmp5\n",
    "          tmp13 = tmp12 & tmp8\n",
    "          tmp14 = tmp13 & tmp11\n",
    "          tmp15 = ops.load(arg2_1, i1 + -3 * ps1 + 3 * i3 + 3 * s0 * (i2 - ps0))\n",
    "          tmp16 = ops.masked(tmp14, tmp15, 0.0)\n",
    "          return tmp16\n",
    "      ,\n",
    "      ranges=[1, 3, s53 + Max(0, -s53 + 2*CeilToInt(IntTrueDiv(s53, 2)) + 5), s0 + Max(0, -s0 + 2*CeilToInt(IntTrueDiv(s0, 2)) + 5)],\n",
    "      origin_node=None,\n",
    "      origins=OrderedSet([convolution, constant_pad_nd, view_5, mul...,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcfc390",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fbb217a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiler run complete. Printing summary...\n",
      "--------------------------------------------------\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               aten::to         0.04%       4.709us        19.22%       2.555ms       2.555ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                         aten::_to_copy         0.12%      16.611us        19.18%       2.550ms       2.550ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                    aten::empty_strided         0.19%      24.656us         0.19%      24.656us      24.656us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                            aten::copy_         0.29%      39.024us        18.87%       2.509ms       2.509ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                Activity Buffer Request        15.86%       2.108ms        15.86%       2.108ms       2.108ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       cudaLaunchKernel         2.95%     392.158us         2.95%     392.158us     196.079us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                               TorchDynamo Cache Lookup         6.22%     827.220us         6.22%     827.220us      91.913us       0.000us         0.00%       0.000us       0.000us             9  \n",
      "                             Torch-Compiled Region: 0/3         0.23%      31.119us        30.59%       4.067ms       4.067ms       0.000us         0.00%       7.492ms       7.492ms             1  \n",
      "                             Torch-Compiled Region: 1/3         0.20%      27.052us        29.01%       3.857ms       3.857ms       0.000us         0.00%       7.492ms       7.492ms             1  \n",
      "                             Torch-Compiled Region: 2/3         0.07%       9.487us        23.98%       3.188ms       3.188ms       0.000us         0.00%       4.845ms       4.845ms             1  \n",
      "                             Torch-Compiled Region: 3/3         3.32%     441.020us        23.85%       3.170ms       3.170ms       0.000us         0.00%       4.845ms       4.845ms             1  \n",
      "                             Torch-Compiled Region: 4/4         0.83%     109.836us        13.60%       1.808ms       1.808ms       4.562ms        60.89%       4.562ms       4.562ms             1  \n",
      "                                      Pregraph bytecode         0.91%     121.629us         0.91%     121.629us      30.407us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                 AOTDispatcher Runtime Wrapper Prologue         0.24%      31.318us         0.24%      31.318us       7.830us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "## Call CompiledFxGraph f44qjcsmxtrjesnygsobp7jlvtrk...         3.22%     427.806us        11.97%       1.591ms       1.591ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                   aten::_foreach_copy_         0.25%      32.883us         0.48%      63.570us      63.570us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaStreamIsCapturing         0.05%       6.701us         0.05%       6.701us       2.234us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                        cudaGraphLaunch        10.91%       1.451ms        10.91%       1.451ms     483.544us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                   cudaDriverGetVersion         0.01%       0.740us         0.01%       0.740us       0.247us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                             Torch-Compiled Region: 5/1         0.45%      60.304us         1.96%     259.899us     259.899us     283.135us         3.78%     283.135us     283.135us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 13.293ms\n",
      "Self CUDA time total: 7.492ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESOLUTION = 384\n",
    "WARMUP_FRAMES = 20\n",
    "MEASURE_FRAMES = 200\n",
    "dummy_input = torch.randn(1, 3, RESOLUTION, RESOLUTION).to(DEVICE)\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA, # Only include if CUDA is available\n",
    "    ],\n",
    "    # schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    ") as prof:\n",
    "    # for i in range(10):\n",
    "    with torch.no_grad():\n",
    "        dummy_input = align_input_dtype(compiled_model, dummy_input)\n",
    "        compiled_model(dummy_input)\n",
    "    prof.step()\n",
    "print(\"Profiler run complete. Printing summary...\")\n",
    "print(\"-\" * 50)\n",
    "print(prof.key_averages().table(sort_by=\"flops\", row_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfe37318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0222 00:39:40.540000 3501339 torch/onnx/_internal/exporter/_compat.py:125] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W0222 00:39:41.149000 3501339 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
      "W0222 00:39:41.150000 3501339 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
      "W0222 00:39:41.150000 3501339 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n",
      "W0222 00:39:41.151000 3501339 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `DPTDepthModel([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `DPTDepthModel([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Translate the graph into ONNX... ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to convert the model to the target version 17 using the ONNX C API. The model was not modified\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/onnxscript/version_converter/__init__.py\", line 120, in call\n",
      "    converted_proto = _c_api_utils.call_onnx_api(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
      "    result = func(proto)\n",
      "             ^^^^^^^^^^^\n",
      "  File \"/home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/onnxscript/version_converter/__init__.py\", line 115, in _partial_convert_version\n",
      "    return onnx.version_converter.convert_version(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/onnx/version_converter.py\", line 39, in convert_version\n",
      "    converted_model_str = C.convert_version(model_str, target_version)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: /github/workspace/onnx/version_converter/BaseConverter.h:65: adapter_lookup: Assertion `false` failed: No Adapter To Version $17 for Resize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 95 of general pattern rewrite rules.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 18},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.10.0+cu128',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"input\"<FLOAT16,[batch,3,384,384]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"depth\"<FLOAT16,[1,384,384]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"pretrained.model.cls_token\"<FLOAT16,[1,1,768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.proj.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.0.norm1.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.0.norm1.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.0.attn.proj.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.0.norm2.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.0.norm2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.0.mlp.fc2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.1.norm1.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.1.norm1.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.1.attn.proj.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.1.norm2.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.1.norm2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.1.mlp.fc2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.2.norm1.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.2.norm1.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.2.attn.proj.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.2.norm2.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.2.norm2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.2.mlp.fc2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.3.norm1.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.3.norm1.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.3.attn.proj.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.3.norm2.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.3.norm2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.3.mlp.fc2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.4.norm1.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.4.norm1.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.4.attn.proj.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.4.norm2.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.4.norm2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.4.mlp.fc2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.5.norm1.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.5.norm1.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.5.attn.proj.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.5.norm2.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.5.norm2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.5.mlp.fc2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.6.norm1.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.6.norm1.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.6.attn.proj.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.6.norm2.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.6.norm2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.6.mlp.fc2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.7.norm1.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.7.norm1.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.7.attn.proj.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.7.norm2.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.7.norm2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.7.mlp.fc2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.8.norm1.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.8.norm1.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.8.attn.proj.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.8.norm2.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.8.norm2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.8.mlp.fc2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.9.norm1.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.9.norm1.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.9.attn.proj.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.9.norm2.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.9.norm2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.9.mlp.fc2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.10.norm1.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.10.norm1.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.10.attn.proj.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.10.norm2.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.10.norm2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.10.mlp.fc2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.11.norm1.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.11.norm1.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.11.attn.proj.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.11.norm2.weight\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.11.norm2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.11.mlp.fc2.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.act_postprocess3.0.project.0.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.act_postprocess3.3.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.act_postprocess4.0.project.0.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.act_postprocess4.3.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"pretrained.act_postprocess4.4.bias\"<FLOAT16,[768]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet1.out_conv.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet1.resConfUnit1.conv1.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet1.resConfUnit1.conv2.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet1.resConfUnit2.conv1.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet1.resConfUnit2.conv2.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet2.out_conv.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet2.resConfUnit1.conv1.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet2.resConfUnit1.conv2.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet2.resConfUnit2.conv1.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet2.resConfUnit2.conv2.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet3.out_conv.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet3.resConfUnit1.conv1.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet3.resConfUnit1.conv2.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet3.resConfUnit2.conv1.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet3.resConfUnit2.conv2.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet4.out_conv.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet4.resConfUnit2.conv1.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet4.resConfUnit2.conv2.bias\"<FLOAT16,[256]>{TorchTensor(...)},\n",
       "                %\"scratch.output_conv.0.bias\"<FLOAT16,[128]>{TorchTensor(...)},\n",
       "                %\"scratch.output_conv.2.bias\"<FLOAT16,[32]>{TorchTensor(...)},\n",
       "                %\"scratch.output_conv.4.weight\"<FLOAT16,[1,32,1,1]>{TorchTensor(...)},\n",
       "                %\"scratch.output_conv.4.bias\"<FLOAT16,[1]>{TorchTensor<FLOAT16,[1]>(Parameter containing: tensor([0.4929], device='cuda:0', dtype=torch.float16, requires_grad=True), name='scratch.output_conv.4.bias')},\n",
       "                %\"pretrained.model.pos_embed\"<FLOAT16,[1,577,768]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stem.conv.weight\"<FLOAT16,[64,3,7,7]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.0.blocks.0.downsample.conv.weight\"<FLOAT16,[256,64,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.0.blocks.0.conv2.weight\"<FLOAT16,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.0.blocks.0.conv3.weight\"<FLOAT16,[256,64,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.0.blocks.1.conv1.weight\"<FLOAT16,[64,256,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.0.blocks.1.conv2.weight\"<FLOAT16,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.0.blocks.1.conv3.weight\"<FLOAT16,[256,64,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.0.blocks.2.conv1.weight\"<FLOAT16,[64,256,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.0.blocks.2.conv2.weight\"<FLOAT16,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.0.blocks.2.conv3.weight\"<FLOAT16,[256,64,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.1.blocks.0.downsample.conv.weight\"<FLOAT16,[512,256,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.1.blocks.0.conv1.weight\"<FLOAT16,[128,256,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.1.blocks.0.conv2.weight\"<FLOAT16,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.1.blocks.0.conv3.weight\"<FLOAT16,[512,128,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.1.blocks.1.conv1.weight\"<FLOAT16,[128,512,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.1.blocks.1.conv2.weight\"<FLOAT16,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.1.blocks.1.conv3.weight\"<FLOAT16,[512,128,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.1.blocks.2.conv1.weight\"<FLOAT16,[128,512,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.1.blocks.2.conv2.weight\"<FLOAT16,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.1.blocks.2.conv3.weight\"<FLOAT16,[512,128,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.1.blocks.3.conv1.weight\"<FLOAT16,[128,512,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.1.blocks.3.conv2.weight\"<FLOAT16,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.1.blocks.3.conv3.weight\"<FLOAT16,[512,128,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.0.downsample.conv.weight\"<FLOAT16,[1024,512,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.0.conv1.weight\"<FLOAT16,[256,512,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.0.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.0.conv3.weight\"<FLOAT16,[1024,256,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.1.conv1.weight\"<FLOAT16,[256,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.1.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.1.conv3.weight\"<FLOAT16,[1024,256,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.2.conv1.weight\"<FLOAT16,[256,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.2.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.2.conv3.weight\"<FLOAT16,[1024,256,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.3.conv1.weight\"<FLOAT16,[256,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.3.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.3.conv3.weight\"<FLOAT16,[1024,256,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.4.conv1.weight\"<FLOAT16,[256,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.4.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.4.conv3.weight\"<FLOAT16,[1024,256,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.5.conv1.weight\"<FLOAT16,[256,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.5.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.5.conv3.weight\"<FLOAT16,[1024,256,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.6.conv1.weight\"<FLOAT16,[256,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.6.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.6.conv3.weight\"<FLOAT16,[1024,256,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.7.conv1.weight\"<FLOAT16,[256,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.7.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.7.conv3.weight\"<FLOAT16,[1024,256,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.8.conv1.weight\"<FLOAT16,[256,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.8.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.backbone.stages.2.blocks.8.conv3.weight\"<FLOAT16,[1024,256,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.patch_embed.proj.weight\"<FLOAT16,[768,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.0.attn.qkv.bias\"<FLOAT16,[2304]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.0.mlp.fc1.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.1.attn.qkv.bias\"<FLOAT16,[2304]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.1.mlp.fc1.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.2.attn.qkv.bias\"<FLOAT16,[2304]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.2.mlp.fc1.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.3.attn.qkv.bias\"<FLOAT16,[2304]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.3.mlp.fc1.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.4.attn.qkv.bias\"<FLOAT16,[2304]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.4.mlp.fc1.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.5.attn.qkv.bias\"<FLOAT16,[2304]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.5.mlp.fc1.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.6.attn.qkv.bias\"<FLOAT16,[2304]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.6.mlp.fc1.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.7.attn.qkv.bias\"<FLOAT16,[2304]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.7.mlp.fc1.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.8.attn.qkv.bias\"<FLOAT16,[2304]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.8.mlp.fc1.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.9.attn.qkv.bias\"<FLOAT16,[2304]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.9.mlp.fc1.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.10.attn.qkv.bias\"<FLOAT16,[2304]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.10.mlp.fc1.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.11.attn.qkv.bias\"<FLOAT16,[2304]>{TorchTensor(...)},\n",
       "                %\"pretrained.model.blocks.11.mlp.fc1.bias\"<FLOAT16,[3072]>{TorchTensor(...)},\n",
       "                %\"pretrained.act_postprocess3.3.weight\"<FLOAT16,[768,768,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.act_postprocess4.3.weight\"<FLOAT16,[768,768,1,1]>{TorchTensor(...)},\n",
       "                %\"pretrained.act_postprocess4.4.weight\"<FLOAT16,[768,768,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.layer1_rn.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.layer2_rn.weight\"<FLOAT16,[256,512,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.layer3_rn.weight\"<FLOAT16,[256,768,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.layer4_rn.weight\"<FLOAT16,[256,768,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet1.out_conv.weight\"<FLOAT16,[256,256,1,1]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet1.resConfUnit1.conv1.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet1.resConfUnit1.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet1.resConfUnit2.conv1.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet1.resConfUnit2.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet2.out_conv.weight\"<FLOAT16,[256,256,1,1]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet2.resConfUnit1.conv1.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet2.resConfUnit1.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet2.resConfUnit2.conv1.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet2.resConfUnit2.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet3.out_conv.weight\"<FLOAT16,[256,256,1,1]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet3.resConfUnit1.conv1.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet3.resConfUnit1.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet3.resConfUnit2.conv1.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet3.resConfUnit2.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet4.out_conv.weight\"<FLOAT16,[256,256,1,1]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet4.resConfUnit2.conv1.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.refinenet4.resConfUnit2.conv2.weight\"<FLOAT16,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.output_conv.0.weight\"<FLOAT16,[128,256,3,3]>{TorchTensor(...)},\n",
       "                %\"scratch.output_conv.2.weight\"<FLOAT16,[32,128,3,3]>{TorchTensor(...)},\n",
       "                %\"val_5\"<INT64,[1]>{Tensor<INT64,[1]>(array([0]), name='val_5')},\n",
       "                %\"val_8\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_8')},\n",
       "                %\"val_19\"<INT64,[1]>{Tensor<INT64,[1]>(array([9223372036854775807]), name='val_19')},\n",
       "                %\"val_29\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 1, 24, 24, -1]), name='val_29')},\n",
       "                %\"val_33\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 768,  24,  24]), name='val_33')},\n",
       "                %\"val_38\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 576,  -1]), name='val_38')},\n",
       "                %\"val_45\"<INT64,[3]>{Tensor<INT64,[3]>(array([ 1, 64, -1]), name='val_45')},\n",
       "                %\"val_49\"<FLOAT16,[64]>{Tensor(...)},\n",
       "                %\"val_53\"<FLOAT16,[64]>{Tensor(...)},\n",
       "                %\"val_72\"<INT64,[4]>{Tensor<INT64,[4]>(array([64,  3,  7,  7]), name='val_72')},\n",
       "                %\"val_83\"<INT64,[3]>{Tensor<INT64,[3]>(array([ 0, 32, -1]), name='val_83')},\n",
       "                %\"val_87\"<FLOAT16,[32]>{Tensor(...)},\n",
       "                %\"val_90\"<FLOAT16,[32]>{Tensor(...)},\n",
       "                %\"val_100\"<FLOAT16,[64,1,1]>{Tensor(...)},\n",
       "                %\"val_102\"<FLOAT16,[64,1,1]>{Tensor(...)},\n",
       "                %\"val_110\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 256,  -1]), name='val_110')},\n",
       "                %\"val_114\"<FLOAT16,[256]>{Tensor(...)},\n",
       "                %\"val_118\"<FLOAT16,[256]>{Tensor(...)},\n",
       "                %\"val_135\"<INT64,[4]>{Tensor<INT64,[4]>(array([256,  64,   1,   1]), name='val_135')},\n",
       "                %\"val_160\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_162\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"view_7\"<FLOAT16,[64,64,1,1]>{Tensor(...)},\n",
       "                %\"val_217\"<FLOAT16,[64,1,1]>{Tensor(...)},\n",
       "                %\"val_219\"<FLOAT16,[64,1,1]>{Tensor(...)},\n",
       "                %\"val_249\"<INT64,[4]>{Tensor<INT64,[4]>(array([64, 64,  3,  3]), name='val_249')},\n",
       "                %\"val_274\"<FLOAT16,[64,1,1]>{Tensor(...)},\n",
       "                %\"val_276\"<FLOAT16,[64,1,1]>{Tensor(...)},\n",
       "                %\"val_331\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_363\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 64, 256,   1,   1]), name='val_363')},\n",
       "                %\"val_388\"<FLOAT16,[64,1,1]>{Tensor(...)},\n",
       "                %\"val_390\"<FLOAT16,[64,1,1]>{Tensor(...)},\n",
       "                %\"val_445\"<FLOAT16,[64,1,1]>{Tensor(...)},\n",
       "                %\"val_447\"<FLOAT16,[64,1,1]>{Tensor(...)},\n",
       "                %\"val_502\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_504\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_559\"<FLOAT16,[64,1,1]>{Tensor(...)},\n",
       "                %\"val_561\"<FLOAT16,[64,1,1]>{Tensor(...)},\n",
       "                %\"val_616\"<FLOAT16,[64,1,1]>{Tensor(...)},\n",
       "                %\"val_618\"<FLOAT16,[64,1,1]>{Tensor(...)},\n",
       "                %\"val_673\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_675\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_682\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 512,  -1]), name='val_682')},\n",
       "                %\"val_686\"<FLOAT16,[512]>{Tensor(...)},\n",
       "                %\"val_690\"<FLOAT16,[512]>{Tensor(...)},\n",
       "                %\"val_707\"<INT64,[4]>{Tensor<INT64,[4]>(array([512, 256,   1,   1]), name='val_707')},\n",
       "                %\"val_732\"<FLOAT16,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_734\"<FLOAT16,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_739\"<INT64,[3]>{Tensor<INT64,[3]>(array([  1, 128,  -1]), name='val_739')},\n",
       "                %\"val_743\"<FLOAT16,[128]>{Tensor(...)},\n",
       "                %\"val_747\"<FLOAT16,[128]>{Tensor(...)},\n",
       "                %\"val_764\"<INT64,[4]>{Tensor<INT64,[4]>(array([128, 256,   1,   1]), name='val_764')},\n",
       "                %\"val_789\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_791\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_823\"<INT64,[4]>{Tensor<INT64,[4]>(array([128, 128,   3,   3]), name='val_823')},\n",
       "                %\"val_848\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_850\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_880\"<INT64,[4]>{Tensor<INT64,[4]>(array([512, 128,   1,   1]), name='val_880')},\n",
       "                %\"val_905\"<FLOAT16,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_907\"<FLOAT16,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_937\"<INT64,[4]>{Tensor<INT64,[4]>(array([128, 512,   1,   1]), name='val_937')},\n",
       "                %\"val_962\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_964\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_1019\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_1021\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_1076\"<FLOAT16,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_1078\"<FLOAT16,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_1133\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_1135\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_1190\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_1192\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_1247\"<FLOAT16,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_1249\"<FLOAT16,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_1304\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_1306\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_1361\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_1363\"<FLOAT16,[128,1,1]>{Tensor(...)},\n",
       "                %\"val_1418\"<FLOAT16,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_1420\"<FLOAT16,[512,1,1]>{Tensor(...)},\n",
       "                %\"val_1427\"<INT64,[3]>{Tensor<INT64,[3]>(array([   1, 1024,   -1]), name='val_1427')},\n",
       "                %\"val_1431\"<FLOAT16,[1024]>{Tensor(...)},\n",
       "                %\"val_1435\"<FLOAT16,[1024]>{Tensor(...)},\n",
       "                %\"val_1452\"<INT64,[4]>{Tensor<INT64,[4]>(array([1024,  512,    1,    1]), name='val_1452')},\n",
       "                %\"val_1477\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_1479\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_1509\"<INT64,[4]>{Tensor<INT64,[4]>(array([256, 512,   1,   1]), name='val_1509')},\n",
       "                %\"val_1534\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_1536\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_1568\"<INT64,[4]>{Tensor<INT64,[4]>(array([256, 256,   3,   3]), name='val_1568')},\n",
       "                %\"val_1593\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_1595\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_1625\"<INT64,[4]>{Tensor<INT64,[4]>(array([1024,  256,    1,    1]), name='val_1625')},\n",
       "                %\"val_1650\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_1652\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_1682\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 256, 1024,    1,    1]), name='val_1682')},\n",
       "                %\"val_1707\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_1709\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_1764\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_1766\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_1821\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_1823\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_1878\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_1880\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_1935\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_1937\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_1992\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_1994\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_2049\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2051\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2106\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2108\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2163\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_2165\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_2220\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2222\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2277\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2279\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2334\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_2336\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_2391\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2393\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2448\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2450\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2505\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_2507\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_2562\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2564\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2619\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2621\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2676\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_2678\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_2733\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2735\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2790\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2792\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2847\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_2849\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_2904\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2906\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2961\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_2963\"<FLOAT16,[256,1,1]>{Tensor(...)},\n",
       "                %\"val_3018\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_3020\"<FLOAT16,[1024,1,1]>{Tensor(...)},\n",
       "                %\"val_3033\"<FLOAT16,[768,2304]>{Tensor(...)},\n",
       "                %\"val_3078\"<FLOAT16,[1]>{Tensor<FLOAT16,[1]>(array([0.3535], dtype=float16), name='val_3078')},\n",
       "                %\"val_3090\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_3094\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_3103\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_3107\"<FLOAT16,[768,2304]>{Tensor(...)},\n",
       "                %\"val_3161\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_3165\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_3174\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_3178\"<FLOAT16,[768,2304]>{Tensor(...)},\n",
       "                %\"val_3232\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_3236\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_3245\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_3249\"<FLOAT16,[768,2304]>{Tensor(...)},\n",
       "                %\"val_3303\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_3307\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_3316\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_3320\"<FLOAT16,[768,2304]>{Tensor(...)},\n",
       "                %\"val_3374\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_3378\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_3387\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_3391\"<FLOAT16,[768,2304]>{Tensor(...)},\n",
       "                %\"val_3445\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_3449\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_3458\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_3462\"<FLOAT16,[768,2304]>{Tensor(...)},\n",
       "                %\"val_3516\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_3520\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_3529\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_3533\"<FLOAT16,[768,2304]>{Tensor(...)},\n",
       "                %\"val_3587\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_3591\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_3600\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_3604\"<FLOAT16,[768,2304]>{Tensor(...)},\n",
       "                %\"val_3658\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_3662\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_3671\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_3675\"<FLOAT16,[768,2304]>{Tensor(...)},\n",
       "                %\"val_3729\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_3733\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_3742\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_3746\"<FLOAT16,[768,2304]>{Tensor(...)},\n",
       "                %\"val_3800\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_3804\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_3813\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_3817\"<FLOAT16,[768,2304]>{Tensor(...)},\n",
       "                %\"val_3871\"<FLOAT16,[768,768]>{Tensor(...)},\n",
       "                %\"val_3875\"<FLOAT16,[768,3072]>{Tensor(...)},\n",
       "                %\"val_3884\"<FLOAT16,[3072,768]>{Tensor(...)},\n",
       "                %\"val_3932\"<FLOAT16,[1536,768]>{Tensor(...)},\n",
       "                %\"val_3986\"<FLOAT16,[1536,768]>{Tensor(...)},\n",
       "                %\"val_4028\"<INT64,[2]>{Tensor<INT64,[2]>(array([24, 24]), name='val_4028')},\n",
       "                %\"val_4032\"<INT64,[2]>{Tensor<INT64,[2]>(array([48, 48]), name='val_4032')},\n",
       "                %\"val_4036\"<INT64,[2]>{Tensor<INT64,[2]>(array([96, 96]), name='val_4036')},\n",
       "                %\"val_2\"<INT64,[]>{Tensor<INT64,[]>(array(0), name='val_2')},\n",
       "                %\"val_54\"<INT64,[2]>{Tensor<INT64,[2]>(array([0, 2]), name='val_54')},\n",
       "                %\"val_103\"<INT64,[8]>{Tensor<INT64,[8]>(array([0, 0, 0, 0, 0, 0, 1, 1]), name='val_103')},\n",
       "                %\"val_104\"<FLOAT16,[]>{Tensor<FLOAT16,[]>(array(-inf, dtype=float16), name='val_104')},\n",
       "                %\"val_3023\"<INT64,[1]>{Tensor<INT64,[1]>(array([768]), name='val_3023')},\n",
       "                %\"val_3024\"<INT64,[1]>{Tensor<INT64,[1]>(array([576]), name='val_3024')},\n",
       "                %\"val_3037\"<INT64,[1]>{Tensor<INT64,[1]>(array([577]), name='val_3037')},\n",
       "                %\"val_3038\"<INT64,[1]>{Tensor<INT64,[1]>(array([3]), name='val_3038')},\n",
       "                %\"val_3039\"<INT64,[1]>{Tensor<INT64,[1]>(array([12]), name='val_3039')},\n",
       "                %\"val_3040\"<INT64,[1]>{Tensor<INT64,[1]>(array([64]), name='val_3040')},\n",
       "                %\"val_3048\"<INT64,[1]>{Tensor<INT64,[1]>(array([2]), name='val_3048')},\n",
       "                %\"val_3066\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1]), name='val_3066')},\n",
       "                %\"val_3068\"<INT64,[1]>{Tensor<INT64,[1]>(array([-2]), name='val_3068')},\n",
       "                %\"val_3070\"<INT64,[1]>{Tensor<INT64,[1]>(array([-9223372036854775808]), name='val_3070')},\n",
       "                %\"val_3096\"<FLOAT16,[]>{Tensor<FLOAT16,[]>(array(1.414, dtype=float16), name='val_3096')},\n",
       "                %\"val_3099\"<FLOAT16,[]>{Tensor<FLOAT16,[]>(array(1., dtype=float16), name='val_3099')},\n",
       "                %\"val_3101\"<FLOAT16,[]>{Tensor<FLOAT16,[]>(array(0.5, dtype=float16), name='val_3101')},\n",
       "                %\"val_3998\"<INT64,[1]>{Tensor<INT64,[1]>(array([24]), name='val_3998')},\n",
       "                %\"val_4038\"<FLOAT,[4]>{Tensor<FLOAT,[4]>(array([1., 1., 2., 2.], dtype=float32), name='val_4038')}\n",
       "            ),\n",
       "        ) {\n",
       "               0 |  # node_Shape_0\n",
       "                    %\"val_0\"<INT64,[1]> ⬅️ ::Shape(%\"input\") {end=1, start=0}\n",
       "               1 |  # node_slice_1\n",
       "                    %\"slice_1\"<FLOAT16,[1,1,768]> ⬅️ ::Slice(%\"pretrained.model.pos_embed\"{...}, %\"val_5\"{[0]}, %\"val_8\"{[1]}, %\"val_8\"{[1]}, %\"val_8\"{[1]})\n",
       "               2 |  # node_select\n",
       "                    %\"select\"<FLOAT16,[577,768]> ⬅️ ::Gather(%\"pretrained.model.pos_embed\"{...}, %\"val_2\"{0}) {axis=0}\n",
       "               3 |  # node_slice_2\n",
       "                    %\"slice_2\"<FLOAT16,[576,768]> ⬅️ ::Slice(%\"select\", %\"val_8\"{[1]}, %\"val_19\"{[9223372036854775807]}, %\"val_5\"{[0]}, %\"val_8\"{[1]})\n",
       "               4 |  # node_view\n",
       "                    %\"view\"<FLOAT16,[1,24,24,768]> ⬅️ ::Reshape(%\"slice_2\", %\"val_29\"{[1, 24, 24, -1]}) {allowzero=1}\n",
       "               5 |  # node_permute\n",
       "                    %\"permute\"<FLOAT16,[1,768,24,24]> ⬅️ ::Transpose(%\"view\") {perm=(0, 3, 1, 2)}\n",
       "               6 |  # node_upsample_bilinear2d\n",
       "                    %\"upsample_bilinear2d\"<FLOAT16,[1,768,24,24]> ⬅️ ::Resize(%\"permute\", None, None, %\"val_33\"{[1, 768, 24, 24]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='pytorch_half_pixel', cubic_coeff_a=-0.75, mode='linear'}\n",
       "               7 |  # node_permute_1\n",
       "                    %\"permute_1\"<FLOAT16,[1,24,24,768]> ⬅️ ::Transpose(%\"upsample_bilinear2d\") {perm=(0, 2, 3, 1)}\n",
       "               8 |  # node_view_1\n",
       "                    %\"view_1\"<FLOAT16,[1,576,768]> ⬅️ ::Reshape(%\"permute_1\", %\"val_38\"{[1, 576, -1]}) {allowzero=1}\n",
       "               9 |  # node_cat\n",
       "                    %\"cat\"<FLOAT16,[1,577,768]> ⬅️ ::Concat(%\"slice_1\", %\"view_1\") {axis=1}\n",
       "              10 |  # node_view_2\n",
       "                    %\"view_2\"<FLOAT16,[1,64,147]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stem.conv.weight\"{...}, %\"val_45\"{[1, 64, -1]}) {allowzero=1}\n",
       "              11 |  # node_ReduceMean_55\n",
       "                    %\"val_55\"<FLOAT16,[64]> ⬅️ ::ReduceMean(%\"view_2\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "              12 |  # node_ReduceMean_56\n",
       "                    %\"val_56\"<FLOAT16,[1,64,1]> ⬅️ ::ReduceMean(%\"view_2\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "              13 |  # node_Sub_57\n",
       "                    %\"val_57\"<FLOAT16,[1,64,147]> ⬅️ ::Sub(%\"view_2\", %\"val_56\")\n",
       "              14 |  # node_Mul_58\n",
       "                    %\"val_58\"<FLOAT16,[1,64,147]> ⬅️ ::Mul(%\"val_57\", %\"val_57\")\n",
       "              15 |  # node_ReduceMean_59\n",
       "                    %\"val_59\"<FLOAT16,[64]> ⬅️ ::ReduceMean(%\"val_58\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "              16 |  # node__native_batch_norm_legit__0\n",
       "                    %\"getitem\"<FLOAT16,[1,64,147]> ⬅️ ::BatchNormalization(%\"view_2\", %\"val_49\"{...}, %\"val_53\"{...}, %\"val_55\", %\"val_59\") {momentum=1.0, epsilon=1e-08}\n",
       "              17 |  # node_view_3\n",
       "                    %\"view_3\"<FLOAT16,[64,3,7,7]> ⬅️ ::Reshape(%\"getitem\", %\"val_72\"{[64, 3, 7, 7]}) {allowzero=1}\n",
       "              18 |  # node_Conv_4664\n",
       "                    %\"conv2d\"<FLOAT16,[batch,64,192,192]> ⬅️ ::Conv(%\"input\", %\"view_3\") {group=1, pads=(2, 2, 3, 3), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "              19 |  # node_Reshape_84\n",
       "                    %\"val_84\"<FLOAT16,[batch,32,73728]> ⬅️ ::Reshape(%\"conv2d\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "              20 |  # node_InstanceNormalization_91\n",
       "                    %\"val_91\"<FLOAT16,[batch,32,73728]> ⬅️ ::InstanceNormalization(%\"val_84\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "              21 |  # node_Shape_92\n",
       "                    %\"val_92\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d\") {start=0}\n",
       "              22 |  # node_Reshape_93\n",
       "                    %\"val_93\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_91\", %\"val_92\") {allowzero=0}\n",
       "              23 |  # node_Mul_101\n",
       "                    %\"val_101\"<FLOAT16,[None,64,None,None]> ⬅️ ::Mul(%\"val_93\", %\"val_100\"{...})\n",
       "              24 |  # node_group_norm\n",
       "                    %\"group_norm\"<FLOAT16,[batch,64,192,192]> ⬅️ ::Add(%\"val_101\", %\"val_102\"{...})\n",
       "              25 |  # node_relu\n",
       "                    %\"relu\"<FLOAT16,[batch,64,192,192]> ⬅️ ::Relu(%\"group_norm\")\n",
       "              26 |  # node_pad_1\n",
       "                    %\"pad_1\"<FLOAT16,[batch,64,193,193]> ⬅️ ::Pad(%\"relu\", %\"val_103\"{[0, 0, 0, 0, 0, 0, 1, 1]}, %\"val_104\"{-inf}) {mode='constant'}\n",
       "              27 |  # node_max_pool2d\n",
       "                    %\"max_pool2d\"<FLOAT16,[batch,64,96,96]> ⬅️ ::MaxPool(%\"pad_1\") {storage_order=0, dilations=(1, 1), ceil_mode=0, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(2, 2), kernel_shape=(3, 3)}\n",
       "              28 |  # node_view_4\n",
       "                    %\"view_4\"<FLOAT16,[1,256,64]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.0.blocks.0.downsample.conv.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "              29 |  # node_ReduceMean_119\n",
       "                    %\"val_120\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_4\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "              30 |  # node_ReduceMean_120\n",
       "                    %\"val_121\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_4\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "              31 |  # node_Sub_121\n",
       "                    %\"val_122\"<FLOAT16,[1,256,64]> ⬅️ ::Sub(%\"view_4\", %\"val_121\")\n",
       "              32 |  # node_Mul_122\n",
       "                    %\"val_123\"<FLOAT16,[1,256,64]> ⬅️ ::Mul(%\"val_122\", %\"val_122\")\n",
       "              33 |  # node_ReduceMean_123\n",
       "                    %\"val_124\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_123\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "              34 |  # node__native_batch_norm_legit_1__0\n",
       "                    %\"getitem_3\"<FLOAT16,[1,256,64]> ⬅️ ::BatchNormalization(%\"view_4\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_120\", %\"val_124\") {momentum=1.0, epsilon=1e-08}\n",
       "              35 |  # node_view_5\n",
       "                    %\"view_5\"<FLOAT16,[256,64,1,1]> ⬅️ ::Reshape(%\"getitem_3\", %\"val_135\"{[256, 64, 1, 1]}) {allowzero=1}\n",
       "              36 |  # node_Conv_4665\n",
       "                    %\"conv2d_1\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Conv(%\"max_pool2d\", %\"view_5\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "              37 |  # node_Reshape_144\n",
       "                    %\"val_145\"<FLOAT16,[batch,32,73728]> ⬅️ ::Reshape(%\"conv2d_1\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "              38 |  # node_InstanceNormalization_151\n",
       "                    %\"val_152\"<FLOAT16,[batch,32,73728]> ⬅️ ::InstanceNormalization(%\"val_145\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "              39 |  # node_Shape_152\n",
       "                    %\"val_153\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_1\") {start=0}\n",
       "              40 |  # node_Reshape_153\n",
       "                    %\"val_154\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_152\", %\"val_153\") {allowzero=0}\n",
       "              41 |  # node_Mul_160\n",
       "                    %\"val_161\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_154\", %\"val_160\"{...})\n",
       "              42 |  # node_group_norm_1\n",
       "                    %\"group_norm_1\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Add(%\"val_161\", %\"val_162\"{...})\n",
       "              43 |  # node_Conv_4666\n",
       "                    %\"conv2d_2\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Conv(%\"max_pool2d\", %\"view_7\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "              44 |  # node_Reshape_201\n",
       "                    %\"val_202\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_2\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "              45 |  # node_InstanceNormalization_208\n",
       "                    %\"val_209\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_202\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "              46 |  # node_Shape_209\n",
       "                    %\"val_210\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_2\") {start=0}\n",
       "              47 |  # node_Reshape_210\n",
       "                    %\"val_211\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_209\", %\"val_210\") {allowzero=0}\n",
       "              48 |  # node_Mul_217\n",
       "                    %\"val_218\"<FLOAT16,[None,64,None,None]> ⬅️ ::Mul(%\"val_211\", %\"val_217\"{...})\n",
       "              49 |  # node_group_norm_2\n",
       "                    %\"group_norm_2\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Add(%\"val_218\", %\"val_219\"{...})\n",
       "              50 |  # node_relu_1\n",
       "                    %\"relu_1\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Relu(%\"group_norm_2\")\n",
       "              51 |  # node_view_8\n",
       "                    %\"view_8\"<FLOAT16,[1,64,576]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.0.blocks.0.conv2.weight\"{...}, %\"val_45\"{[1, 64, -1]}) {allowzero=1}\n",
       "              52 |  # node_ReduceMean_233\n",
       "                    %\"val_234\"<FLOAT16,[64]> ⬅️ ::ReduceMean(%\"view_8\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "              53 |  # node_ReduceMean_234\n",
       "                    %\"val_235\"<FLOAT16,[1,64,1]> ⬅️ ::ReduceMean(%\"view_8\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "              54 |  # node_Sub_235\n",
       "                    %\"val_236\"<FLOAT16,[1,64,576]> ⬅️ ::Sub(%\"view_8\", %\"val_235\")\n",
       "              55 |  # node_Mul_236\n",
       "                    %\"val_237\"<FLOAT16,[1,64,576]> ⬅️ ::Mul(%\"val_236\", %\"val_236\")\n",
       "              56 |  # node_ReduceMean_237\n",
       "                    %\"val_238\"<FLOAT16,[64]> ⬅️ ::ReduceMean(%\"val_237\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "              57 |  # node__native_batch_norm_legit_3__0\n",
       "                    %\"getitem_9\"<FLOAT16,[1,64,576]> ⬅️ ::BatchNormalization(%\"view_8\", %\"val_49\"{...}, %\"val_53\"{...}, %\"val_234\", %\"val_238\") {momentum=1.0, epsilon=1e-08}\n",
       "              58 |  # node_view_9\n",
       "                    %\"view_9\"<FLOAT16,[64,64,3,3]> ⬅️ ::Reshape(%\"getitem_9\", %\"val_249\"{[64, 64, 3, 3]}) {allowzero=1}\n",
       "              59 |  # node_Conv_4667\n",
       "                    %\"conv2d_3\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Conv(%\"relu_1\", %\"view_9\") {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "              60 |  # node_Reshape_258\n",
       "                    %\"val_259\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_3\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "              61 |  # node_InstanceNormalization_265\n",
       "                    %\"val_266\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_259\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "              62 |  # node_Shape_266\n",
       "                    %\"val_267\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_3\") {start=0}\n",
       "              63 |  # node_Reshape_267\n",
       "                    %\"val_268\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_266\", %\"val_267\") {allowzero=0}\n",
       "              64 |  # node_Mul_274\n",
       "                    %\"val_275\"<FLOAT16,[None,64,None,None]> ⬅️ ::Mul(%\"val_268\", %\"val_274\"{...})\n",
       "              65 |  # node_group_norm_3\n",
       "                    %\"group_norm_3\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Add(%\"val_275\", %\"val_276\"{...})\n",
       "              66 |  # node_relu_2\n",
       "                    %\"relu_2\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Relu(%\"group_norm_3\")\n",
       "              67 |  # node_view_10\n",
       "                    %\"view_10\"<FLOAT16,[1,256,64]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.0.blocks.0.conv3.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "              68 |  # node_ReduceMean_290\n",
       "                    %\"val_291\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_10\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "              69 |  # node_ReduceMean_291\n",
       "                    %\"val_292\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_10\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "              70 |  # node_Sub_292\n",
       "                    %\"val_293\"<FLOAT16,[1,256,64]> ⬅️ ::Sub(%\"view_10\", %\"val_292\")\n",
       "              71 |  # node_Mul_293\n",
       "                    %\"val_294\"<FLOAT16,[1,256,64]> ⬅️ ::Mul(%\"val_293\", %\"val_293\")\n",
       "              72 |  # node_ReduceMean_294\n",
       "                    %\"val_295\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_294\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "              73 |  # node__native_batch_norm_legit_4__0\n",
       "                    %\"getitem_12\"<FLOAT16,[1,256,64]> ⬅️ ::BatchNormalization(%\"view_10\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_291\", %\"val_295\") {momentum=1.0, epsilon=1e-08}\n",
       "              74 |  # node_view_11\n",
       "                    %\"view_11\"<FLOAT16,[256,64,1,1]> ⬅️ ::Reshape(%\"getitem_12\", %\"val_135\"{[256, 64, 1, 1]}) {allowzero=1}\n",
       "              75 |  # node_Conv_4668\n",
       "                    %\"conv2d_4\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Conv(%\"relu_2\", %\"view_11\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "              76 |  # node_Reshape_315\n",
       "                    %\"val_316\"<FLOAT16,[batch,32,73728]> ⬅️ ::Reshape(%\"conv2d_4\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "              77 |  # node_InstanceNormalization_322\n",
       "                    %\"val_323\"<FLOAT16,[batch,32,73728]> ⬅️ ::InstanceNormalization(%\"val_316\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "              78 |  # node_Shape_323\n",
       "                    %\"val_324\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_4\") {start=0}\n",
       "              79 |  # node_Reshape_324\n",
       "                    %\"val_325\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_323\", %\"val_324\") {allowzero=0}\n",
       "              80 |  # node_Mul_331\n",
       "                    %\"val_332\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_325\", %\"val_331\"{...})\n",
       "              81 |  # node_group_norm_4\n",
       "                    %\"group_norm_4\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Add(%\"val_332\", %\"val_162\"{...})\n",
       "              82 |  # node_add_95\n",
       "                    %\"add_95\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Add(%\"group_norm_4\", %\"group_norm_1\")\n",
       "              83 |  # node_relu_3\n",
       "                    %\"relu_3\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Relu(%\"add_95\")\n",
       "              84 |  # node_view_12\n",
       "                    %\"view_12\"<FLOAT16,[1,64,256]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.0.blocks.1.conv1.weight\"{...}, %\"val_45\"{[1, 64, -1]}) {allowzero=1}\n",
       "              85 |  # node_ReduceMean_347\n",
       "                    %\"val_348\"<FLOAT16,[64]> ⬅️ ::ReduceMean(%\"view_12\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "              86 |  # node_ReduceMean_348\n",
       "                    %\"val_349\"<FLOAT16,[1,64,1]> ⬅️ ::ReduceMean(%\"view_12\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "              87 |  # node_Sub_349\n",
       "                    %\"val_350\"<FLOAT16,[1,64,256]> ⬅️ ::Sub(%\"view_12\", %\"val_349\")\n",
       "              88 |  # node_Mul_350\n",
       "                    %\"val_351\"<FLOAT16,[1,64,256]> ⬅️ ::Mul(%\"val_350\", %\"val_350\")\n",
       "              89 |  # node_ReduceMean_351\n",
       "                    %\"val_352\"<FLOAT16,[64]> ⬅️ ::ReduceMean(%\"val_351\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "              90 |  # node__native_batch_norm_legit_5__0\n",
       "                    %\"getitem_15\"<FLOAT16,[1,64,256]> ⬅️ ::BatchNormalization(%\"view_12\", %\"val_49\"{...}, %\"val_53\"{...}, %\"val_348\", %\"val_352\") {momentum=1.0, epsilon=1e-08}\n",
       "              91 |  # node_view_13\n",
       "                    %\"view_13\"<FLOAT16,[64,256,1,1]> ⬅️ ::Reshape(%\"getitem_15\", %\"val_363\"{[64, 256, 1, 1]}) {allowzero=1}\n",
       "              92 |  # node_Conv_4669\n",
       "                    %\"conv2d_5\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Conv(%\"relu_3\", %\"view_13\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "              93 |  # node_Reshape_372\n",
       "                    %\"val_373\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_5\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "              94 |  # node_InstanceNormalization_379\n",
       "                    %\"val_380\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_373\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "              95 |  # node_Shape_380\n",
       "                    %\"val_381\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_5\") {start=0}\n",
       "              96 |  # node_Reshape_381\n",
       "                    %\"val_382\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_380\", %\"val_381\") {allowzero=0}\n",
       "              97 |  # node_Mul_388\n",
       "                    %\"val_389\"<FLOAT16,[None,64,None,None]> ⬅️ ::Mul(%\"val_382\", %\"val_388\"{...})\n",
       "              98 |  # node_group_norm_5\n",
       "                    %\"group_norm_5\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Add(%\"val_389\", %\"val_390\"{...})\n",
       "              99 |  # node_relu_4\n",
       "                    %\"relu_4\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Relu(%\"group_norm_5\")\n",
       "             100 |  # node_view_14\n",
       "                    %\"view_14\"<FLOAT16,[1,64,576]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.0.blocks.1.conv2.weight\"{...}, %\"val_45\"{[1, 64, -1]}) {allowzero=1}\n",
       "             101 |  # node_ReduceMean_404\n",
       "                    %\"val_405\"<FLOAT16,[64]> ⬅️ ::ReduceMean(%\"view_14\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             102 |  # node_ReduceMean_405\n",
       "                    %\"val_406\"<FLOAT16,[1,64,1]> ⬅️ ::ReduceMean(%\"view_14\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             103 |  # node_Sub_406\n",
       "                    %\"val_407\"<FLOAT16,[1,64,576]> ⬅️ ::Sub(%\"view_14\", %\"val_406\")\n",
       "             104 |  # node_Mul_407\n",
       "                    %\"val_408\"<FLOAT16,[1,64,576]> ⬅️ ::Mul(%\"val_407\", %\"val_407\")\n",
       "             105 |  # node_ReduceMean_408\n",
       "                    %\"val_409\"<FLOAT16,[64]> ⬅️ ::ReduceMean(%\"val_408\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             106 |  # node__native_batch_norm_legit_6__0\n",
       "                    %\"getitem_18\"<FLOAT16,[1,64,576]> ⬅️ ::BatchNormalization(%\"view_14\", %\"val_49\"{...}, %\"val_53\"{...}, %\"val_405\", %\"val_409\") {momentum=1.0, epsilon=1e-08}\n",
       "             107 |  # node_view_15\n",
       "                    %\"view_15\"<FLOAT16,[64,64,3,3]> ⬅️ ::Reshape(%\"getitem_18\", %\"val_249\"{[64, 64, 3, 3]}) {allowzero=1}\n",
       "             108 |  # node_Conv_4670\n",
       "                    %\"conv2d_6\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Conv(%\"relu_4\", %\"view_15\") {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             109 |  # node_Reshape_429\n",
       "                    %\"val_430\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_6\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             110 |  # node_InstanceNormalization_436\n",
       "                    %\"val_437\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_430\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             111 |  # node_Shape_437\n",
       "                    %\"val_438\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_6\") {start=0}\n",
       "             112 |  # node_Reshape_438\n",
       "                    %\"val_439\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_437\", %\"val_438\") {allowzero=0}\n",
       "             113 |  # node_Mul_445\n",
       "                    %\"val_446\"<FLOAT16,[None,64,None,None]> ⬅️ ::Mul(%\"val_439\", %\"val_445\"{...})\n",
       "             114 |  # node_group_norm_6\n",
       "                    %\"group_norm_6\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Add(%\"val_446\", %\"val_447\"{...})\n",
       "             115 |  # node_relu_5\n",
       "                    %\"relu_5\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Relu(%\"group_norm_6\")\n",
       "             116 |  # node_view_16\n",
       "                    %\"view_16\"<FLOAT16,[1,256,64]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.0.blocks.1.conv3.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             117 |  # node_ReduceMean_461\n",
       "                    %\"val_462\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_16\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             118 |  # node_ReduceMean_462\n",
       "                    %\"val_463\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_16\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             119 |  # node_Sub_463\n",
       "                    %\"val_464\"<FLOAT16,[1,256,64]> ⬅️ ::Sub(%\"view_16\", %\"val_463\")\n",
       "             120 |  # node_Mul_464\n",
       "                    %\"val_465\"<FLOAT16,[1,256,64]> ⬅️ ::Mul(%\"val_464\", %\"val_464\")\n",
       "             121 |  # node_ReduceMean_465\n",
       "                    %\"val_466\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_465\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             122 |  # node__native_batch_norm_legit_7__0\n",
       "                    %\"getitem_21\"<FLOAT16,[1,256,64]> ⬅️ ::BatchNormalization(%\"view_16\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_462\", %\"val_466\") {momentum=1.0, epsilon=1e-08}\n",
       "             123 |  # node_view_17\n",
       "                    %\"view_17\"<FLOAT16,[256,64,1,1]> ⬅️ ::Reshape(%\"getitem_21\", %\"val_135\"{[256, 64, 1, 1]}) {allowzero=1}\n",
       "             124 |  # node_Conv_4671\n",
       "                    %\"conv2d_7\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Conv(%\"relu_5\", %\"view_17\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             125 |  # node_Reshape_486\n",
       "                    %\"val_487\"<FLOAT16,[batch,32,73728]> ⬅️ ::Reshape(%\"conv2d_7\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             126 |  # node_InstanceNormalization_493\n",
       "                    %\"val_494\"<FLOAT16,[batch,32,73728]> ⬅️ ::InstanceNormalization(%\"val_487\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             127 |  # node_Shape_494\n",
       "                    %\"val_495\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_7\") {start=0}\n",
       "             128 |  # node_Reshape_495\n",
       "                    %\"val_496\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_494\", %\"val_495\") {allowzero=0}\n",
       "             129 |  # node_Mul_502\n",
       "                    %\"val_503\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_496\", %\"val_502\"{...})\n",
       "             130 |  # node_group_norm_7\n",
       "                    %\"group_norm_7\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Add(%\"val_503\", %\"val_504\"{...})\n",
       "             131 |  # node_add_161\n",
       "                    %\"add_161\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Add(%\"group_norm_7\", %\"relu_3\")\n",
       "             132 |  # node_relu_6\n",
       "                    %\"relu_6\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Relu(%\"add_161\")\n",
       "             133 |  # node_view_18\n",
       "                    %\"view_18\"<FLOAT16,[1,64,256]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.0.blocks.2.conv1.weight\"{...}, %\"val_45\"{[1, 64, -1]}) {allowzero=1}\n",
       "             134 |  # node_ReduceMean_518\n",
       "                    %\"val_519\"<FLOAT16,[64]> ⬅️ ::ReduceMean(%\"view_18\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             135 |  # node_ReduceMean_519\n",
       "                    %\"val_520\"<FLOAT16,[1,64,1]> ⬅️ ::ReduceMean(%\"view_18\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             136 |  # node_Sub_520\n",
       "                    %\"val_521\"<FLOAT16,[1,64,256]> ⬅️ ::Sub(%\"view_18\", %\"val_520\")\n",
       "             137 |  # node_Mul_521\n",
       "                    %\"val_522\"<FLOAT16,[1,64,256]> ⬅️ ::Mul(%\"val_521\", %\"val_521\")\n",
       "             138 |  # node_ReduceMean_522\n",
       "                    %\"val_523\"<FLOAT16,[64]> ⬅️ ::ReduceMean(%\"val_522\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             139 |  # node__native_batch_norm_legit_8__0\n",
       "                    %\"getitem_24\"<FLOAT16,[1,64,256]> ⬅️ ::BatchNormalization(%\"view_18\", %\"val_49\"{...}, %\"val_53\"{...}, %\"val_519\", %\"val_523\") {momentum=1.0, epsilon=1e-08}\n",
       "             140 |  # node_view_19\n",
       "                    %\"view_19\"<FLOAT16,[64,256,1,1]> ⬅️ ::Reshape(%\"getitem_24\", %\"val_363\"{[64, 256, 1, 1]}) {allowzero=1}\n",
       "             141 |  # node_Conv_4672\n",
       "                    %\"conv2d_8\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Conv(%\"relu_6\", %\"view_19\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             142 |  # node_Reshape_543\n",
       "                    %\"val_544\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_8\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             143 |  # node_InstanceNormalization_550\n",
       "                    %\"val_551\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_544\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             144 |  # node_Shape_551\n",
       "                    %\"val_552\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_8\") {start=0}\n",
       "             145 |  # node_Reshape_552\n",
       "                    %\"val_553\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_551\", %\"val_552\") {allowzero=0}\n",
       "             146 |  # node_Mul_559\n",
       "                    %\"val_560\"<FLOAT16,[None,64,None,None]> ⬅️ ::Mul(%\"val_553\", %\"val_559\"{...})\n",
       "             147 |  # node_group_norm_8\n",
       "                    %\"group_norm_8\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Add(%\"val_560\", %\"val_561\"{...})\n",
       "             148 |  # node_relu_7\n",
       "                    %\"relu_7\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Relu(%\"group_norm_8\")\n",
       "             149 |  # node_view_20\n",
       "                    %\"view_20\"<FLOAT16,[1,64,576]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.0.blocks.2.conv2.weight\"{...}, %\"val_45\"{[1, 64, -1]}) {allowzero=1}\n",
       "             150 |  # node_ReduceMean_575\n",
       "                    %\"val_576\"<FLOAT16,[64]> ⬅️ ::ReduceMean(%\"view_20\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             151 |  # node_ReduceMean_576\n",
       "                    %\"val_577\"<FLOAT16,[1,64,1]> ⬅️ ::ReduceMean(%\"view_20\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             152 |  # node_Sub_577\n",
       "                    %\"val_578\"<FLOAT16,[1,64,576]> ⬅️ ::Sub(%\"view_20\", %\"val_577\")\n",
       "             153 |  # node_Mul_578\n",
       "                    %\"val_579\"<FLOAT16,[1,64,576]> ⬅️ ::Mul(%\"val_578\", %\"val_578\")\n",
       "             154 |  # node_ReduceMean_579\n",
       "                    %\"val_580\"<FLOAT16,[64]> ⬅️ ::ReduceMean(%\"val_579\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             155 |  # node__native_batch_norm_legit_9__0\n",
       "                    %\"getitem_27\"<FLOAT16,[1,64,576]> ⬅️ ::BatchNormalization(%\"view_20\", %\"val_49\"{...}, %\"val_53\"{...}, %\"val_576\", %\"val_580\") {momentum=1.0, epsilon=1e-08}\n",
       "             156 |  # node_view_21\n",
       "                    %\"view_21\"<FLOAT16,[64,64,3,3]> ⬅️ ::Reshape(%\"getitem_27\", %\"val_249\"{[64, 64, 3, 3]}) {allowzero=1}\n",
       "             157 |  # node_Conv_4673\n",
       "                    %\"conv2d_9\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Conv(%\"relu_7\", %\"view_21\") {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             158 |  # node_Reshape_600\n",
       "                    %\"val_601\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_9\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             159 |  # node_InstanceNormalization_607\n",
       "                    %\"val_608\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_601\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             160 |  # node_Shape_608\n",
       "                    %\"val_609\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_9\") {start=0}\n",
       "             161 |  # node_Reshape_609\n",
       "                    %\"val_610\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_608\", %\"val_609\") {allowzero=0}\n",
       "             162 |  # node_Mul_616\n",
       "                    %\"val_617\"<FLOAT16,[None,64,None,None]> ⬅️ ::Mul(%\"val_610\", %\"val_616\"{...})\n",
       "             163 |  # node_group_norm_9\n",
       "                    %\"group_norm_9\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Add(%\"val_617\", %\"val_618\"{...})\n",
       "             164 |  # node_relu_8\n",
       "                    %\"relu_8\"<FLOAT16,[batch,64,96,96]> ⬅️ ::Relu(%\"group_norm_9\")\n",
       "             165 |  # node_view_22\n",
       "                    %\"view_22\"<FLOAT16,[1,256,64]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.0.blocks.2.conv3.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             166 |  # node_ReduceMean_632\n",
       "                    %\"val_633\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_22\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             167 |  # node_ReduceMean_633\n",
       "                    %\"val_634\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_22\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             168 |  # node_Sub_634\n",
       "                    %\"val_635\"<FLOAT16,[1,256,64]> ⬅️ ::Sub(%\"view_22\", %\"val_634\")\n",
       "             169 |  # node_Mul_635\n",
       "                    %\"val_636\"<FLOAT16,[1,256,64]> ⬅️ ::Mul(%\"val_635\", %\"val_635\")\n",
       "             170 |  # node_ReduceMean_636\n",
       "                    %\"val_637\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_636\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             171 |  # node__native_batch_norm_legit_10__0\n",
       "                    %\"getitem_30\"<FLOAT16,[1,256,64]> ⬅️ ::BatchNormalization(%\"view_22\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_633\", %\"val_637\") {momentum=1.0, epsilon=1e-08}\n",
       "             172 |  # node_view_23\n",
       "                    %\"view_23\"<FLOAT16,[256,64,1,1]> ⬅️ ::Reshape(%\"getitem_30\", %\"val_135\"{[256, 64, 1, 1]}) {allowzero=1}\n",
       "             173 |  # node_Conv_4674\n",
       "                    %\"conv2d_10\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Conv(%\"relu_8\", %\"view_23\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             174 |  # node_Reshape_657\n",
       "                    %\"val_658\"<FLOAT16,[batch,32,73728]> ⬅️ ::Reshape(%\"conv2d_10\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             175 |  # node_InstanceNormalization_664\n",
       "                    %\"val_665\"<FLOAT16,[batch,32,73728]> ⬅️ ::InstanceNormalization(%\"val_658\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             176 |  # node_Shape_665\n",
       "                    %\"val_666\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_10\") {start=0}\n",
       "             177 |  # node_Reshape_666\n",
       "                    %\"val_667\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_665\", %\"val_666\") {allowzero=0}\n",
       "             178 |  # node_Mul_673\n",
       "                    %\"val_674\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_667\", %\"val_673\"{...})\n",
       "             179 |  # node_group_norm_10\n",
       "                    %\"group_norm_10\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Add(%\"val_674\", %\"val_675\"{...})\n",
       "             180 |  # node_add_227\n",
       "                    %\"add_227\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Add(%\"group_norm_10\", %\"relu_6\")\n",
       "             181 |  # node_relu_9\n",
       "                    %\"relu_9\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Relu(%\"add_227\")\n",
       "             182 |  # node_view_24\n",
       "                    %\"view_24\"<FLOAT16,[1,512,256]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.1.blocks.0.downsample.conv.weight\"{...}, %\"val_682\"{[1, 512, -1]}) {allowzero=1}\n",
       "             183 |  # node_ReduceMean_691\n",
       "                    %\"val_692\"<FLOAT16,[512]> ⬅️ ::ReduceMean(%\"view_24\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             184 |  # node_ReduceMean_692\n",
       "                    %\"val_693\"<FLOAT16,[1,512,1]> ⬅️ ::ReduceMean(%\"view_24\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             185 |  # node_Sub_693\n",
       "                    %\"val_694\"<FLOAT16,[1,512,256]> ⬅️ ::Sub(%\"view_24\", %\"val_693\")\n",
       "             186 |  # node_Mul_694\n",
       "                    %\"val_695\"<FLOAT16,[1,512,256]> ⬅️ ::Mul(%\"val_694\", %\"val_694\")\n",
       "             187 |  # node_ReduceMean_695\n",
       "                    %\"val_696\"<FLOAT16,[512]> ⬅️ ::ReduceMean(%\"val_695\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             188 |  # node__native_batch_norm_legit_11__0\n",
       "                    %\"getitem_33\"<FLOAT16,[1,512,256]> ⬅️ ::BatchNormalization(%\"view_24\", %\"val_686\"{...}, %\"val_690\"{...}, %\"val_692\", %\"val_696\") {momentum=1.0, epsilon=1e-08}\n",
       "             189 |  # node_view_25\n",
       "                    %\"view_25\"<FLOAT16,[512,256,1,1]> ⬅️ ::Reshape(%\"getitem_33\", %\"val_707\"{[512, 256, 1, 1]}) {allowzero=1}\n",
       "             190 |  # node_Conv_4675\n",
       "                    %\"conv2d_11\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Conv(%\"relu_9\", %\"view_25\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "             191 |  # node_Reshape_716\n",
       "                    %\"val_717\"<FLOAT16,[batch,32,36864]> ⬅️ ::Reshape(%\"conv2d_11\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             192 |  # node_InstanceNormalization_723\n",
       "                    %\"val_724\"<FLOAT16,[batch,32,36864]> ⬅️ ::InstanceNormalization(%\"val_717\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             193 |  # node_Shape_724\n",
       "                    %\"val_725\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_11\") {start=0}\n",
       "             194 |  # node_Reshape_725\n",
       "                    %\"val_726\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_724\", %\"val_725\") {allowzero=0}\n",
       "             195 |  # node_Mul_732\n",
       "                    %\"val_733\"<FLOAT16,[None,512,None,None]> ⬅️ ::Mul(%\"val_726\", %\"val_732\"{...})\n",
       "             196 |  # node_group_norm_11\n",
       "                    %\"group_norm_11\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Add(%\"val_733\", %\"val_734\"{...})\n",
       "             197 |  # node_view_26\n",
       "                    %\"view_26\"<FLOAT16,[1,128,256]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.1.blocks.0.conv1.weight\"{...}, %\"val_739\"{[1, 128, -1]}) {allowzero=1}\n",
       "             198 |  # node_ReduceMean_748\n",
       "                    %\"val_749\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"view_26\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             199 |  # node_ReduceMean_749\n",
       "                    %\"val_750\"<FLOAT16,[1,128,1]> ⬅️ ::ReduceMean(%\"view_26\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             200 |  # node_Sub_750\n",
       "                    %\"val_751\"<FLOAT16,[1,128,256]> ⬅️ ::Sub(%\"view_26\", %\"val_750\")\n",
       "             201 |  # node_Mul_751\n",
       "                    %\"val_752\"<FLOAT16,[1,128,256]> ⬅️ ::Mul(%\"val_751\", %\"val_751\")\n",
       "             202 |  # node_ReduceMean_752\n",
       "                    %\"val_753\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"val_752\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             203 |  # node__native_batch_norm_legit_12__0\n",
       "                    %\"getitem_36\"<FLOAT16,[1,128,256]> ⬅️ ::BatchNormalization(%\"view_26\", %\"val_743\"{...}, %\"val_747\"{...}, %\"val_749\", %\"val_753\") {momentum=1.0, epsilon=1e-08}\n",
       "             204 |  # node_view_27\n",
       "                    %\"view_27\"<FLOAT16,[128,256,1,1]> ⬅️ ::Reshape(%\"getitem_36\", %\"val_764\"{[128, 256, 1, 1]}) {allowzero=1}\n",
       "             205 |  # node_Conv_4676\n",
       "                    %\"conv2d_12\"<FLOAT16,[batch,128,96,96]> ⬅️ ::Conv(%\"relu_9\", %\"view_27\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             206 |  # node_Reshape_773\n",
       "                    %\"val_774\"<FLOAT16,[batch,32,36864]> ⬅️ ::Reshape(%\"conv2d_12\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             207 |  # node_InstanceNormalization_780\n",
       "                    %\"val_781\"<FLOAT16,[batch,32,36864]> ⬅️ ::InstanceNormalization(%\"val_774\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             208 |  # node_Shape_781\n",
       "                    %\"val_782\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_12\") {start=0}\n",
       "             209 |  # node_Reshape_782\n",
       "                    %\"val_783\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_781\", %\"val_782\") {allowzero=0}\n",
       "             210 |  # node_Mul_789\n",
       "                    %\"val_790\"<FLOAT16,[None,128,None,None]> ⬅️ ::Mul(%\"val_783\", %\"val_789\"{...})\n",
       "             211 |  # node_group_norm_12\n",
       "                    %\"group_norm_12\"<FLOAT16,[batch,128,96,96]> ⬅️ ::Add(%\"val_790\", %\"val_791\"{...})\n",
       "             212 |  # node_relu_10\n",
       "                    %\"relu_10\"<FLOAT16,[batch,128,96,96]> ⬅️ ::Relu(%\"group_norm_12\")\n",
       "             213 |  # node_view_28\n",
       "                    %\"view_28\"<FLOAT16,[1,128,1152]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.1.blocks.0.conv2.weight\"{...}, %\"val_739\"{[1, 128, -1]}) {allowzero=1}\n",
       "             214 |  # node_ReduceMean_807\n",
       "                    %\"val_808\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"view_28\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             215 |  # node_ReduceMean_808\n",
       "                    %\"val_809\"<FLOAT16,[1,128,1]> ⬅️ ::ReduceMean(%\"view_28\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             216 |  # node_Sub_809\n",
       "                    %\"val_810\"<FLOAT16,[1,128,1152]> ⬅️ ::Sub(%\"view_28\", %\"val_809\")\n",
       "             217 |  # node_Mul_810\n",
       "                    %\"val_811\"<FLOAT16,[1,128,1152]> ⬅️ ::Mul(%\"val_810\", %\"val_810\")\n",
       "             218 |  # node_ReduceMean_811\n",
       "                    %\"val_812\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"val_811\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             219 |  # node__native_batch_norm_legit_13__0\n",
       "                    %\"getitem_39\"<FLOAT16,[1,128,1152]> ⬅️ ::BatchNormalization(%\"view_28\", %\"val_743\"{...}, %\"val_747\"{...}, %\"val_808\", %\"val_812\") {momentum=1.0, epsilon=1e-08}\n",
       "             220 |  # node_view_29\n",
       "                    %\"view_29\"<FLOAT16,[128,128,3,3]> ⬅️ ::Reshape(%\"getitem_39\", %\"val_823\"{[128, 128, 3, 3]}) {allowzero=1}\n",
       "             221 |  # node_Conv_4677\n",
       "                    %\"conv2d_13\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Conv(%\"relu_10\", %\"view_29\") {group=1, pads=(0, 0, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "             222 |  # node_Reshape_832\n",
       "                    %\"val_833\"<FLOAT16,[batch,32,9216]> ⬅️ ::Reshape(%\"conv2d_13\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             223 |  # node_InstanceNormalization_839\n",
       "                    %\"val_840\"<FLOAT16,[batch,32,9216]> ⬅️ ::InstanceNormalization(%\"val_833\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             224 |  # node_Shape_840\n",
       "                    %\"val_841\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_13\") {start=0}\n",
       "             225 |  # node_Reshape_841\n",
       "                    %\"val_842\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_840\", %\"val_841\") {allowzero=0}\n",
       "             226 |  # node_Mul_848\n",
       "                    %\"val_849\"<FLOAT16,[None,128,None,None]> ⬅️ ::Mul(%\"val_842\", %\"val_848\"{...})\n",
       "             227 |  # node_group_norm_13\n",
       "                    %\"group_norm_13\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Add(%\"val_849\", %\"val_850\"{...})\n",
       "             228 |  # node_relu_11\n",
       "                    %\"relu_11\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Relu(%\"group_norm_13\")\n",
       "             229 |  # node_view_30\n",
       "                    %\"view_30\"<FLOAT16,[1,512,128]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.1.blocks.0.conv3.weight\"{...}, %\"val_682\"{[1, 512, -1]}) {allowzero=1}\n",
       "             230 |  # node_ReduceMean_864\n",
       "                    %\"val_865\"<FLOAT16,[512]> ⬅️ ::ReduceMean(%\"view_30\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             231 |  # node_ReduceMean_865\n",
       "                    %\"val_866\"<FLOAT16,[1,512,1]> ⬅️ ::ReduceMean(%\"view_30\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             232 |  # node_Sub_866\n",
       "                    %\"val_867\"<FLOAT16,[1,512,128]> ⬅️ ::Sub(%\"view_30\", %\"val_866\")\n",
       "             233 |  # node_Mul_867\n",
       "                    %\"val_868\"<FLOAT16,[1,512,128]> ⬅️ ::Mul(%\"val_867\", %\"val_867\")\n",
       "             234 |  # node_ReduceMean_868\n",
       "                    %\"val_869\"<FLOAT16,[512]> ⬅️ ::ReduceMean(%\"val_868\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             235 |  # node__native_batch_norm_legit_14__0\n",
       "                    %\"getitem_42\"<FLOAT16,[1,512,128]> ⬅️ ::BatchNormalization(%\"view_30\", %\"val_686\"{...}, %\"val_690\"{...}, %\"val_865\", %\"val_869\") {momentum=1.0, epsilon=1e-08}\n",
       "             236 |  # node_view_31\n",
       "                    %\"view_31\"<FLOAT16,[512,128,1,1]> ⬅️ ::Reshape(%\"getitem_42\", %\"val_880\"{[512, 128, 1, 1]}) {allowzero=1}\n",
       "             237 |  # node_Conv_4678\n",
       "                    %\"conv2d_14\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Conv(%\"relu_11\", %\"view_31\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             238 |  # node_Reshape_889\n",
       "                    %\"val_890\"<FLOAT16,[batch,32,36864]> ⬅️ ::Reshape(%\"conv2d_14\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             239 |  # node_InstanceNormalization_896\n",
       "                    %\"val_897\"<FLOAT16,[batch,32,36864]> ⬅️ ::InstanceNormalization(%\"val_890\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             240 |  # node_Shape_897\n",
       "                    %\"val_898\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_14\") {start=0}\n",
       "             241 |  # node_Reshape_898\n",
       "                    %\"val_899\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_897\", %\"val_898\") {allowzero=0}\n",
       "             242 |  # node_Mul_905\n",
       "                    %\"val_906\"<FLOAT16,[None,512,None,None]> ⬅️ ::Mul(%\"val_899\", %\"val_905\"{...})\n",
       "             243 |  # node_group_norm_14\n",
       "                    %\"group_norm_14\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Add(%\"val_906\", %\"val_907\"{...})\n",
       "             244 |  # node_add_313\n",
       "                    %\"add_313\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Add(%\"group_norm_14\", %\"group_norm_11\")\n",
       "             245 |  # node_relu_12\n",
       "                    %\"relu_12\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Relu(%\"add_313\")\n",
       "             246 |  # node_view_32\n",
       "                    %\"view_32\"<FLOAT16,[1,128,512]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.1.blocks.1.conv1.weight\"{...}, %\"val_739\"{[1, 128, -1]}) {allowzero=1}\n",
       "             247 |  # node_ReduceMean_921\n",
       "                    %\"val_922\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"view_32\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             248 |  # node_ReduceMean_922\n",
       "                    %\"val_923\"<FLOAT16,[1,128,1]> ⬅️ ::ReduceMean(%\"view_32\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             249 |  # node_Sub_923\n",
       "                    %\"val_924\"<FLOAT16,[1,128,512]> ⬅️ ::Sub(%\"view_32\", %\"val_923\")\n",
       "             250 |  # node_Mul_924\n",
       "                    %\"val_925\"<FLOAT16,[1,128,512]> ⬅️ ::Mul(%\"val_924\", %\"val_924\")\n",
       "             251 |  # node_ReduceMean_925\n",
       "                    %\"val_926\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"val_925\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             252 |  # node__native_batch_norm_legit_15__0\n",
       "                    %\"getitem_45\"<FLOAT16,[1,128,512]> ⬅️ ::BatchNormalization(%\"view_32\", %\"val_743\"{...}, %\"val_747\"{...}, %\"val_922\", %\"val_926\") {momentum=1.0, epsilon=1e-08}\n",
       "             253 |  # node_view_33\n",
       "                    %\"view_33\"<FLOAT16,[128,512,1,1]> ⬅️ ::Reshape(%\"getitem_45\", %\"val_937\"{[128, 512, 1, 1]}) {allowzero=1}\n",
       "             254 |  # node_Conv_4679\n",
       "                    %\"conv2d_15\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Conv(%\"relu_12\", %\"view_33\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             255 |  # node_Reshape_946\n",
       "                    %\"val_947\"<FLOAT16,[batch,32,9216]> ⬅️ ::Reshape(%\"conv2d_15\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             256 |  # node_InstanceNormalization_953\n",
       "                    %\"val_954\"<FLOAT16,[batch,32,9216]> ⬅️ ::InstanceNormalization(%\"val_947\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             257 |  # node_Shape_954\n",
       "                    %\"val_955\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_15\") {start=0}\n",
       "             258 |  # node_Reshape_955\n",
       "                    %\"val_956\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_954\", %\"val_955\") {allowzero=0}\n",
       "             259 |  # node_Mul_962\n",
       "                    %\"val_963\"<FLOAT16,[None,128,None,None]> ⬅️ ::Mul(%\"val_956\", %\"val_962\"{...})\n",
       "             260 |  # node_group_norm_15\n",
       "                    %\"group_norm_15\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Add(%\"val_963\", %\"val_964\"{...})\n",
       "             261 |  # node_relu_13\n",
       "                    %\"relu_13\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Relu(%\"group_norm_15\")\n",
       "             262 |  # node_view_34\n",
       "                    %\"view_34\"<FLOAT16,[1,128,1152]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.1.blocks.1.conv2.weight\"{...}, %\"val_739\"{[1, 128, -1]}) {allowzero=1}\n",
       "             263 |  # node_ReduceMean_978\n",
       "                    %\"val_979\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"view_34\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             264 |  # node_ReduceMean_979\n",
       "                    %\"val_980\"<FLOAT16,[1,128,1]> ⬅️ ::ReduceMean(%\"view_34\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             265 |  # node_Sub_980\n",
       "                    %\"val_981\"<FLOAT16,[1,128,1152]> ⬅️ ::Sub(%\"view_34\", %\"val_980\")\n",
       "             266 |  # node_Mul_981\n",
       "                    %\"val_982\"<FLOAT16,[1,128,1152]> ⬅️ ::Mul(%\"val_981\", %\"val_981\")\n",
       "             267 |  # node_ReduceMean_982\n",
       "                    %\"val_983\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"val_982\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             268 |  # node__native_batch_norm_legit_16__0\n",
       "                    %\"getitem_48\"<FLOAT16,[1,128,1152]> ⬅️ ::BatchNormalization(%\"view_34\", %\"val_743\"{...}, %\"val_747\"{...}, %\"val_979\", %\"val_983\") {momentum=1.0, epsilon=1e-08}\n",
       "             269 |  # node_view_35\n",
       "                    %\"view_35\"<FLOAT16,[128,128,3,3]> ⬅️ ::Reshape(%\"getitem_48\", %\"val_823\"{[128, 128, 3, 3]}) {allowzero=1}\n",
       "             270 |  # node_Conv_4680\n",
       "                    %\"conv2d_16\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Conv(%\"relu_13\", %\"view_35\") {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             271 |  # node_Reshape_1003\n",
       "                    %\"val_1004\"<FLOAT16,[batch,32,9216]> ⬅️ ::Reshape(%\"conv2d_16\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             272 |  # node_InstanceNormalization_1010\n",
       "                    %\"val_1011\"<FLOAT16,[batch,32,9216]> ⬅️ ::InstanceNormalization(%\"val_1004\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             273 |  # node_Shape_1011\n",
       "                    %\"val_1012\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_16\") {start=0}\n",
       "             274 |  # node_Reshape_1012\n",
       "                    %\"val_1013\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1011\", %\"val_1012\") {allowzero=0}\n",
       "             275 |  # node_Mul_1019\n",
       "                    %\"val_1020\"<FLOAT16,[None,128,None,None]> ⬅️ ::Mul(%\"val_1013\", %\"val_1019\"{...})\n",
       "             276 |  # node_group_norm_16\n",
       "                    %\"group_norm_16\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Add(%\"val_1020\", %\"val_1021\"{...})\n",
       "             277 |  # node_relu_14\n",
       "                    %\"relu_14\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Relu(%\"group_norm_16\")\n",
       "             278 |  # node_view_36\n",
       "                    %\"view_36\"<FLOAT16,[1,512,128]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.1.blocks.1.conv3.weight\"{...}, %\"val_682\"{[1, 512, -1]}) {allowzero=1}\n",
       "             279 |  # node_ReduceMean_1035\n",
       "                    %\"val_1036\"<FLOAT16,[512]> ⬅️ ::ReduceMean(%\"view_36\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             280 |  # node_ReduceMean_1036\n",
       "                    %\"val_1037\"<FLOAT16,[1,512,1]> ⬅️ ::ReduceMean(%\"view_36\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             281 |  # node_Sub_1037\n",
       "                    %\"val_1038\"<FLOAT16,[1,512,128]> ⬅️ ::Sub(%\"view_36\", %\"val_1037\")\n",
       "             282 |  # node_Mul_1038\n",
       "                    %\"val_1039\"<FLOAT16,[1,512,128]> ⬅️ ::Mul(%\"val_1038\", %\"val_1038\")\n",
       "             283 |  # node_ReduceMean_1039\n",
       "                    %\"val_1040\"<FLOAT16,[512]> ⬅️ ::ReduceMean(%\"val_1039\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             284 |  # node__native_batch_norm_legit_17__0\n",
       "                    %\"getitem_51\"<FLOAT16,[1,512,128]> ⬅️ ::BatchNormalization(%\"view_36\", %\"val_686\"{...}, %\"val_690\"{...}, %\"val_1036\", %\"val_1040\") {momentum=1.0, epsilon=1e-08}\n",
       "             285 |  # node_view_37\n",
       "                    %\"view_37\"<FLOAT16,[512,128,1,1]> ⬅️ ::Reshape(%\"getitem_51\", %\"val_880\"{[512, 128, 1, 1]}) {allowzero=1}\n",
       "             286 |  # node_Conv_4681\n",
       "                    %\"conv2d_17\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Conv(%\"relu_14\", %\"view_37\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             287 |  # node_Reshape_1060\n",
       "                    %\"val_1061\"<FLOAT16,[batch,32,36864]> ⬅️ ::Reshape(%\"conv2d_17\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             288 |  # node_InstanceNormalization_1067\n",
       "                    %\"val_1068\"<FLOAT16,[batch,32,36864]> ⬅️ ::InstanceNormalization(%\"val_1061\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             289 |  # node_Shape_1068\n",
       "                    %\"val_1069\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_17\") {start=0}\n",
       "             290 |  # node_Reshape_1069\n",
       "                    %\"val_1070\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1068\", %\"val_1069\") {allowzero=0}\n",
       "             291 |  # node_Mul_1076\n",
       "                    %\"val_1077\"<FLOAT16,[None,512,None,None]> ⬅️ ::Mul(%\"val_1070\", %\"val_1076\"{...})\n",
       "             292 |  # node_group_norm_17\n",
       "                    %\"group_norm_17\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Add(%\"val_1077\", %\"val_1078\"{...})\n",
       "             293 |  # node_add_379\n",
       "                    %\"add_379\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Add(%\"group_norm_17\", %\"relu_12\")\n",
       "             294 |  # node_relu_15\n",
       "                    %\"relu_15\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Relu(%\"add_379\")\n",
       "             295 |  # node_view_38\n",
       "                    %\"view_38\"<FLOAT16,[1,128,512]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.1.blocks.2.conv1.weight\"{...}, %\"val_739\"{[1, 128, -1]}) {allowzero=1}\n",
       "             296 |  # node_ReduceMean_1092\n",
       "                    %\"val_1093\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"view_38\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             297 |  # node_ReduceMean_1093\n",
       "                    %\"val_1094\"<FLOAT16,[1,128,1]> ⬅️ ::ReduceMean(%\"view_38\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             298 |  # node_Sub_1094\n",
       "                    %\"val_1095\"<FLOAT16,[1,128,512]> ⬅️ ::Sub(%\"view_38\", %\"val_1094\")\n",
       "             299 |  # node_Mul_1095\n",
       "                    %\"val_1096\"<FLOAT16,[1,128,512]> ⬅️ ::Mul(%\"val_1095\", %\"val_1095\")\n",
       "             300 |  # node_ReduceMean_1096\n",
       "                    %\"val_1097\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"val_1096\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             301 |  # node__native_batch_norm_legit_18__0\n",
       "                    %\"getitem_54\"<FLOAT16,[1,128,512]> ⬅️ ::BatchNormalization(%\"view_38\", %\"val_743\"{...}, %\"val_747\"{...}, %\"val_1093\", %\"val_1097\") {momentum=1.0, epsilon=1e-08}\n",
       "             302 |  # node_view_39\n",
       "                    %\"view_39\"<FLOAT16,[128,512,1,1]> ⬅️ ::Reshape(%\"getitem_54\", %\"val_937\"{[128, 512, 1, 1]}) {allowzero=1}\n",
       "             303 |  # node_Conv_4682\n",
       "                    %\"conv2d_18\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Conv(%\"relu_15\", %\"view_39\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             304 |  # node_Reshape_1117\n",
       "                    %\"val_1118\"<FLOAT16,[batch,32,9216]> ⬅️ ::Reshape(%\"conv2d_18\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             305 |  # node_InstanceNormalization_1124\n",
       "                    %\"val_1125\"<FLOAT16,[batch,32,9216]> ⬅️ ::InstanceNormalization(%\"val_1118\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             306 |  # node_Shape_1125\n",
       "                    %\"val_1126\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_18\") {start=0}\n",
       "             307 |  # node_Reshape_1126\n",
       "                    %\"val_1127\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1125\", %\"val_1126\") {allowzero=0}\n",
       "             308 |  # node_Mul_1133\n",
       "                    %\"val_1134\"<FLOAT16,[None,128,None,None]> ⬅️ ::Mul(%\"val_1127\", %\"val_1133\"{...})\n",
       "             309 |  # node_group_norm_18\n",
       "                    %\"group_norm_18\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Add(%\"val_1134\", %\"val_1135\"{...})\n",
       "             310 |  # node_relu_16\n",
       "                    %\"relu_16\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Relu(%\"group_norm_18\")\n",
       "             311 |  # node_view_40\n",
       "                    %\"view_40\"<FLOAT16,[1,128,1152]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.1.blocks.2.conv2.weight\"{...}, %\"val_739\"{[1, 128, -1]}) {allowzero=1}\n",
       "             312 |  # node_ReduceMean_1149\n",
       "                    %\"val_1150\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"view_40\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             313 |  # node_ReduceMean_1150\n",
       "                    %\"val_1151\"<FLOAT16,[1,128,1]> ⬅️ ::ReduceMean(%\"view_40\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             314 |  # node_Sub_1151\n",
       "                    %\"val_1152\"<FLOAT16,[1,128,1152]> ⬅️ ::Sub(%\"view_40\", %\"val_1151\")\n",
       "             315 |  # node_Mul_1152\n",
       "                    %\"val_1153\"<FLOAT16,[1,128,1152]> ⬅️ ::Mul(%\"val_1152\", %\"val_1152\")\n",
       "             316 |  # node_ReduceMean_1153\n",
       "                    %\"val_1154\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"val_1153\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             317 |  # node__native_batch_norm_legit_19__0\n",
       "                    %\"getitem_57\"<FLOAT16,[1,128,1152]> ⬅️ ::BatchNormalization(%\"view_40\", %\"val_743\"{...}, %\"val_747\"{...}, %\"val_1150\", %\"val_1154\") {momentum=1.0, epsilon=1e-08}\n",
       "             318 |  # node_view_41\n",
       "                    %\"view_41\"<FLOAT16,[128,128,3,3]> ⬅️ ::Reshape(%\"getitem_57\", %\"val_823\"{[128, 128, 3, 3]}) {allowzero=1}\n",
       "             319 |  # node_Conv_4683\n",
       "                    %\"conv2d_19\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Conv(%\"relu_16\", %\"view_41\") {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             320 |  # node_Reshape_1174\n",
       "                    %\"val_1175\"<FLOAT16,[batch,32,9216]> ⬅️ ::Reshape(%\"conv2d_19\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             321 |  # node_InstanceNormalization_1181\n",
       "                    %\"val_1182\"<FLOAT16,[batch,32,9216]> ⬅️ ::InstanceNormalization(%\"val_1175\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             322 |  # node_Shape_1182\n",
       "                    %\"val_1183\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_19\") {start=0}\n",
       "             323 |  # node_Reshape_1183\n",
       "                    %\"val_1184\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1182\", %\"val_1183\") {allowzero=0}\n",
       "             324 |  # node_Mul_1190\n",
       "                    %\"val_1191\"<FLOAT16,[None,128,None,None]> ⬅️ ::Mul(%\"val_1184\", %\"val_1190\"{...})\n",
       "             325 |  # node_group_norm_19\n",
       "                    %\"group_norm_19\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Add(%\"val_1191\", %\"val_1192\"{...})\n",
       "             326 |  # node_relu_17\n",
       "                    %\"relu_17\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Relu(%\"group_norm_19\")\n",
       "             327 |  # node_view_42\n",
       "                    %\"view_42\"<FLOAT16,[1,512,128]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.1.blocks.2.conv3.weight\"{...}, %\"val_682\"{[1, 512, -1]}) {allowzero=1}\n",
       "             328 |  # node_ReduceMean_1206\n",
       "                    %\"val_1207\"<FLOAT16,[512]> ⬅️ ::ReduceMean(%\"view_42\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             329 |  # node_ReduceMean_1207\n",
       "                    %\"val_1208\"<FLOAT16,[1,512,1]> ⬅️ ::ReduceMean(%\"view_42\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             330 |  # node_Sub_1208\n",
       "                    %\"val_1209\"<FLOAT16,[1,512,128]> ⬅️ ::Sub(%\"view_42\", %\"val_1208\")\n",
       "             331 |  # node_Mul_1209\n",
       "                    %\"val_1210\"<FLOAT16,[1,512,128]> ⬅️ ::Mul(%\"val_1209\", %\"val_1209\")\n",
       "             332 |  # node_ReduceMean_1210\n",
       "                    %\"val_1211\"<FLOAT16,[512]> ⬅️ ::ReduceMean(%\"val_1210\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             333 |  # node__native_batch_norm_legit_20__0\n",
       "                    %\"getitem_60\"<FLOAT16,[1,512,128]> ⬅️ ::BatchNormalization(%\"view_42\", %\"val_686\"{...}, %\"val_690\"{...}, %\"val_1207\", %\"val_1211\") {momentum=1.0, epsilon=1e-08}\n",
       "             334 |  # node_view_43\n",
       "                    %\"view_43\"<FLOAT16,[512,128,1,1]> ⬅️ ::Reshape(%\"getitem_60\", %\"val_880\"{[512, 128, 1, 1]}) {allowzero=1}\n",
       "             335 |  # node_Conv_4684\n",
       "                    %\"conv2d_20\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Conv(%\"relu_17\", %\"view_43\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             336 |  # node_Reshape_1231\n",
       "                    %\"val_1232\"<FLOAT16,[batch,32,36864]> ⬅️ ::Reshape(%\"conv2d_20\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             337 |  # node_InstanceNormalization_1238\n",
       "                    %\"val_1239\"<FLOAT16,[batch,32,36864]> ⬅️ ::InstanceNormalization(%\"val_1232\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             338 |  # node_Shape_1239\n",
       "                    %\"val_1240\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_20\") {start=0}\n",
       "             339 |  # node_Reshape_1240\n",
       "                    %\"val_1241\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1239\", %\"val_1240\") {allowzero=0}\n",
       "             340 |  # node_Mul_1247\n",
       "                    %\"val_1248\"<FLOAT16,[None,512,None,None]> ⬅️ ::Mul(%\"val_1241\", %\"val_1247\"{...})\n",
       "             341 |  # node_group_norm_20\n",
       "                    %\"group_norm_20\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Add(%\"val_1248\", %\"val_1249\"{...})\n",
       "             342 |  # node_add_445\n",
       "                    %\"add_445\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Add(%\"group_norm_20\", %\"relu_15\")\n",
       "             343 |  # node_relu_18\n",
       "                    %\"relu_18\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Relu(%\"add_445\")\n",
       "             344 |  # node_view_44\n",
       "                    %\"view_44\"<FLOAT16,[1,128,512]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.1.blocks.3.conv1.weight\"{...}, %\"val_739\"{[1, 128, -1]}) {allowzero=1}\n",
       "             345 |  # node_ReduceMean_1263\n",
       "                    %\"val_1264\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"view_44\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             346 |  # node_ReduceMean_1264\n",
       "                    %\"val_1265\"<FLOAT16,[1,128,1]> ⬅️ ::ReduceMean(%\"view_44\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             347 |  # node_Sub_1265\n",
       "                    %\"val_1266\"<FLOAT16,[1,128,512]> ⬅️ ::Sub(%\"view_44\", %\"val_1265\")\n",
       "             348 |  # node_Mul_1266\n",
       "                    %\"val_1267\"<FLOAT16,[1,128,512]> ⬅️ ::Mul(%\"val_1266\", %\"val_1266\")\n",
       "             349 |  # node_ReduceMean_1267\n",
       "                    %\"val_1268\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"val_1267\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             350 |  # node__native_batch_norm_legit_21__0\n",
       "                    %\"getitem_63\"<FLOAT16,[1,128,512]> ⬅️ ::BatchNormalization(%\"view_44\", %\"val_743\"{...}, %\"val_747\"{...}, %\"val_1264\", %\"val_1268\") {momentum=1.0, epsilon=1e-08}\n",
       "             351 |  # node_view_45\n",
       "                    %\"view_45\"<FLOAT16,[128,512,1,1]> ⬅️ ::Reshape(%\"getitem_63\", %\"val_937\"{[128, 512, 1, 1]}) {allowzero=1}\n",
       "             352 |  # node_Conv_4685\n",
       "                    %\"conv2d_21\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Conv(%\"relu_18\", %\"view_45\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             353 |  # node_Reshape_1288\n",
       "                    %\"val_1289\"<FLOAT16,[batch,32,9216]> ⬅️ ::Reshape(%\"conv2d_21\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             354 |  # node_InstanceNormalization_1295\n",
       "                    %\"val_1296\"<FLOAT16,[batch,32,9216]> ⬅️ ::InstanceNormalization(%\"val_1289\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             355 |  # node_Shape_1296\n",
       "                    %\"val_1297\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_21\") {start=0}\n",
       "             356 |  # node_Reshape_1297\n",
       "                    %\"val_1298\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1296\", %\"val_1297\") {allowzero=0}\n",
       "             357 |  # node_Mul_1304\n",
       "                    %\"val_1305\"<FLOAT16,[None,128,None,None]> ⬅️ ::Mul(%\"val_1298\", %\"val_1304\"{...})\n",
       "             358 |  # node_group_norm_21\n",
       "                    %\"group_norm_21\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Add(%\"val_1305\", %\"val_1306\"{...})\n",
       "             359 |  # node_relu_19\n",
       "                    %\"relu_19\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Relu(%\"group_norm_21\")\n",
       "             360 |  # node_view_46\n",
       "                    %\"view_46\"<FLOAT16,[1,128,1152]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.1.blocks.3.conv2.weight\"{...}, %\"val_739\"{[1, 128, -1]}) {allowzero=1}\n",
       "             361 |  # node_ReduceMean_1320\n",
       "                    %\"val_1321\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"view_46\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             362 |  # node_ReduceMean_1321\n",
       "                    %\"val_1322\"<FLOAT16,[1,128,1]> ⬅️ ::ReduceMean(%\"view_46\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             363 |  # node_Sub_1322\n",
       "                    %\"val_1323\"<FLOAT16,[1,128,1152]> ⬅️ ::Sub(%\"view_46\", %\"val_1322\")\n",
       "             364 |  # node_Mul_1323\n",
       "                    %\"val_1324\"<FLOAT16,[1,128,1152]> ⬅️ ::Mul(%\"val_1323\", %\"val_1323\")\n",
       "             365 |  # node_ReduceMean_1324\n",
       "                    %\"val_1325\"<FLOAT16,[128]> ⬅️ ::ReduceMean(%\"val_1324\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             366 |  # node__native_batch_norm_legit_22__0\n",
       "                    %\"getitem_66\"<FLOAT16,[1,128,1152]> ⬅️ ::BatchNormalization(%\"view_46\", %\"val_743\"{...}, %\"val_747\"{...}, %\"val_1321\", %\"val_1325\") {momentum=1.0, epsilon=1e-08}\n",
       "             367 |  # node_view_47\n",
       "                    %\"view_47\"<FLOAT16,[128,128,3,3]> ⬅️ ::Reshape(%\"getitem_66\", %\"val_823\"{[128, 128, 3, 3]}) {allowzero=1}\n",
       "             368 |  # node_Conv_4686\n",
       "                    %\"conv2d_22\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Conv(%\"relu_19\", %\"view_47\") {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             369 |  # node_Reshape_1345\n",
       "                    %\"val_1346\"<FLOAT16,[batch,32,9216]> ⬅️ ::Reshape(%\"conv2d_22\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             370 |  # node_InstanceNormalization_1352\n",
       "                    %\"val_1353\"<FLOAT16,[batch,32,9216]> ⬅️ ::InstanceNormalization(%\"val_1346\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             371 |  # node_Shape_1353\n",
       "                    %\"val_1354\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_22\") {start=0}\n",
       "             372 |  # node_Reshape_1354\n",
       "                    %\"val_1355\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1353\", %\"val_1354\") {allowzero=0}\n",
       "             373 |  # node_Mul_1361\n",
       "                    %\"val_1362\"<FLOAT16,[None,128,None,None]> ⬅️ ::Mul(%\"val_1355\", %\"val_1361\"{...})\n",
       "             374 |  # node_group_norm_22\n",
       "                    %\"group_norm_22\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Add(%\"val_1362\", %\"val_1363\"{...})\n",
       "             375 |  # node_relu_20\n",
       "                    %\"relu_20\"<FLOAT16,[batch,128,48,48]> ⬅️ ::Relu(%\"group_norm_22\")\n",
       "             376 |  # node_view_48\n",
       "                    %\"view_48\"<FLOAT16,[1,512,128]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.1.blocks.3.conv3.weight\"{...}, %\"val_682\"{[1, 512, -1]}) {allowzero=1}\n",
       "             377 |  # node_ReduceMean_1377\n",
       "                    %\"val_1378\"<FLOAT16,[512]> ⬅️ ::ReduceMean(%\"view_48\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             378 |  # node_ReduceMean_1378\n",
       "                    %\"val_1379\"<FLOAT16,[1,512,1]> ⬅️ ::ReduceMean(%\"view_48\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             379 |  # node_Sub_1379\n",
       "                    %\"val_1380\"<FLOAT16,[1,512,128]> ⬅️ ::Sub(%\"view_48\", %\"val_1379\")\n",
       "             380 |  # node_Mul_1380\n",
       "                    %\"val_1381\"<FLOAT16,[1,512,128]> ⬅️ ::Mul(%\"val_1380\", %\"val_1380\")\n",
       "             381 |  # node_ReduceMean_1381\n",
       "                    %\"val_1382\"<FLOAT16,[512]> ⬅️ ::ReduceMean(%\"val_1381\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             382 |  # node__native_batch_norm_legit_23__0\n",
       "                    %\"getitem_69\"<FLOAT16,[1,512,128]> ⬅️ ::BatchNormalization(%\"view_48\", %\"val_686\"{...}, %\"val_690\"{...}, %\"val_1378\", %\"val_1382\") {momentum=1.0, epsilon=1e-08}\n",
       "             383 |  # node_view_49\n",
       "                    %\"view_49\"<FLOAT16,[512,128,1,1]> ⬅️ ::Reshape(%\"getitem_69\", %\"val_880\"{[512, 128, 1, 1]}) {allowzero=1}\n",
       "             384 |  # node_Conv_4687\n",
       "                    %\"conv2d_23\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Conv(%\"relu_20\", %\"view_49\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             385 |  # node_Reshape_1402\n",
       "                    %\"val_1403\"<FLOAT16,[batch,32,36864]> ⬅️ ::Reshape(%\"conv2d_23\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             386 |  # node_InstanceNormalization_1409\n",
       "                    %\"val_1410\"<FLOAT16,[batch,32,36864]> ⬅️ ::InstanceNormalization(%\"val_1403\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             387 |  # node_Shape_1410\n",
       "                    %\"val_1411\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_23\") {start=0}\n",
       "             388 |  # node_Reshape_1411\n",
       "                    %\"val_1412\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1410\", %\"val_1411\") {allowzero=0}\n",
       "             389 |  # node_Mul_1418\n",
       "                    %\"val_1419\"<FLOAT16,[None,512,None,None]> ⬅️ ::Mul(%\"val_1412\", %\"val_1418\"{...})\n",
       "             390 |  # node_group_norm_23\n",
       "                    %\"group_norm_23\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Add(%\"val_1419\", %\"val_1420\"{...})\n",
       "             391 |  # node_add_511\n",
       "                    %\"add_511\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Add(%\"group_norm_23\", %\"relu_18\")\n",
       "             392 |  # node_relu_21\n",
       "                    %\"relu_21\"<FLOAT16,[batch,512,48,48]> ⬅️ ::Relu(%\"add_511\")\n",
       "             393 |  # node_view_50\n",
       "                    %\"view_50\"<FLOAT16,[1,1024,512]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.0.downsample.conv.weight\"{...}, %\"val_1427\"{[1, 1024, -1]}) {allowzero=1}\n",
       "             394 |  # node_ReduceMean_1436\n",
       "                    %\"val_1437\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"view_50\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             395 |  # node_ReduceMean_1437\n",
       "                    %\"val_1438\"<FLOAT16,[1,1024,1]> ⬅️ ::ReduceMean(%\"view_50\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             396 |  # node_Sub_1438\n",
       "                    %\"val_1439\"<FLOAT16,[1,1024,512]> ⬅️ ::Sub(%\"view_50\", %\"val_1438\")\n",
       "             397 |  # node_Mul_1439\n",
       "                    %\"val_1440\"<FLOAT16,[1,1024,512]> ⬅️ ::Mul(%\"val_1439\", %\"val_1439\")\n",
       "             398 |  # node_ReduceMean_1440\n",
       "                    %\"val_1441\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"val_1440\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             399 |  # node__native_batch_norm_legit_24__0\n",
       "                    %\"getitem_72\"<FLOAT16,[1,1024,512]> ⬅️ ::BatchNormalization(%\"view_50\", %\"val_1431\"{...}, %\"val_1435\"{...}, %\"val_1437\", %\"val_1441\") {momentum=1.0, epsilon=1e-08}\n",
       "             400 |  # node_view_51\n",
       "                    %\"view_51\"<FLOAT16,[1024,512,1,1]> ⬅️ ::Reshape(%\"getitem_72\", %\"val_1452\"{[1024, 512, 1, 1]}) {allowzero=1}\n",
       "             401 |  # node_Conv_4688\n",
       "                    %\"conv2d_24\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Conv(%\"relu_21\", %\"view_51\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "             402 |  # node_Reshape_1461\n",
       "                    %\"val_1462\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_24\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             403 |  # node_InstanceNormalization_1468\n",
       "                    %\"val_1469\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_1462\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             404 |  # node_Shape_1469\n",
       "                    %\"val_1470\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_24\") {start=0}\n",
       "             405 |  # node_Reshape_1470\n",
       "                    %\"val_1471\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1469\", %\"val_1470\") {allowzero=0}\n",
       "             406 |  # node_Mul_1477\n",
       "                    %\"val_1478\"<FLOAT16,[None,1024,None,None]> ⬅️ ::Mul(%\"val_1471\", %\"val_1477\"{...})\n",
       "             407 |  # node_group_norm_24\n",
       "                    %\"group_norm_24\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"val_1478\", %\"val_1479\"{...})\n",
       "             408 |  # node_view_52\n",
       "                    %\"view_52\"<FLOAT16,[1,256,512]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.0.conv1.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             409 |  # node_ReduceMean_1493\n",
       "                    %\"val_1494\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_52\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             410 |  # node_ReduceMean_1494\n",
       "                    %\"val_1495\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_52\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             411 |  # node_Sub_1495\n",
       "                    %\"val_1496\"<FLOAT16,[1,256,512]> ⬅️ ::Sub(%\"view_52\", %\"val_1495\")\n",
       "             412 |  # node_Mul_1496\n",
       "                    %\"val_1497\"<FLOAT16,[1,256,512]> ⬅️ ::Mul(%\"val_1496\", %\"val_1496\")\n",
       "             413 |  # node_ReduceMean_1497\n",
       "                    %\"val_1498\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_1497\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             414 |  # node__native_batch_norm_legit_25__0\n",
       "                    %\"getitem_75\"<FLOAT16,[1,256,512]> ⬅️ ::BatchNormalization(%\"view_52\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_1494\", %\"val_1498\") {momentum=1.0, epsilon=1e-08}\n",
       "             415 |  # node_view_53\n",
       "                    %\"view_53\"<FLOAT16,[256,512,1,1]> ⬅️ ::Reshape(%\"getitem_75\", %\"val_1509\"{[256, 512, 1, 1]}) {allowzero=1}\n",
       "             416 |  # node_Conv_4689\n",
       "                    %\"conv2d_25\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Conv(%\"relu_21\", %\"view_53\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             417 |  # node_Reshape_1518\n",
       "                    %\"val_1519\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_25\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             418 |  # node_InstanceNormalization_1525\n",
       "                    %\"val_1526\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_1519\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             419 |  # node_Shape_1526\n",
       "                    %\"val_1527\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_25\") {start=0}\n",
       "             420 |  # node_Reshape_1527\n",
       "                    %\"val_1528\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1526\", %\"val_1527\") {allowzero=0}\n",
       "             421 |  # node_Mul_1534\n",
       "                    %\"val_1535\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_1528\", %\"val_1534\"{...})\n",
       "             422 |  # node_group_norm_25\n",
       "                    %\"group_norm_25\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Add(%\"val_1535\", %\"val_1536\"{...})\n",
       "             423 |  # node_relu_22\n",
       "                    %\"relu_22\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Relu(%\"group_norm_25\")\n",
       "             424 |  # node_view_54\n",
       "                    %\"view_54\"<FLOAT16,[1,256,2304]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.0.conv2.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             425 |  # node_ReduceMean_1552\n",
       "                    %\"val_1553\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_54\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             426 |  # node_ReduceMean_1553\n",
       "                    %\"val_1554\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_54\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             427 |  # node_Sub_1554\n",
       "                    %\"val_1555\"<FLOAT16,[1,256,2304]> ⬅️ ::Sub(%\"view_54\", %\"val_1554\")\n",
       "             428 |  # node_Mul_1555\n",
       "                    %\"val_1556\"<FLOAT16,[1,256,2304]> ⬅️ ::Mul(%\"val_1555\", %\"val_1555\")\n",
       "             429 |  # node_ReduceMean_1556\n",
       "                    %\"val_1557\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_1556\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             430 |  # node__native_batch_norm_legit_26__0\n",
       "                    %\"getitem_78\"<FLOAT16,[1,256,2304]> ⬅️ ::BatchNormalization(%\"view_54\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_1553\", %\"val_1557\") {momentum=1.0, epsilon=1e-08}\n",
       "             431 |  # node_view_55\n",
       "                    %\"view_55\"<FLOAT16,[256,256,3,3]> ⬅️ ::Reshape(%\"getitem_78\", %\"val_1568\"{[256, 256, 3, 3]}) {allowzero=1}\n",
       "             432 |  # node_Conv_4690\n",
       "                    %\"conv2d_26\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_22\", %\"view_55\") {group=1, pads=(0, 0, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "             433 |  # node_Reshape_1577\n",
       "                    %\"val_1578\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_26\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             434 |  # node_InstanceNormalization_1584\n",
       "                    %\"val_1585\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_1578\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             435 |  # node_Shape_1585\n",
       "                    %\"val_1586\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_26\") {start=0}\n",
       "             436 |  # node_Reshape_1586\n",
       "                    %\"val_1587\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1585\", %\"val_1586\") {allowzero=0}\n",
       "             437 |  # node_Mul_1593\n",
       "                    %\"val_1594\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_1587\", %\"val_1593\"{...})\n",
       "             438 |  # node_group_norm_26\n",
       "                    %\"group_norm_26\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_1594\", %\"val_1595\"{...})\n",
       "             439 |  # node_relu_23\n",
       "                    %\"relu_23\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_26\")\n",
       "             440 |  # node_view_56\n",
       "                    %\"view_56\"<FLOAT16,[1,1024,256]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.0.conv3.weight\"{...}, %\"val_1427\"{[1, 1024, -1]}) {allowzero=1}\n",
       "             441 |  # node_ReduceMean_1609\n",
       "                    %\"val_1610\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"view_56\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             442 |  # node_ReduceMean_1610\n",
       "                    %\"val_1611\"<FLOAT16,[1,1024,1]> ⬅️ ::ReduceMean(%\"view_56\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             443 |  # node_Sub_1611\n",
       "                    %\"val_1612\"<FLOAT16,[1,1024,256]> ⬅️ ::Sub(%\"view_56\", %\"val_1611\")\n",
       "             444 |  # node_Mul_1612\n",
       "                    %\"val_1613\"<FLOAT16,[1,1024,256]> ⬅️ ::Mul(%\"val_1612\", %\"val_1612\")\n",
       "             445 |  # node_ReduceMean_1613\n",
       "                    %\"val_1614\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"val_1613\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             446 |  # node__native_batch_norm_legit_27__0\n",
       "                    %\"getitem_81\"<FLOAT16,[1,1024,256]> ⬅️ ::BatchNormalization(%\"view_56\", %\"val_1431\"{...}, %\"val_1435\"{...}, %\"val_1610\", %\"val_1614\") {momentum=1.0, epsilon=1e-08}\n",
       "             447 |  # node_view_57\n",
       "                    %\"view_57\"<FLOAT16,[1024,256,1,1]> ⬅️ ::Reshape(%\"getitem_81\", %\"val_1625\"{[1024, 256, 1, 1]}) {allowzero=1}\n",
       "             448 |  # node_Conv_4691\n",
       "                    %\"conv2d_27\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Conv(%\"relu_23\", %\"view_57\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             449 |  # node_Reshape_1634\n",
       "                    %\"val_1635\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_27\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             450 |  # node_InstanceNormalization_1641\n",
       "                    %\"val_1642\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_1635\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             451 |  # node_Shape_1642\n",
       "                    %\"val_1643\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_27\") {start=0}\n",
       "             452 |  # node_Reshape_1643\n",
       "                    %\"val_1644\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1642\", %\"val_1643\") {allowzero=0}\n",
       "             453 |  # node_Mul_1650\n",
       "                    %\"val_1651\"<FLOAT16,[None,1024,None,None]> ⬅️ ::Mul(%\"val_1644\", %\"val_1650\"{...})\n",
       "             454 |  # node_group_norm_27\n",
       "                    %\"group_norm_27\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"val_1651\", %\"val_1652\"{...})\n",
       "             455 |  # node_add_597\n",
       "                    %\"add_597\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"group_norm_27\", %\"group_norm_24\")\n",
       "             456 |  # node_relu_24\n",
       "                    %\"relu_24\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Relu(%\"add_597\")\n",
       "             457 |  # node_view_58\n",
       "                    %\"view_58\"<FLOAT16,[1,256,1024]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.1.conv1.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             458 |  # node_ReduceMean_1666\n",
       "                    %\"val_1667\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_58\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             459 |  # node_ReduceMean_1667\n",
       "                    %\"val_1668\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_58\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             460 |  # node_Sub_1668\n",
       "                    %\"val_1669\"<FLOAT16,[1,256,1024]> ⬅️ ::Sub(%\"view_58\", %\"val_1668\")\n",
       "             461 |  # node_Mul_1669\n",
       "                    %\"val_1670\"<FLOAT16,[1,256,1024]> ⬅️ ::Mul(%\"val_1669\", %\"val_1669\")\n",
       "             462 |  # node_ReduceMean_1670\n",
       "                    %\"val_1671\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_1670\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             463 |  # node__native_batch_norm_legit_28__0\n",
       "                    %\"getitem_84\"<FLOAT16,[1,256,1024]> ⬅️ ::BatchNormalization(%\"view_58\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_1667\", %\"val_1671\") {momentum=1.0, epsilon=1e-08}\n",
       "             464 |  # node_view_59\n",
       "                    %\"view_59\"<FLOAT16,[256,1024,1,1]> ⬅️ ::Reshape(%\"getitem_84\", %\"val_1682\"{[256, 1024, 1, 1]}) {allowzero=1}\n",
       "             465 |  # node_Conv_4692\n",
       "                    %\"conv2d_28\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_24\", %\"view_59\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             466 |  # node_Reshape_1691\n",
       "                    %\"val_1692\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_28\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             467 |  # node_InstanceNormalization_1698\n",
       "                    %\"val_1699\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_1692\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             468 |  # node_Shape_1699\n",
       "                    %\"val_1700\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_28\") {start=0}\n",
       "             469 |  # node_Reshape_1700\n",
       "                    %\"val_1701\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1699\", %\"val_1700\") {allowzero=0}\n",
       "             470 |  # node_Mul_1707\n",
       "                    %\"val_1708\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_1701\", %\"val_1707\"{...})\n",
       "             471 |  # node_group_norm_28\n",
       "                    %\"group_norm_28\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_1708\", %\"val_1709\"{...})\n",
       "             472 |  # node_relu_25\n",
       "                    %\"relu_25\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_28\")\n",
       "             473 |  # node_view_60\n",
       "                    %\"view_60\"<FLOAT16,[1,256,2304]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.1.conv2.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             474 |  # node_ReduceMean_1723\n",
       "                    %\"val_1724\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_60\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             475 |  # node_ReduceMean_1724\n",
       "                    %\"val_1725\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_60\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             476 |  # node_Sub_1725\n",
       "                    %\"val_1726\"<FLOAT16,[1,256,2304]> ⬅️ ::Sub(%\"view_60\", %\"val_1725\")\n",
       "             477 |  # node_Mul_1726\n",
       "                    %\"val_1727\"<FLOAT16,[1,256,2304]> ⬅️ ::Mul(%\"val_1726\", %\"val_1726\")\n",
       "             478 |  # node_ReduceMean_1727\n",
       "                    %\"val_1728\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_1727\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             479 |  # node__native_batch_norm_legit_29__0\n",
       "                    %\"getitem_87\"<FLOAT16,[1,256,2304]> ⬅️ ::BatchNormalization(%\"view_60\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_1724\", %\"val_1728\") {momentum=1.0, epsilon=1e-08}\n",
       "             480 |  # node_view_61\n",
       "                    %\"view_61\"<FLOAT16,[256,256,3,3]> ⬅️ ::Reshape(%\"getitem_87\", %\"val_1568\"{[256, 256, 3, 3]}) {allowzero=1}\n",
       "             481 |  # node_Conv_4693\n",
       "                    %\"conv2d_29\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_25\", %\"view_61\") {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             482 |  # node_Reshape_1748\n",
       "                    %\"val_1749\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_29\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             483 |  # node_InstanceNormalization_1755\n",
       "                    %\"val_1756\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_1749\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             484 |  # node_Shape_1756\n",
       "                    %\"val_1757\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_29\") {start=0}\n",
       "             485 |  # node_Reshape_1757\n",
       "                    %\"val_1758\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1756\", %\"val_1757\") {allowzero=0}\n",
       "             486 |  # node_Mul_1764\n",
       "                    %\"val_1765\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_1758\", %\"val_1764\"{...})\n",
       "             487 |  # node_group_norm_29\n",
       "                    %\"group_norm_29\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_1765\", %\"val_1766\"{...})\n",
       "             488 |  # node_relu_26\n",
       "                    %\"relu_26\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_29\")\n",
       "             489 |  # node_view_62\n",
       "                    %\"view_62\"<FLOAT16,[1,1024,256]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.1.conv3.weight\"{...}, %\"val_1427\"{[1, 1024, -1]}) {allowzero=1}\n",
       "             490 |  # node_ReduceMean_1780\n",
       "                    %\"val_1781\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"view_62\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             491 |  # node_ReduceMean_1781\n",
       "                    %\"val_1782\"<FLOAT16,[1,1024,1]> ⬅️ ::ReduceMean(%\"view_62\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             492 |  # node_Sub_1782\n",
       "                    %\"val_1783\"<FLOAT16,[1,1024,256]> ⬅️ ::Sub(%\"view_62\", %\"val_1782\")\n",
       "             493 |  # node_Mul_1783\n",
       "                    %\"val_1784\"<FLOAT16,[1,1024,256]> ⬅️ ::Mul(%\"val_1783\", %\"val_1783\")\n",
       "             494 |  # node_ReduceMean_1784\n",
       "                    %\"val_1785\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"val_1784\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             495 |  # node__native_batch_norm_legit_30__0\n",
       "                    %\"getitem_90\"<FLOAT16,[1,1024,256]> ⬅️ ::BatchNormalization(%\"view_62\", %\"val_1431\"{...}, %\"val_1435\"{...}, %\"val_1781\", %\"val_1785\") {momentum=1.0, epsilon=1e-08}\n",
       "             496 |  # node_view_63\n",
       "                    %\"view_63\"<FLOAT16,[1024,256,1,1]> ⬅️ ::Reshape(%\"getitem_90\", %\"val_1625\"{[1024, 256, 1, 1]}) {allowzero=1}\n",
       "             497 |  # node_Conv_4694\n",
       "                    %\"conv2d_30\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Conv(%\"relu_26\", %\"view_63\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             498 |  # node_Reshape_1805\n",
       "                    %\"val_1806\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_30\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             499 |  # node_InstanceNormalization_1812\n",
       "                    %\"val_1813\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_1806\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             500 |  # node_Shape_1813\n",
       "                    %\"val_1814\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_30\") {start=0}\n",
       "             501 |  # node_Reshape_1814\n",
       "                    %\"val_1815\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1813\", %\"val_1814\") {allowzero=0}\n",
       "             502 |  # node_Mul_1821\n",
       "                    %\"val_1822\"<FLOAT16,[None,1024,None,None]> ⬅️ ::Mul(%\"val_1815\", %\"val_1821\"{...})\n",
       "             503 |  # node_group_norm_30\n",
       "                    %\"group_norm_30\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"val_1822\", %\"val_1823\"{...})\n",
       "             504 |  # node_add_663\n",
       "                    %\"add_663\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"group_norm_30\", %\"relu_24\")\n",
       "             505 |  # node_relu_27\n",
       "                    %\"relu_27\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Relu(%\"add_663\")\n",
       "             506 |  # node_view_64\n",
       "                    %\"view_64\"<FLOAT16,[1,256,1024]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.2.conv1.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             507 |  # node_ReduceMean_1837\n",
       "                    %\"val_1838\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_64\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             508 |  # node_ReduceMean_1838\n",
       "                    %\"val_1839\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_64\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             509 |  # node_Sub_1839\n",
       "                    %\"val_1840\"<FLOAT16,[1,256,1024]> ⬅️ ::Sub(%\"view_64\", %\"val_1839\")\n",
       "             510 |  # node_Mul_1840\n",
       "                    %\"val_1841\"<FLOAT16,[1,256,1024]> ⬅️ ::Mul(%\"val_1840\", %\"val_1840\")\n",
       "             511 |  # node_ReduceMean_1841\n",
       "                    %\"val_1842\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_1841\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             512 |  # node__native_batch_norm_legit_31__0\n",
       "                    %\"getitem_93\"<FLOAT16,[1,256,1024]> ⬅️ ::BatchNormalization(%\"view_64\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_1838\", %\"val_1842\") {momentum=1.0, epsilon=1e-08}\n",
       "             513 |  # node_view_65\n",
       "                    %\"view_65\"<FLOAT16,[256,1024,1,1]> ⬅️ ::Reshape(%\"getitem_93\", %\"val_1682\"{[256, 1024, 1, 1]}) {allowzero=1}\n",
       "             514 |  # node_Conv_4695\n",
       "                    %\"conv2d_31\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_27\", %\"view_65\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             515 |  # node_Reshape_1862\n",
       "                    %\"val_1863\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_31\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             516 |  # node_InstanceNormalization_1869\n",
       "                    %\"val_1870\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_1863\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             517 |  # node_Shape_1870\n",
       "                    %\"val_1871\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_31\") {start=0}\n",
       "             518 |  # node_Reshape_1871\n",
       "                    %\"val_1872\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1870\", %\"val_1871\") {allowzero=0}\n",
       "             519 |  # node_Mul_1878\n",
       "                    %\"val_1879\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_1872\", %\"val_1878\"{...})\n",
       "             520 |  # node_group_norm_31\n",
       "                    %\"group_norm_31\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_1879\", %\"val_1880\"{...})\n",
       "             521 |  # node_relu_28\n",
       "                    %\"relu_28\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_31\")\n",
       "             522 |  # node_view_66\n",
       "                    %\"view_66\"<FLOAT16,[1,256,2304]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.2.conv2.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             523 |  # node_ReduceMean_1894\n",
       "                    %\"val_1895\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_66\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             524 |  # node_ReduceMean_1895\n",
       "                    %\"val_1896\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_66\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             525 |  # node_Sub_1896\n",
       "                    %\"val_1897\"<FLOAT16,[1,256,2304]> ⬅️ ::Sub(%\"view_66\", %\"val_1896\")\n",
       "             526 |  # node_Mul_1897\n",
       "                    %\"val_1898\"<FLOAT16,[1,256,2304]> ⬅️ ::Mul(%\"val_1897\", %\"val_1897\")\n",
       "             527 |  # node_ReduceMean_1898\n",
       "                    %\"val_1899\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_1898\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             528 |  # node__native_batch_norm_legit_32__0\n",
       "                    %\"getitem_96\"<FLOAT16,[1,256,2304]> ⬅️ ::BatchNormalization(%\"view_66\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_1895\", %\"val_1899\") {momentum=1.0, epsilon=1e-08}\n",
       "             529 |  # node_view_67\n",
       "                    %\"view_67\"<FLOAT16,[256,256,3,3]> ⬅️ ::Reshape(%\"getitem_96\", %\"val_1568\"{[256, 256, 3, 3]}) {allowzero=1}\n",
       "             530 |  # node_Conv_4696\n",
       "                    %\"conv2d_32\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_28\", %\"view_67\") {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             531 |  # node_Reshape_1919\n",
       "                    %\"val_1920\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_32\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             532 |  # node_InstanceNormalization_1926\n",
       "                    %\"val_1927\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_1920\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             533 |  # node_Shape_1927\n",
       "                    %\"val_1928\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_32\") {start=0}\n",
       "             534 |  # node_Reshape_1928\n",
       "                    %\"val_1929\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1927\", %\"val_1928\") {allowzero=0}\n",
       "             535 |  # node_Mul_1935\n",
       "                    %\"val_1936\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_1929\", %\"val_1935\"{...})\n",
       "             536 |  # node_group_norm_32\n",
       "                    %\"group_norm_32\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_1936\", %\"val_1937\"{...})\n",
       "             537 |  # node_relu_29\n",
       "                    %\"relu_29\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_32\")\n",
       "             538 |  # node_view_68\n",
       "                    %\"view_68\"<FLOAT16,[1,1024,256]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.2.conv3.weight\"{...}, %\"val_1427\"{[1, 1024, -1]}) {allowzero=1}\n",
       "             539 |  # node_ReduceMean_1951\n",
       "                    %\"val_1952\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"view_68\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             540 |  # node_ReduceMean_1952\n",
       "                    %\"val_1953\"<FLOAT16,[1,1024,1]> ⬅️ ::ReduceMean(%\"view_68\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             541 |  # node_Sub_1953\n",
       "                    %\"val_1954\"<FLOAT16,[1,1024,256]> ⬅️ ::Sub(%\"view_68\", %\"val_1953\")\n",
       "             542 |  # node_Mul_1954\n",
       "                    %\"val_1955\"<FLOAT16,[1,1024,256]> ⬅️ ::Mul(%\"val_1954\", %\"val_1954\")\n",
       "             543 |  # node_ReduceMean_1955\n",
       "                    %\"val_1956\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"val_1955\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             544 |  # node__native_batch_norm_legit_33__0\n",
       "                    %\"getitem_99\"<FLOAT16,[1,1024,256]> ⬅️ ::BatchNormalization(%\"view_68\", %\"val_1431\"{...}, %\"val_1435\"{...}, %\"val_1952\", %\"val_1956\") {momentum=1.0, epsilon=1e-08}\n",
       "             545 |  # node_view_69\n",
       "                    %\"view_69\"<FLOAT16,[1024,256,1,1]> ⬅️ ::Reshape(%\"getitem_99\", %\"val_1625\"{[1024, 256, 1, 1]}) {allowzero=1}\n",
       "             546 |  # node_Conv_4697\n",
       "                    %\"conv2d_33\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Conv(%\"relu_29\", %\"view_69\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             547 |  # node_Reshape_1976\n",
       "                    %\"val_1977\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_33\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             548 |  # node_InstanceNormalization_1983\n",
       "                    %\"val_1984\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_1977\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             549 |  # node_Shape_1984\n",
       "                    %\"val_1985\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_33\") {start=0}\n",
       "             550 |  # node_Reshape_1985\n",
       "                    %\"val_1986\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_1984\", %\"val_1985\") {allowzero=0}\n",
       "             551 |  # node_Mul_1992\n",
       "                    %\"val_1993\"<FLOAT16,[None,1024,None,None]> ⬅️ ::Mul(%\"val_1986\", %\"val_1992\"{...})\n",
       "             552 |  # node_group_norm_33\n",
       "                    %\"group_norm_33\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"val_1993\", %\"val_1994\"{...})\n",
       "             553 |  # node_add_729\n",
       "                    %\"add_729\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"group_norm_33\", %\"relu_27\")\n",
       "             554 |  # node_relu_30\n",
       "                    %\"relu_30\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Relu(%\"add_729\")\n",
       "             555 |  # node_view_70\n",
       "                    %\"view_70\"<FLOAT16,[1,256,1024]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.3.conv1.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             556 |  # node_ReduceMean_2008\n",
       "                    %\"val_2009\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_70\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             557 |  # node_ReduceMean_2009\n",
       "                    %\"val_2010\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_70\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             558 |  # node_Sub_2010\n",
       "                    %\"val_2011\"<FLOAT16,[1,256,1024]> ⬅️ ::Sub(%\"view_70\", %\"val_2010\")\n",
       "             559 |  # node_Mul_2011\n",
       "                    %\"val_2012\"<FLOAT16,[1,256,1024]> ⬅️ ::Mul(%\"val_2011\", %\"val_2011\")\n",
       "             560 |  # node_ReduceMean_2012\n",
       "                    %\"val_2013\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_2012\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             561 |  # node__native_batch_norm_legit_34__0\n",
       "                    %\"getitem_102\"<FLOAT16,[1,256,1024]> ⬅️ ::BatchNormalization(%\"view_70\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_2009\", %\"val_2013\") {momentum=1.0, epsilon=1e-08}\n",
       "             562 |  # node_view_71\n",
       "                    %\"view_71\"<FLOAT16,[256,1024,1,1]> ⬅️ ::Reshape(%\"getitem_102\", %\"val_1682\"{[256, 1024, 1, 1]}) {allowzero=1}\n",
       "             563 |  # node_Conv_4698\n",
       "                    %\"conv2d_34\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_30\", %\"view_71\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             564 |  # node_Reshape_2033\n",
       "                    %\"val_2034\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_34\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             565 |  # node_InstanceNormalization_2040\n",
       "                    %\"val_2041\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_2034\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             566 |  # node_Shape_2041\n",
       "                    %\"val_2042\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_34\") {start=0}\n",
       "             567 |  # node_Reshape_2042\n",
       "                    %\"val_2043\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2041\", %\"val_2042\") {allowzero=0}\n",
       "             568 |  # node_Mul_2049\n",
       "                    %\"val_2050\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_2043\", %\"val_2049\"{...})\n",
       "             569 |  # node_group_norm_34\n",
       "                    %\"group_norm_34\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_2050\", %\"val_2051\"{...})\n",
       "             570 |  # node_relu_31\n",
       "                    %\"relu_31\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_34\")\n",
       "             571 |  # node_view_72\n",
       "                    %\"view_72\"<FLOAT16,[1,256,2304]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.3.conv2.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             572 |  # node_ReduceMean_2065\n",
       "                    %\"val_2066\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_72\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             573 |  # node_ReduceMean_2066\n",
       "                    %\"val_2067\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_72\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             574 |  # node_Sub_2067\n",
       "                    %\"val_2068\"<FLOAT16,[1,256,2304]> ⬅️ ::Sub(%\"view_72\", %\"val_2067\")\n",
       "             575 |  # node_Mul_2068\n",
       "                    %\"val_2069\"<FLOAT16,[1,256,2304]> ⬅️ ::Mul(%\"val_2068\", %\"val_2068\")\n",
       "             576 |  # node_ReduceMean_2069\n",
       "                    %\"val_2070\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_2069\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             577 |  # node__native_batch_norm_legit_35__0\n",
       "                    %\"getitem_105\"<FLOAT16,[1,256,2304]> ⬅️ ::BatchNormalization(%\"view_72\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_2066\", %\"val_2070\") {momentum=1.0, epsilon=1e-08}\n",
       "             578 |  # node_view_73\n",
       "                    %\"view_73\"<FLOAT16,[256,256,3,3]> ⬅️ ::Reshape(%\"getitem_105\", %\"val_1568\"{[256, 256, 3, 3]}) {allowzero=1}\n",
       "             579 |  # node_Conv_4699\n",
       "                    %\"conv2d_35\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_31\", %\"view_73\") {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             580 |  # node_Reshape_2090\n",
       "                    %\"val_2091\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_35\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             581 |  # node_InstanceNormalization_2097\n",
       "                    %\"val_2098\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_2091\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             582 |  # node_Shape_2098\n",
       "                    %\"val_2099\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_35\") {start=0}\n",
       "             583 |  # node_Reshape_2099\n",
       "                    %\"val_2100\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2098\", %\"val_2099\") {allowzero=0}\n",
       "             584 |  # node_Mul_2106\n",
       "                    %\"val_2107\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_2100\", %\"val_2106\"{...})\n",
       "             585 |  # node_group_norm_35\n",
       "                    %\"group_norm_35\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_2107\", %\"val_2108\"{...})\n",
       "             586 |  # node_relu_32\n",
       "                    %\"relu_32\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_35\")\n",
       "             587 |  # node_view_74\n",
       "                    %\"view_74\"<FLOAT16,[1,1024,256]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.3.conv3.weight\"{...}, %\"val_1427\"{[1, 1024, -1]}) {allowzero=1}\n",
       "             588 |  # node_ReduceMean_2122\n",
       "                    %\"val_2123\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"view_74\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             589 |  # node_ReduceMean_2123\n",
       "                    %\"val_2124\"<FLOAT16,[1,1024,1]> ⬅️ ::ReduceMean(%\"view_74\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             590 |  # node_Sub_2124\n",
       "                    %\"val_2125\"<FLOAT16,[1,1024,256]> ⬅️ ::Sub(%\"view_74\", %\"val_2124\")\n",
       "             591 |  # node_Mul_2125\n",
       "                    %\"val_2126\"<FLOAT16,[1,1024,256]> ⬅️ ::Mul(%\"val_2125\", %\"val_2125\")\n",
       "             592 |  # node_ReduceMean_2126\n",
       "                    %\"val_2127\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"val_2126\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             593 |  # node__native_batch_norm_legit_36__0\n",
       "                    %\"getitem_108\"<FLOAT16,[1,1024,256]> ⬅️ ::BatchNormalization(%\"view_74\", %\"val_1431\"{...}, %\"val_1435\"{...}, %\"val_2123\", %\"val_2127\") {momentum=1.0, epsilon=1e-08}\n",
       "             594 |  # node_view_75\n",
       "                    %\"view_75\"<FLOAT16,[1024,256,1,1]> ⬅️ ::Reshape(%\"getitem_108\", %\"val_1625\"{[1024, 256, 1, 1]}) {allowzero=1}\n",
       "             595 |  # node_Conv_4700\n",
       "                    %\"conv2d_36\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Conv(%\"relu_32\", %\"view_75\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             596 |  # node_Reshape_2147\n",
       "                    %\"val_2148\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_36\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             597 |  # node_InstanceNormalization_2154\n",
       "                    %\"val_2155\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_2148\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             598 |  # node_Shape_2155\n",
       "                    %\"val_2156\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_36\") {start=0}\n",
       "             599 |  # node_Reshape_2156\n",
       "                    %\"val_2157\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2155\", %\"val_2156\") {allowzero=0}\n",
       "             600 |  # node_Mul_2163\n",
       "                    %\"val_2164\"<FLOAT16,[None,1024,None,None]> ⬅️ ::Mul(%\"val_2157\", %\"val_2163\"{...})\n",
       "             601 |  # node_group_norm_36\n",
       "                    %\"group_norm_36\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"val_2164\", %\"val_2165\"{...})\n",
       "             602 |  # node_add_795\n",
       "                    %\"add_795\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"group_norm_36\", %\"relu_30\")\n",
       "             603 |  # node_relu_33\n",
       "                    %\"relu_33\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Relu(%\"add_795\")\n",
       "             604 |  # node_view_76\n",
       "                    %\"view_76\"<FLOAT16,[1,256,1024]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.4.conv1.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             605 |  # node_ReduceMean_2179\n",
       "                    %\"val_2180\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_76\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             606 |  # node_ReduceMean_2180\n",
       "                    %\"val_2181\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_76\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             607 |  # node_Sub_2181\n",
       "                    %\"val_2182\"<FLOAT16,[1,256,1024]> ⬅️ ::Sub(%\"view_76\", %\"val_2181\")\n",
       "             608 |  # node_Mul_2182\n",
       "                    %\"val_2183\"<FLOAT16,[1,256,1024]> ⬅️ ::Mul(%\"val_2182\", %\"val_2182\")\n",
       "             609 |  # node_ReduceMean_2183\n",
       "                    %\"val_2184\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_2183\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             610 |  # node__native_batch_norm_legit_37__0\n",
       "                    %\"getitem_111\"<FLOAT16,[1,256,1024]> ⬅️ ::BatchNormalization(%\"view_76\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_2180\", %\"val_2184\") {momentum=1.0, epsilon=1e-08}\n",
       "             611 |  # node_view_77\n",
       "                    %\"view_77\"<FLOAT16,[256,1024,1,1]> ⬅️ ::Reshape(%\"getitem_111\", %\"val_1682\"{[256, 1024, 1, 1]}) {allowzero=1}\n",
       "             612 |  # node_Conv_4701\n",
       "                    %\"conv2d_37\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_33\", %\"view_77\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             613 |  # node_Reshape_2204\n",
       "                    %\"val_2205\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_37\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             614 |  # node_InstanceNormalization_2211\n",
       "                    %\"val_2212\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_2205\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             615 |  # node_Shape_2212\n",
       "                    %\"val_2213\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_37\") {start=0}\n",
       "             616 |  # node_Reshape_2213\n",
       "                    %\"val_2214\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2212\", %\"val_2213\") {allowzero=0}\n",
       "             617 |  # node_Mul_2220\n",
       "                    %\"val_2221\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_2214\", %\"val_2220\"{...})\n",
       "             618 |  # node_group_norm_37\n",
       "                    %\"group_norm_37\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_2221\", %\"val_2222\"{...})\n",
       "             619 |  # node_relu_34\n",
       "                    %\"relu_34\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_37\")\n",
       "             620 |  # node_view_78\n",
       "                    %\"view_78\"<FLOAT16,[1,256,2304]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.4.conv2.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             621 |  # node_ReduceMean_2236\n",
       "                    %\"val_2237\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_78\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             622 |  # node_ReduceMean_2237\n",
       "                    %\"val_2238\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_78\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             623 |  # node_Sub_2238\n",
       "                    %\"val_2239\"<FLOAT16,[1,256,2304]> ⬅️ ::Sub(%\"view_78\", %\"val_2238\")\n",
       "             624 |  # node_Mul_2239\n",
       "                    %\"val_2240\"<FLOAT16,[1,256,2304]> ⬅️ ::Mul(%\"val_2239\", %\"val_2239\")\n",
       "             625 |  # node_ReduceMean_2240\n",
       "                    %\"val_2241\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_2240\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             626 |  # node__native_batch_norm_legit_38__0\n",
       "                    %\"getitem_114\"<FLOAT16,[1,256,2304]> ⬅️ ::BatchNormalization(%\"view_78\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_2237\", %\"val_2241\") {momentum=1.0, epsilon=1e-08}\n",
       "             627 |  # node_view_79\n",
       "                    %\"view_79\"<FLOAT16,[256,256,3,3]> ⬅️ ::Reshape(%\"getitem_114\", %\"val_1568\"{[256, 256, 3, 3]}) {allowzero=1}\n",
       "             628 |  # node_Conv_4702\n",
       "                    %\"conv2d_38\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_34\", %\"view_79\") {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             629 |  # node_Reshape_2261\n",
       "                    %\"val_2262\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_38\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             630 |  # node_InstanceNormalization_2268\n",
       "                    %\"val_2269\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_2262\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             631 |  # node_Shape_2269\n",
       "                    %\"val_2270\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_38\") {start=0}\n",
       "             632 |  # node_Reshape_2270\n",
       "                    %\"val_2271\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2269\", %\"val_2270\") {allowzero=0}\n",
       "             633 |  # node_Mul_2277\n",
       "                    %\"val_2278\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_2271\", %\"val_2277\"{...})\n",
       "             634 |  # node_group_norm_38\n",
       "                    %\"group_norm_38\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_2278\", %\"val_2279\"{...})\n",
       "             635 |  # node_relu_35\n",
       "                    %\"relu_35\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_38\")\n",
       "             636 |  # node_view_80\n",
       "                    %\"view_80\"<FLOAT16,[1,1024,256]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.4.conv3.weight\"{...}, %\"val_1427\"{[1, 1024, -1]}) {allowzero=1}\n",
       "             637 |  # node_ReduceMean_2293\n",
       "                    %\"val_2294\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"view_80\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             638 |  # node_ReduceMean_2294\n",
       "                    %\"val_2295\"<FLOAT16,[1,1024,1]> ⬅️ ::ReduceMean(%\"view_80\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             639 |  # node_Sub_2295\n",
       "                    %\"val_2296\"<FLOAT16,[1,1024,256]> ⬅️ ::Sub(%\"view_80\", %\"val_2295\")\n",
       "             640 |  # node_Mul_2296\n",
       "                    %\"val_2297\"<FLOAT16,[1,1024,256]> ⬅️ ::Mul(%\"val_2296\", %\"val_2296\")\n",
       "             641 |  # node_ReduceMean_2297\n",
       "                    %\"val_2298\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"val_2297\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             642 |  # node__native_batch_norm_legit_39__0\n",
       "                    %\"getitem_117\"<FLOAT16,[1,1024,256]> ⬅️ ::BatchNormalization(%\"view_80\", %\"val_1431\"{...}, %\"val_1435\"{...}, %\"val_2294\", %\"val_2298\") {momentum=1.0, epsilon=1e-08}\n",
       "             643 |  # node_view_81\n",
       "                    %\"view_81\"<FLOAT16,[1024,256,1,1]> ⬅️ ::Reshape(%\"getitem_117\", %\"val_1625\"{[1024, 256, 1, 1]}) {allowzero=1}\n",
       "             644 |  # node_Conv_4703\n",
       "                    %\"conv2d_39\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Conv(%\"relu_35\", %\"view_81\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             645 |  # node_Reshape_2318\n",
       "                    %\"val_2319\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_39\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             646 |  # node_InstanceNormalization_2325\n",
       "                    %\"val_2326\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_2319\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             647 |  # node_Shape_2326\n",
       "                    %\"val_2327\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_39\") {start=0}\n",
       "             648 |  # node_Reshape_2327\n",
       "                    %\"val_2328\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2326\", %\"val_2327\") {allowzero=0}\n",
       "             649 |  # node_Mul_2334\n",
       "                    %\"val_2335\"<FLOAT16,[None,1024,None,None]> ⬅️ ::Mul(%\"val_2328\", %\"val_2334\"{...})\n",
       "             650 |  # node_group_norm_39\n",
       "                    %\"group_norm_39\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"val_2335\", %\"val_2336\"{...})\n",
       "             651 |  # node_add_861\n",
       "                    %\"add_861\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"group_norm_39\", %\"relu_33\")\n",
       "             652 |  # node_relu_36\n",
       "                    %\"relu_36\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Relu(%\"add_861\")\n",
       "             653 |  # node_view_82\n",
       "                    %\"view_82\"<FLOAT16,[1,256,1024]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.5.conv1.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             654 |  # node_ReduceMean_2350\n",
       "                    %\"val_2351\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_82\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             655 |  # node_ReduceMean_2351\n",
       "                    %\"val_2352\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_82\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             656 |  # node_Sub_2352\n",
       "                    %\"val_2353\"<FLOAT16,[1,256,1024]> ⬅️ ::Sub(%\"view_82\", %\"val_2352\")\n",
       "             657 |  # node_Mul_2353\n",
       "                    %\"val_2354\"<FLOAT16,[1,256,1024]> ⬅️ ::Mul(%\"val_2353\", %\"val_2353\")\n",
       "             658 |  # node_ReduceMean_2354\n",
       "                    %\"val_2355\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_2354\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             659 |  # node__native_batch_norm_legit_40__0\n",
       "                    %\"getitem_120\"<FLOAT16,[1,256,1024]> ⬅️ ::BatchNormalization(%\"view_82\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_2351\", %\"val_2355\") {momentum=1.0, epsilon=1e-08}\n",
       "             660 |  # node_view_83\n",
       "                    %\"view_83\"<FLOAT16,[256,1024,1,1]> ⬅️ ::Reshape(%\"getitem_120\", %\"val_1682\"{[256, 1024, 1, 1]}) {allowzero=1}\n",
       "             661 |  # node_Conv_4704\n",
       "                    %\"conv2d_40\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_36\", %\"view_83\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             662 |  # node_Reshape_2375\n",
       "                    %\"val_2376\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_40\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             663 |  # node_InstanceNormalization_2382\n",
       "                    %\"val_2383\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_2376\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             664 |  # node_Shape_2383\n",
       "                    %\"val_2384\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_40\") {start=0}\n",
       "             665 |  # node_Reshape_2384\n",
       "                    %\"val_2385\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2383\", %\"val_2384\") {allowzero=0}\n",
       "             666 |  # node_Mul_2391\n",
       "                    %\"val_2392\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_2385\", %\"val_2391\"{...})\n",
       "             667 |  # node_group_norm_40\n",
       "                    %\"group_norm_40\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_2392\", %\"val_2393\"{...})\n",
       "             668 |  # node_relu_37\n",
       "                    %\"relu_37\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_40\")\n",
       "             669 |  # node_view_84\n",
       "                    %\"view_84\"<FLOAT16,[1,256,2304]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.5.conv2.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             670 |  # node_ReduceMean_2407\n",
       "                    %\"val_2408\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_84\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             671 |  # node_ReduceMean_2408\n",
       "                    %\"val_2409\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_84\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             672 |  # node_Sub_2409\n",
       "                    %\"val_2410\"<FLOAT16,[1,256,2304]> ⬅️ ::Sub(%\"view_84\", %\"val_2409\")\n",
       "             673 |  # node_Mul_2410\n",
       "                    %\"val_2411\"<FLOAT16,[1,256,2304]> ⬅️ ::Mul(%\"val_2410\", %\"val_2410\")\n",
       "             674 |  # node_ReduceMean_2411\n",
       "                    %\"val_2412\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_2411\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             675 |  # node__native_batch_norm_legit_41__0\n",
       "                    %\"getitem_123\"<FLOAT16,[1,256,2304]> ⬅️ ::BatchNormalization(%\"view_84\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_2408\", %\"val_2412\") {momentum=1.0, epsilon=1e-08}\n",
       "             676 |  # node_view_85\n",
       "                    %\"view_85\"<FLOAT16,[256,256,3,3]> ⬅️ ::Reshape(%\"getitem_123\", %\"val_1568\"{[256, 256, 3, 3]}) {allowzero=1}\n",
       "             677 |  # node_Conv_4705\n",
       "                    %\"conv2d_41\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_37\", %\"view_85\") {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             678 |  # node_Reshape_2432\n",
       "                    %\"val_2433\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_41\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             679 |  # node_InstanceNormalization_2439\n",
       "                    %\"val_2440\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_2433\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             680 |  # node_Shape_2440\n",
       "                    %\"val_2441\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_41\") {start=0}\n",
       "             681 |  # node_Reshape_2441\n",
       "                    %\"val_2442\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2440\", %\"val_2441\") {allowzero=0}\n",
       "             682 |  # node_Mul_2448\n",
       "                    %\"val_2449\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_2442\", %\"val_2448\"{...})\n",
       "             683 |  # node_group_norm_41\n",
       "                    %\"group_norm_41\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_2449\", %\"val_2450\"{...})\n",
       "             684 |  # node_relu_38\n",
       "                    %\"relu_38\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_41\")\n",
       "             685 |  # node_view_86\n",
       "                    %\"view_86\"<FLOAT16,[1,1024,256]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.5.conv3.weight\"{...}, %\"val_1427\"{[1, 1024, -1]}) {allowzero=1}\n",
       "             686 |  # node_ReduceMean_2464\n",
       "                    %\"val_2465\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"view_86\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             687 |  # node_ReduceMean_2465\n",
       "                    %\"val_2466\"<FLOAT16,[1,1024,1]> ⬅️ ::ReduceMean(%\"view_86\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             688 |  # node_Sub_2466\n",
       "                    %\"val_2467\"<FLOAT16,[1,1024,256]> ⬅️ ::Sub(%\"view_86\", %\"val_2466\")\n",
       "             689 |  # node_Mul_2467\n",
       "                    %\"val_2468\"<FLOAT16,[1,1024,256]> ⬅️ ::Mul(%\"val_2467\", %\"val_2467\")\n",
       "             690 |  # node_ReduceMean_2468\n",
       "                    %\"val_2469\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"val_2468\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             691 |  # node__native_batch_norm_legit_42__0\n",
       "                    %\"getitem_126\"<FLOAT16,[1,1024,256]> ⬅️ ::BatchNormalization(%\"view_86\", %\"val_1431\"{...}, %\"val_1435\"{...}, %\"val_2465\", %\"val_2469\") {momentum=1.0, epsilon=1e-08}\n",
       "             692 |  # node_view_87\n",
       "                    %\"view_87\"<FLOAT16,[1024,256,1,1]> ⬅️ ::Reshape(%\"getitem_126\", %\"val_1625\"{[1024, 256, 1, 1]}) {allowzero=1}\n",
       "             693 |  # node_Conv_4706\n",
       "                    %\"conv2d_42\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Conv(%\"relu_38\", %\"view_87\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             694 |  # node_Reshape_2489\n",
       "                    %\"val_2490\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_42\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             695 |  # node_InstanceNormalization_2496\n",
       "                    %\"val_2497\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_2490\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             696 |  # node_Shape_2497\n",
       "                    %\"val_2498\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_42\") {start=0}\n",
       "             697 |  # node_Reshape_2498\n",
       "                    %\"val_2499\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2497\", %\"val_2498\") {allowzero=0}\n",
       "             698 |  # node_Mul_2505\n",
       "                    %\"val_2506\"<FLOAT16,[None,1024,None,None]> ⬅️ ::Mul(%\"val_2499\", %\"val_2505\"{...})\n",
       "             699 |  # node_group_norm_42\n",
       "                    %\"group_norm_42\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"val_2506\", %\"val_2507\"{...})\n",
       "             700 |  # node_add_927\n",
       "                    %\"add_927\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"group_norm_42\", %\"relu_36\")\n",
       "             701 |  # node_relu_39\n",
       "                    %\"relu_39\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Relu(%\"add_927\")\n",
       "             702 |  # node_view_88\n",
       "                    %\"view_88\"<FLOAT16,[1,256,1024]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.6.conv1.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             703 |  # node_ReduceMean_2521\n",
       "                    %\"val_2522\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_88\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             704 |  # node_ReduceMean_2522\n",
       "                    %\"val_2523\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_88\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             705 |  # node_Sub_2523\n",
       "                    %\"val_2524\"<FLOAT16,[1,256,1024]> ⬅️ ::Sub(%\"view_88\", %\"val_2523\")\n",
       "             706 |  # node_Mul_2524\n",
       "                    %\"val_2525\"<FLOAT16,[1,256,1024]> ⬅️ ::Mul(%\"val_2524\", %\"val_2524\")\n",
       "             707 |  # node_ReduceMean_2525\n",
       "                    %\"val_2526\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_2525\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             708 |  # node__native_batch_norm_legit_43__0\n",
       "                    %\"getitem_129\"<FLOAT16,[1,256,1024]> ⬅️ ::BatchNormalization(%\"view_88\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_2522\", %\"val_2526\") {momentum=1.0, epsilon=1e-08}\n",
       "             709 |  # node_view_89\n",
       "                    %\"view_89\"<FLOAT16,[256,1024,1,1]> ⬅️ ::Reshape(%\"getitem_129\", %\"val_1682\"{[256, 1024, 1, 1]}) {allowzero=1}\n",
       "             710 |  # node_Conv_4707\n",
       "                    %\"conv2d_43\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_39\", %\"view_89\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             711 |  # node_Reshape_2546\n",
       "                    %\"val_2547\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_43\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             712 |  # node_InstanceNormalization_2553\n",
       "                    %\"val_2554\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_2547\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             713 |  # node_Shape_2554\n",
       "                    %\"val_2555\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_43\") {start=0}\n",
       "             714 |  # node_Reshape_2555\n",
       "                    %\"val_2556\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2554\", %\"val_2555\") {allowzero=0}\n",
       "             715 |  # node_Mul_2562\n",
       "                    %\"val_2563\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_2556\", %\"val_2562\"{...})\n",
       "             716 |  # node_group_norm_43\n",
       "                    %\"group_norm_43\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_2563\", %\"val_2564\"{...})\n",
       "             717 |  # node_relu_40\n",
       "                    %\"relu_40\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_43\")\n",
       "             718 |  # node_view_90\n",
       "                    %\"view_90\"<FLOAT16,[1,256,2304]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.6.conv2.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             719 |  # node_ReduceMean_2578\n",
       "                    %\"val_2579\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_90\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             720 |  # node_ReduceMean_2579\n",
       "                    %\"val_2580\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_90\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             721 |  # node_Sub_2580\n",
       "                    %\"val_2581\"<FLOAT16,[1,256,2304]> ⬅️ ::Sub(%\"view_90\", %\"val_2580\")\n",
       "             722 |  # node_Mul_2581\n",
       "                    %\"val_2582\"<FLOAT16,[1,256,2304]> ⬅️ ::Mul(%\"val_2581\", %\"val_2581\")\n",
       "             723 |  # node_ReduceMean_2582\n",
       "                    %\"val_2583\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_2582\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             724 |  # node__native_batch_norm_legit_44__0\n",
       "                    %\"getitem_132\"<FLOAT16,[1,256,2304]> ⬅️ ::BatchNormalization(%\"view_90\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_2579\", %\"val_2583\") {momentum=1.0, epsilon=1e-08}\n",
       "             725 |  # node_view_91\n",
       "                    %\"view_91\"<FLOAT16,[256,256,3,3]> ⬅️ ::Reshape(%\"getitem_132\", %\"val_1568\"{[256, 256, 3, 3]}) {allowzero=1}\n",
       "             726 |  # node_Conv_4708\n",
       "                    %\"conv2d_44\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_40\", %\"view_91\") {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             727 |  # node_Reshape_2603\n",
       "                    %\"val_2604\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_44\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             728 |  # node_InstanceNormalization_2610\n",
       "                    %\"val_2611\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_2604\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             729 |  # node_Shape_2611\n",
       "                    %\"val_2612\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_44\") {start=0}\n",
       "             730 |  # node_Reshape_2612\n",
       "                    %\"val_2613\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2611\", %\"val_2612\") {allowzero=0}\n",
       "             731 |  # node_Mul_2619\n",
       "                    %\"val_2620\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_2613\", %\"val_2619\"{...})\n",
       "             732 |  # node_group_norm_44\n",
       "                    %\"group_norm_44\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_2620\", %\"val_2621\"{...})\n",
       "             733 |  # node_relu_41\n",
       "                    %\"relu_41\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_44\")\n",
       "             734 |  # node_view_92\n",
       "                    %\"view_92\"<FLOAT16,[1,1024,256]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.6.conv3.weight\"{...}, %\"val_1427\"{[1, 1024, -1]}) {allowzero=1}\n",
       "             735 |  # node_ReduceMean_2635\n",
       "                    %\"val_2636\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"view_92\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             736 |  # node_ReduceMean_2636\n",
       "                    %\"val_2637\"<FLOAT16,[1,1024,1]> ⬅️ ::ReduceMean(%\"view_92\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             737 |  # node_Sub_2637\n",
       "                    %\"val_2638\"<FLOAT16,[1,1024,256]> ⬅️ ::Sub(%\"view_92\", %\"val_2637\")\n",
       "             738 |  # node_Mul_2638\n",
       "                    %\"val_2639\"<FLOAT16,[1,1024,256]> ⬅️ ::Mul(%\"val_2638\", %\"val_2638\")\n",
       "             739 |  # node_ReduceMean_2639\n",
       "                    %\"val_2640\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"val_2639\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             740 |  # node__native_batch_norm_legit_45__0\n",
       "                    %\"getitem_135\"<FLOAT16,[1,1024,256]> ⬅️ ::BatchNormalization(%\"view_92\", %\"val_1431\"{...}, %\"val_1435\"{...}, %\"val_2636\", %\"val_2640\") {momentum=1.0, epsilon=1e-08}\n",
       "             741 |  # node_view_93\n",
       "                    %\"view_93\"<FLOAT16,[1024,256,1,1]> ⬅️ ::Reshape(%\"getitem_135\", %\"val_1625\"{[1024, 256, 1, 1]}) {allowzero=1}\n",
       "             742 |  # node_Conv_4709\n",
       "                    %\"conv2d_45\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Conv(%\"relu_41\", %\"view_93\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             743 |  # node_Reshape_2660\n",
       "                    %\"val_2661\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_45\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             744 |  # node_InstanceNormalization_2667\n",
       "                    %\"val_2668\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_2661\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             745 |  # node_Shape_2668\n",
       "                    %\"val_2669\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_45\") {start=0}\n",
       "             746 |  # node_Reshape_2669\n",
       "                    %\"val_2670\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2668\", %\"val_2669\") {allowzero=0}\n",
       "             747 |  # node_Mul_2676\n",
       "                    %\"val_2677\"<FLOAT16,[None,1024,None,None]> ⬅️ ::Mul(%\"val_2670\", %\"val_2676\"{...})\n",
       "             748 |  # node_group_norm_45\n",
       "                    %\"group_norm_45\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"val_2677\", %\"val_2678\"{...})\n",
       "             749 |  # node_add_993\n",
       "                    %\"add_993\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"group_norm_45\", %\"relu_39\")\n",
       "             750 |  # node_relu_42\n",
       "                    %\"relu_42\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Relu(%\"add_993\")\n",
       "             751 |  # node_view_94\n",
       "                    %\"view_94\"<FLOAT16,[1,256,1024]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.7.conv1.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             752 |  # node_ReduceMean_2692\n",
       "                    %\"val_2693\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_94\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             753 |  # node_ReduceMean_2693\n",
       "                    %\"val_2694\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_94\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             754 |  # node_Sub_2694\n",
       "                    %\"val_2695\"<FLOAT16,[1,256,1024]> ⬅️ ::Sub(%\"view_94\", %\"val_2694\")\n",
       "             755 |  # node_Mul_2695\n",
       "                    %\"val_2696\"<FLOAT16,[1,256,1024]> ⬅️ ::Mul(%\"val_2695\", %\"val_2695\")\n",
       "             756 |  # node_ReduceMean_2696\n",
       "                    %\"val_2697\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_2696\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             757 |  # node__native_batch_norm_legit_46__0\n",
       "                    %\"getitem_138\"<FLOAT16,[1,256,1024]> ⬅️ ::BatchNormalization(%\"view_94\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_2693\", %\"val_2697\") {momentum=1.0, epsilon=1e-08}\n",
       "             758 |  # node_view_95\n",
       "                    %\"view_95\"<FLOAT16,[256,1024,1,1]> ⬅️ ::Reshape(%\"getitem_138\", %\"val_1682\"{[256, 1024, 1, 1]}) {allowzero=1}\n",
       "             759 |  # node_Conv_4710\n",
       "                    %\"conv2d_46\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_42\", %\"view_95\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             760 |  # node_Reshape_2717\n",
       "                    %\"val_2718\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_46\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             761 |  # node_InstanceNormalization_2724\n",
       "                    %\"val_2725\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_2718\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             762 |  # node_Shape_2725\n",
       "                    %\"val_2726\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_46\") {start=0}\n",
       "             763 |  # node_Reshape_2726\n",
       "                    %\"val_2727\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2725\", %\"val_2726\") {allowzero=0}\n",
       "             764 |  # node_Mul_2733\n",
       "                    %\"val_2734\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_2727\", %\"val_2733\"{...})\n",
       "             765 |  # node_group_norm_46\n",
       "                    %\"group_norm_46\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_2734\", %\"val_2735\"{...})\n",
       "             766 |  # node_relu_43\n",
       "                    %\"relu_43\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_46\")\n",
       "             767 |  # node_view_96\n",
       "                    %\"view_96\"<FLOAT16,[1,256,2304]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.7.conv2.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             768 |  # node_ReduceMean_2749\n",
       "                    %\"val_2750\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_96\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             769 |  # node_ReduceMean_2750\n",
       "                    %\"val_2751\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_96\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             770 |  # node_Sub_2751\n",
       "                    %\"val_2752\"<FLOAT16,[1,256,2304]> ⬅️ ::Sub(%\"view_96\", %\"val_2751\")\n",
       "             771 |  # node_Mul_2752\n",
       "                    %\"val_2753\"<FLOAT16,[1,256,2304]> ⬅️ ::Mul(%\"val_2752\", %\"val_2752\")\n",
       "             772 |  # node_ReduceMean_2753\n",
       "                    %\"val_2754\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_2753\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             773 |  # node__native_batch_norm_legit_47__0\n",
       "                    %\"getitem_141\"<FLOAT16,[1,256,2304]> ⬅️ ::BatchNormalization(%\"view_96\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_2750\", %\"val_2754\") {momentum=1.0, epsilon=1e-08}\n",
       "             774 |  # node_view_97\n",
       "                    %\"view_97\"<FLOAT16,[256,256,3,3]> ⬅️ ::Reshape(%\"getitem_141\", %\"val_1568\"{[256, 256, 3, 3]}) {allowzero=1}\n",
       "             775 |  # node_Conv_4711\n",
       "                    %\"conv2d_47\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_43\", %\"view_97\") {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             776 |  # node_Reshape_2774\n",
       "                    %\"val_2775\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_47\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             777 |  # node_InstanceNormalization_2781\n",
       "                    %\"val_2782\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_2775\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             778 |  # node_Shape_2782\n",
       "                    %\"val_2783\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_47\") {start=0}\n",
       "             779 |  # node_Reshape_2783\n",
       "                    %\"val_2784\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2782\", %\"val_2783\") {allowzero=0}\n",
       "             780 |  # node_Mul_2790\n",
       "                    %\"val_2791\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_2784\", %\"val_2790\"{...})\n",
       "             781 |  # node_group_norm_47\n",
       "                    %\"group_norm_47\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_2791\", %\"val_2792\"{...})\n",
       "             782 |  # node_relu_44\n",
       "                    %\"relu_44\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_47\")\n",
       "             783 |  # node_view_98\n",
       "                    %\"view_98\"<FLOAT16,[1,1024,256]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.7.conv3.weight\"{...}, %\"val_1427\"{[1, 1024, -1]}) {allowzero=1}\n",
       "             784 |  # node_ReduceMean_2806\n",
       "                    %\"val_2807\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"view_98\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             785 |  # node_ReduceMean_2807\n",
       "                    %\"val_2808\"<FLOAT16,[1,1024,1]> ⬅️ ::ReduceMean(%\"view_98\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             786 |  # node_Sub_2808\n",
       "                    %\"val_2809\"<FLOAT16,[1,1024,256]> ⬅️ ::Sub(%\"view_98\", %\"val_2808\")\n",
       "             787 |  # node_Mul_2809\n",
       "                    %\"val_2810\"<FLOAT16,[1,1024,256]> ⬅️ ::Mul(%\"val_2809\", %\"val_2809\")\n",
       "             788 |  # node_ReduceMean_2810\n",
       "                    %\"val_2811\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"val_2810\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             789 |  # node__native_batch_norm_legit_48__0\n",
       "                    %\"getitem_144\"<FLOAT16,[1,1024,256]> ⬅️ ::BatchNormalization(%\"view_98\", %\"val_1431\"{...}, %\"val_1435\"{...}, %\"val_2807\", %\"val_2811\") {momentum=1.0, epsilon=1e-08}\n",
       "             790 |  # node_view_99\n",
       "                    %\"view_99\"<FLOAT16,[1024,256,1,1]> ⬅️ ::Reshape(%\"getitem_144\", %\"val_1625\"{[1024, 256, 1, 1]}) {allowzero=1}\n",
       "             791 |  # node_Conv_4712\n",
       "                    %\"conv2d_48\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Conv(%\"relu_44\", %\"view_99\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             792 |  # node_Reshape_2831\n",
       "                    %\"val_2832\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_48\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             793 |  # node_InstanceNormalization_2838\n",
       "                    %\"val_2839\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_2832\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             794 |  # node_Shape_2839\n",
       "                    %\"val_2840\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_48\") {start=0}\n",
       "             795 |  # node_Reshape_2840\n",
       "                    %\"val_2841\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2839\", %\"val_2840\") {allowzero=0}\n",
       "             796 |  # node_Mul_2847\n",
       "                    %\"val_2848\"<FLOAT16,[None,1024,None,None]> ⬅️ ::Mul(%\"val_2841\", %\"val_2847\"{...})\n",
       "             797 |  # node_group_norm_48\n",
       "                    %\"group_norm_48\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"val_2848\", %\"val_2849\"{...})\n",
       "             798 |  # node_add_1059\n",
       "                    %\"add_1059\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"group_norm_48\", %\"relu_42\")\n",
       "             799 |  # node_relu_45\n",
       "                    %\"relu_45\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Relu(%\"add_1059\")\n",
       "             800 |  # node_view_100\n",
       "                    %\"view_100\"<FLOAT16,[1,256,1024]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.8.conv1.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             801 |  # node_ReduceMean_2863\n",
       "                    %\"val_2864\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_100\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             802 |  # node_ReduceMean_2864\n",
       "                    %\"val_2865\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_100\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             803 |  # node_Sub_2865\n",
       "                    %\"val_2866\"<FLOAT16,[1,256,1024]> ⬅️ ::Sub(%\"view_100\", %\"val_2865\")\n",
       "             804 |  # node_Mul_2866\n",
       "                    %\"val_2867\"<FLOAT16,[1,256,1024]> ⬅️ ::Mul(%\"val_2866\", %\"val_2866\")\n",
       "             805 |  # node_ReduceMean_2867\n",
       "                    %\"val_2868\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_2867\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             806 |  # node__native_batch_norm_legit_49__0\n",
       "                    %\"getitem_147\"<FLOAT16,[1,256,1024]> ⬅️ ::BatchNormalization(%\"view_100\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_2864\", %\"val_2868\") {momentum=1.0, epsilon=1e-08}\n",
       "             807 |  # node_view_101\n",
       "                    %\"view_101\"<FLOAT16,[256,1024,1,1]> ⬅️ ::Reshape(%\"getitem_147\", %\"val_1682\"{[256, 1024, 1, 1]}) {allowzero=1}\n",
       "             808 |  # node_Conv_4713\n",
       "                    %\"conv2d_49\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_45\", %\"view_101\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             809 |  # node_Reshape_2888\n",
       "                    %\"val_2889\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_49\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             810 |  # node_InstanceNormalization_2895\n",
       "                    %\"val_2896\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_2889\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             811 |  # node_Shape_2896\n",
       "                    %\"val_2897\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_49\") {start=0}\n",
       "             812 |  # node_Reshape_2897\n",
       "                    %\"val_2898\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2896\", %\"val_2897\") {allowzero=0}\n",
       "             813 |  # node_Mul_2904\n",
       "                    %\"val_2905\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_2898\", %\"val_2904\"{...})\n",
       "             814 |  # node_group_norm_49\n",
       "                    %\"group_norm_49\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_2905\", %\"val_2906\"{...})\n",
       "             815 |  # node_relu_46\n",
       "                    %\"relu_46\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_49\")\n",
       "             816 |  # node_view_102\n",
       "                    %\"view_102\"<FLOAT16,[1,256,2304]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.8.conv2.weight\"{...}, %\"val_110\"{[1, 256, -1]}) {allowzero=1}\n",
       "             817 |  # node_ReduceMean_2920\n",
       "                    %\"val_2921\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"view_102\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             818 |  # node_ReduceMean_2921\n",
       "                    %\"val_2922\"<FLOAT16,[1,256,1]> ⬅️ ::ReduceMean(%\"view_102\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             819 |  # node_Sub_2922\n",
       "                    %\"val_2923\"<FLOAT16,[1,256,2304]> ⬅️ ::Sub(%\"view_102\", %\"val_2922\")\n",
       "             820 |  # node_Mul_2923\n",
       "                    %\"val_2924\"<FLOAT16,[1,256,2304]> ⬅️ ::Mul(%\"val_2923\", %\"val_2923\")\n",
       "             821 |  # node_ReduceMean_2924\n",
       "                    %\"val_2925\"<FLOAT16,[256]> ⬅️ ::ReduceMean(%\"val_2924\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             822 |  # node__native_batch_norm_legit_50__0\n",
       "                    %\"getitem_150\"<FLOAT16,[1,256,2304]> ⬅️ ::BatchNormalization(%\"view_102\", %\"val_114\"{...}, %\"val_118\"{...}, %\"val_2921\", %\"val_2925\") {momentum=1.0, epsilon=1e-08}\n",
       "             823 |  # node_view_103\n",
       "                    %\"view_103\"<FLOAT16,[256,256,3,3]> ⬅️ ::Reshape(%\"getitem_150\", %\"val_1568\"{[256, 256, 3, 3]}) {allowzero=1}\n",
       "             824 |  # node_Conv_4714\n",
       "                    %\"conv2d_50\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_46\", %\"view_103\") {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             825 |  # node_Reshape_2945\n",
       "                    %\"val_2946\"<FLOAT16,[batch,32,4608]> ⬅️ ::Reshape(%\"conv2d_50\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             826 |  # node_InstanceNormalization_2952\n",
       "                    %\"val_2953\"<FLOAT16,[batch,32,4608]> ⬅️ ::InstanceNormalization(%\"val_2946\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             827 |  # node_Shape_2953\n",
       "                    %\"val_2954\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_50\") {start=0}\n",
       "             828 |  # node_Reshape_2954\n",
       "                    %\"val_2955\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_2953\", %\"val_2954\") {allowzero=0}\n",
       "             829 |  # node_Mul_2961\n",
       "                    %\"val_2962\"<FLOAT16,[None,256,None,None]> ⬅️ ::Mul(%\"val_2955\", %\"val_2961\"{...})\n",
       "             830 |  # node_group_norm_50\n",
       "                    %\"group_norm_50\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"val_2962\", %\"val_2963\"{...})\n",
       "             831 |  # node_relu_47\n",
       "                    %\"relu_47\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"group_norm_50\")\n",
       "             832 |  # node_view_104\n",
       "                    %\"view_104\"<FLOAT16,[1,1024,256]> ⬅️ ::Reshape(%\"pretrained.model.patch_embed.backbone.stages.2.blocks.8.conv3.weight\"{...}, %\"val_1427\"{[1, 1024, -1]}) {allowzero=1}\n",
       "             833 |  # node_ReduceMean_2977\n",
       "                    %\"val_2978\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"view_104\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             834 |  # node_ReduceMean_2978\n",
       "                    %\"val_2979\"<FLOAT16,[1,1024,1]> ⬅️ ::ReduceMean(%\"view_104\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             835 |  # node_Sub_2979\n",
       "                    %\"val_2980\"<FLOAT16,[1,1024,256]> ⬅️ ::Sub(%\"view_104\", %\"val_2979\")\n",
       "             836 |  # node_Mul_2980\n",
       "                    %\"val_2981\"<FLOAT16,[1,1024,256]> ⬅️ ::Mul(%\"val_2980\", %\"val_2980\")\n",
       "             837 |  # node_ReduceMean_2981\n",
       "                    %\"val_2982\"<FLOAT16,[1024]> ⬅️ ::ReduceMean(%\"val_2981\", %\"val_54\"{[0, 2]}) {noop_with_empty_axes=0, keepdims=0}\n",
       "             838 |  # node__native_batch_norm_legit_51__0\n",
       "                    %\"getitem_153\"<FLOAT16,[1,1024,256]> ⬅️ ::BatchNormalization(%\"view_104\", %\"val_1431\"{...}, %\"val_1435\"{...}, %\"val_2978\", %\"val_2982\") {momentum=1.0, epsilon=1e-08}\n",
       "             839 |  # node_view_105\n",
       "                    %\"view_105\"<FLOAT16,[1024,256,1,1]> ⬅️ ::Reshape(%\"getitem_153\", %\"val_1625\"{[1024, 256, 1, 1]}) {allowzero=1}\n",
       "             840 |  # node_Conv_4715\n",
       "                    %\"conv2d_51\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Conv(%\"relu_47\", %\"view_105\") {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             841 |  # node_Reshape_3002\n",
       "                    %\"val_3003\"<FLOAT16,[batch,32,18432]> ⬅️ ::Reshape(%\"conv2d_51\", %\"val_83\"{[0, 32, -1]}) {allowzero=0}\n",
       "             842 |  # node_InstanceNormalization_3009\n",
       "                    %\"val_3010\"<FLOAT16,[batch,32,18432]> ⬅️ ::InstanceNormalization(%\"val_3003\", %\"val_87\"{...}, %\"val_90\"{...}) {epsilon=1e-05}\n",
       "             843 |  # node_Shape_3010\n",
       "                    %\"val_3011\"<INT64,[4]> ⬅️ ::Shape(%\"conv2d_51\") {start=0}\n",
       "             844 |  # node_Reshape_3011\n",
       "                    %\"val_3012\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_3010\", %\"val_3011\") {allowzero=0}\n",
       "             845 |  # node_Mul_3018\n",
       "                    %\"val_3019\"<FLOAT16,[None,1024,None,None]> ⬅️ ::Mul(%\"val_3012\", %\"val_3018\"{...})\n",
       "             846 |  # node_group_norm_51\n",
       "                    %\"group_norm_51\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"val_3019\", %\"val_3020\"{...})\n",
       "             847 |  # node_add_1125\n",
       "                    %\"add_1125\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Add(%\"group_norm_51\", %\"relu_45\")\n",
       "             848 |  # node_relu_48\n",
       "                    %\"relu_48\"<FLOAT16,[batch,1024,24,24]> ⬅️ ::Relu(%\"add_1125\")\n",
       "             849 |  # node_conv2d_52\n",
       "                    %\"conv2d_52\"<FLOAT16,[batch,768,24,24]> ⬅️ ::Conv(%\"relu_48\", %\"pretrained.model.patch_embed.proj.weight\"{...}, %\"pretrained.model.patch_embed.proj.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             850 |  # node_Concat_3024\n",
       "                    %\"val_3025\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_3023\"{[768]}, %\"val_3024\"{[576]}) {axis=0}\n",
       "             851 |  # node_view_106\n",
       "                    %\"view_106\"<FLOAT16,[batch,768,576]> ⬅️ ::Reshape(%\"conv2d_52\", %\"val_3025\") {allowzero=1}\n",
       "             852 |  # node_transpose\n",
       "                    %\"transpose\"<FLOAT16,[batch,576,768]> ⬅️ ::Transpose(%\"view_106\") {perm=(0, 2, 1)}\n",
       "             853 |  # node_Concat_3029\n",
       "                    %\"val_3030\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_8\"{[1]}, %\"val_8\"{[1]}) {axis=0}\n",
       "             854 |  # node_expand\n",
       "                    %\"expand\"<FLOAT16,[batch,1,768]> ⬅️ ::Expand(%\"pretrained.model.cls_token\"{...}, %\"val_3030\")\n",
       "             855 |  # node_cat_1\n",
       "                    %\"cat_1\"<FLOAT16,[batch,577,768]> ⬅️ ::Concat(%\"expand\", %\"transpose\") {axis=1}\n",
       "             856 |  # node_add_1163\n",
       "                    %\"add_1163\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"cat_1\", %\"cat\")\n",
       "             857 |  # node_layer_norm\n",
       "                    %\"layer_norm\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1163\", %\"pretrained.model.blocks.0.norm1.weight\"{...}, %\"pretrained.model.blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "             858 |  # node_MatMul_3031\n",
       "                    %\"val_3034\"<FLOAT16,[batch,577,2304]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_3033\"{...})\n",
       "             859 |  # node_linear\n",
       "                    %\"linear\"<FLOAT16,[batch,577,2304]> ⬅️ ::Add(%\"val_3034\", %\"pretrained.model.blocks.0.attn.qkv.bias\"{...})\n",
       "             860 |  # node_Concat_3038\n",
       "                    %\"val_3041\"<INT64,[5]> ⬅️ ::Concat(%\"val_0\", %\"val_3037\"{[577]}, %\"val_3038\"{[3]}, %\"val_3039\"{[12]}, %\"val_3040\"{[64]}) {axis=0}\n",
       "             861 |  # node_view_107\n",
       "                    %\"view_107\"<FLOAT16,[batch,577,3,12,64]> ⬅️ ::Reshape(%\"linear\", %\"val_3041\") {allowzero=1}\n",
       "             862 |  # node_permute_2\n",
       "                    %\"permute_2\"<FLOAT16,[3,batch,12,577,64]> ⬅️ ::Transpose(%\"view_107\") {perm=(2, 0, 3, 1, 4)}\n",
       "             863 |  # node_Slice_3042\n",
       "                    %\"val_3045\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_2\", %\"val_5\"{[0]}, %\"val_8\"{[1]}, %\"val_5\"{[0]})\n",
       "             864 |  # node_unbind__0\n",
       "                    %\"getitem_156\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3045\", %\"val_5\"{[0]})\n",
       "             865 |  # node_Slice_3047\n",
       "                    %\"val_3050\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_2\", %\"val_8\"{[1]}, %\"val_3048\"{[2]}, %\"val_5\"{[0]})\n",
       "             866 |  # node_unbind__1\n",
       "                    %\"getitem_157\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3050\", %\"val_5\"{[0]})\n",
       "             867 |  # node_Slice_3051\n",
       "                    %\"val_3054\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_2\", %\"val_3048\"{[2]}, %\"val_3038\"{[3]}, %\"val_5\"{[0]})\n",
       "             868 |  # node_unbind__2\n",
       "                    %\"getitem_158\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3054\", %\"val_5\"{[0]})\n",
       "             869 |  # node_Shape_3061\n",
       "                    %\"val_3064\"<INT64,[4]> ⬅️ ::Shape(%\"getitem_157\") {start=0}\n",
       "             870 |  # node_Slice_3064\n",
       "                    %\"val_3067\"<INT64,[1]> ⬅️ ::Slice(%\"val_3064\", %\"val_3066\"{[-1]}, %\"val_19\"{[9223372036854775807]})\n",
       "             871 |  # node_Slice_3066\n",
       "                    %\"val_3069\"<INT64,[1]> ⬅️ ::Slice(%\"val_3064\", %\"val_3068\"{[-2]}, %\"val_3066\"{[-1]})\n",
       "             872 |  # node_Slice_3068\n",
       "                    %\"val_3071\"<INT64,[2]> ⬅️ ::Slice(%\"val_3064\", %\"val_3070\"{[-9223372036854775808]}, %\"val_3068\"{[-2]})\n",
       "             873 |  # node_Concat_3070\n",
       "                    %\"val_3073\"<INT64,[3]> ⬅️ ::Concat(%\"val_3066\"{[-1]}, %\"val_3069\", %\"val_3067\") {axis=0}\n",
       "             874 |  # node_Reshape_3071\n",
       "                    %\"val_3074\"<FLOAT16,[None,None,None]> ⬅️ ::Reshape(%\"getitem_157\", %\"val_3073\") {allowzero=0}\n",
       "             875 |  # node_Transpose_3072\n",
       "                    %\"val_3075\"<FLOAT16,[None,None,None]> ⬅️ ::Transpose(%\"val_3074\") {perm=(0, 2, 1)}\n",
       "             876 |  # node_Concat_3073\n",
       "                    %\"val_3076\"<INT64,[4]> ⬅️ ::Concat(%\"val_3071\", %\"val_3067\", %\"val_3069\") {axis=0}\n",
       "             877 |  # node_Reshape_3074\n",
       "                    %\"val_3077\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_3075\", %\"val_3076\") {allowzero=0}\n",
       "             878 |  # node_Mul_3076\n",
       "                    %\"val_3079\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Mul(%\"getitem_156\", %\"val_3078\"{[0.353515625]})\n",
       "             879 |  # node_Mul_3079\n",
       "                    %\"val_3082\"<FLOAT16,[None,None,None,None]> ⬅️ ::Mul(%\"val_3077\", %\"val_3078\"{[0.353515625]})\n",
       "             880 |  # node_MatMul_3080\n",
       "                    %\"val_3083\"<FLOAT16,[None,12,577,None]> ⬅️ ::MatMul(%\"val_3079\", %\"val_3082\")\n",
       "             881 |  # node_Softmax_3081\n",
       "                    %\"val_3084\"<FLOAT16,[None,12,577,None]> ⬅️ ::Softmax(%\"val_3083\") {axis=-1}\n",
       "             882 |  # node_scaled_dot_product_attention\n",
       "                    %\"scaled_dot_product_attention\"<FLOAT16,[batch,12,577,64]> ⬅️ ::MatMul(%\"val_3084\", %\"getitem_158\")\n",
       "             883 |  # node_transpose_1\n",
       "                    %\"transpose_1\"<FLOAT16,[batch,577,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention\") {perm=(0, 2, 1, 3)}\n",
       "             884 |  # node_Concat_3086\n",
       "                    %\"val_3089\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_3037\"{[577]}, %\"val_3023\"{[768]}) {axis=0}\n",
       "             885 |  # node_view_108\n",
       "                    %\"view_108\"<FLOAT16,[batch,577,768]> ⬅️ ::Reshape(%\"transpose_1\", %\"val_3089\") {allowzero=1}\n",
       "             886 |  # node_MatMul_3088\n",
       "                    %\"val_3091\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"view_108\", %\"val_3090\"{...})\n",
       "             887 |  # node_linear_1\n",
       "                    %\"linear_1\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3091\", %\"pretrained.model.blocks.0.attn.proj.bias\"{...})\n",
       "             888 |  # node_add_1228\n",
       "                    %\"add_1228\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1163\", %\"linear_1\")\n",
       "             889 |  # node_layer_norm_1\n",
       "                    %\"layer_norm_1\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1228\", %\"pretrained.model.blocks.0.norm2.weight\"{...}, %\"pretrained.model.blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "             890 |  # node_MatMul_3090\n",
       "                    %\"val_3095\"<FLOAT16,[batch,577,3072]> ⬅️ ::MatMul(%\"layer_norm_1\", %\"val_3094\"{...})\n",
       "             891 |  # node_linear_2\n",
       "                    %\"linear_2\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3095\", %\"pretrained.model.blocks.0.mlp.fc1.bias\"{...})\n",
       "             892 |  # node_Div_3092\n",
       "                    %\"val_3097\"<FLOAT16,[batch,577,3072]> ⬅️ ::Div(%\"linear_2\", %\"val_3096\"{1.4140625})\n",
       "             893 |  # node_Erf_3093\n",
       "                    %\"val_3098\"<FLOAT16,[batch,577,3072]> ⬅️ ::Erf(%\"val_3097\")\n",
       "             894 |  # node_Add_3095\n",
       "                    %\"val_3100\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3098\", %\"val_3099\"{1.0})\n",
       "             895 |  # node_Mul_3097\n",
       "                    %\"val_3102\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"val_3101\"{0.5}, %\"val_3100\")\n",
       "             896 |  # node_gelu\n",
       "                    %\"gelu\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"linear_2\", %\"val_3102\")\n",
       "             897 |  # node_MatMul_3099\n",
       "                    %\"val_3104\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"gelu\", %\"val_3103\"{...})\n",
       "             898 |  # node_linear_3\n",
       "                    %\"linear_3\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3104\", %\"pretrained.model.blocks.0.mlp.fc2.bias\"{...})\n",
       "             899 |  # node_add_1257\n",
       "                    %\"add_1257\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1228\", %\"linear_3\")\n",
       "             900 |  # node_layer_norm_2\n",
       "                    %\"layer_norm_2\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1257\", %\"pretrained.model.blocks.1.norm1.weight\"{...}, %\"pretrained.model.blocks.1.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "             901 |  # node_MatMul_3101\n",
       "                    %\"val_3108\"<FLOAT16,[batch,577,2304]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_3107\"{...})\n",
       "             902 |  # node_linear_4\n",
       "                    %\"linear_4\"<FLOAT16,[batch,577,2304]> ⬅️ ::Add(%\"val_3108\", %\"pretrained.model.blocks.1.attn.qkv.bias\"{...})\n",
       "             903 |  # node_view_109\n",
       "                    %\"view_109\"<FLOAT16,[batch,577,3,12,64]> ⬅️ ::Reshape(%\"linear_4\", %\"val_3041\") {allowzero=1}\n",
       "             904 |  # node_permute_3\n",
       "                    %\"permute_3\"<FLOAT16,[3,batch,12,577,64]> ⬅️ ::Transpose(%\"view_109\") {perm=(2, 0, 3, 1, 4)}\n",
       "             905 |  # node_Slice_3112\n",
       "                    %\"val_3119\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_3\", %\"val_5\"{[0]}, %\"val_8\"{[1]}, %\"val_5\"{[0]})\n",
       "             906 |  # node_unbind_1__0\n",
       "                    %\"getitem_159\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3119\", %\"val_5\"{[0]})\n",
       "             907 |  # node_Slice_3116\n",
       "                    %\"val_3123\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_3\", %\"val_8\"{[1]}, %\"val_3048\"{[2]}, %\"val_5\"{[0]})\n",
       "             908 |  # node_unbind_1__1\n",
       "                    %\"getitem_160\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3123\", %\"val_5\"{[0]})\n",
       "             909 |  # node_Slice_3120\n",
       "                    %\"val_3127\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_3\", %\"val_3048\"{[2]}, %\"val_3038\"{[3]}, %\"val_5\"{[0]})\n",
       "             910 |  # node_unbind_1__2\n",
       "                    %\"getitem_161\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3127\", %\"val_5\"{[0]})\n",
       "             911 |  # node_Shape_3130\n",
       "                    %\"val_3137\"<INT64,[4]> ⬅️ ::Shape(%\"getitem_160\") {start=0}\n",
       "             912 |  # node_Slice_3132\n",
       "                    %\"val_3139\"<INT64,[1]> ⬅️ ::Slice(%\"val_3137\", %\"val_3066\"{[-1]}, %\"val_19\"{[9223372036854775807]})\n",
       "             913 |  # node_Slice_3133\n",
       "                    %\"val_3140\"<INT64,[1]> ⬅️ ::Slice(%\"val_3137\", %\"val_3068\"{[-2]}, %\"val_3066\"{[-1]})\n",
       "             914 |  # node_Slice_3135\n",
       "                    %\"val_3142\"<INT64,[2]> ⬅️ ::Slice(%\"val_3137\", %\"val_3070\"{[-9223372036854775808]}, %\"val_3068\"{[-2]})\n",
       "             915 |  # node_Concat_3137\n",
       "                    %\"val_3144\"<INT64,[3]> ⬅️ ::Concat(%\"val_3066\"{[-1]}, %\"val_3140\", %\"val_3139\") {axis=0}\n",
       "             916 |  # node_Reshape_3138\n",
       "                    %\"val_3145\"<FLOAT16,[None,None,None]> ⬅️ ::Reshape(%\"getitem_160\", %\"val_3144\") {allowzero=0}\n",
       "             917 |  # node_Transpose_3139\n",
       "                    %\"val_3146\"<FLOAT16,[None,None,None]> ⬅️ ::Transpose(%\"val_3145\") {perm=(0, 2, 1)}\n",
       "             918 |  # node_Concat_3140\n",
       "                    %\"val_3147\"<INT64,[4]> ⬅️ ::Concat(%\"val_3142\", %\"val_3139\", %\"val_3140\") {axis=0}\n",
       "             919 |  # node_Reshape_3141\n",
       "                    %\"val_3148\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_3146\", %\"val_3147\") {allowzero=0}\n",
       "             920 |  # node_Mul_3143\n",
       "                    %\"val_3150\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Mul(%\"getitem_159\", %\"val_3078\"{[0.353515625]})\n",
       "             921 |  # node_Mul_3146\n",
       "                    %\"val_3153\"<FLOAT16,[None,None,None,None]> ⬅️ ::Mul(%\"val_3148\", %\"val_3078\"{[0.353515625]})\n",
       "             922 |  # node_MatMul_3147\n",
       "                    %\"val_3154\"<FLOAT16,[None,12,577,None]> ⬅️ ::MatMul(%\"val_3150\", %\"val_3153\")\n",
       "             923 |  # node_Softmax_3148\n",
       "                    %\"val_3155\"<FLOAT16,[None,12,577,None]> ⬅️ ::Softmax(%\"val_3154\") {axis=-1}\n",
       "             924 |  # node_scaled_dot_product_attention_1\n",
       "                    %\"scaled_dot_product_attention_1\"<FLOAT16,[batch,12,577,64]> ⬅️ ::MatMul(%\"val_3155\", %\"getitem_161\")\n",
       "             925 |  # node_transpose_2\n",
       "                    %\"transpose_2\"<FLOAT16,[batch,577,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_1\") {perm=(0, 2, 1, 3)}\n",
       "             926 |  # node_view_110\n",
       "                    %\"view_110\"<FLOAT16,[batch,577,768]> ⬅️ ::Reshape(%\"transpose_2\", %\"val_3089\") {allowzero=1}\n",
       "             927 |  # node_MatMul_3155\n",
       "                    %\"val_3162\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"view_110\", %\"val_3161\"{...})\n",
       "             928 |  # node_linear_5\n",
       "                    %\"linear_5\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3162\", %\"pretrained.model.blocks.1.attn.proj.bias\"{...})\n",
       "             929 |  # node_add_1318\n",
       "                    %\"add_1318\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1257\", %\"linear_5\")\n",
       "             930 |  # node_layer_norm_3\n",
       "                    %\"layer_norm_3\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1318\", %\"pretrained.model.blocks.1.norm2.weight\"{...}, %\"pretrained.model.blocks.1.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "             931 |  # node_MatMul_3157\n",
       "                    %\"val_3166\"<FLOAT16,[batch,577,3072]> ⬅️ ::MatMul(%\"layer_norm_3\", %\"val_3165\"{...})\n",
       "             932 |  # node_linear_6\n",
       "                    %\"linear_6\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3166\", %\"pretrained.model.blocks.1.mlp.fc1.bias\"{...})\n",
       "             933 |  # node_Div_3159\n",
       "                    %\"val_3168\"<FLOAT16,[batch,577,3072]> ⬅️ ::Div(%\"linear_6\", %\"val_3096\"{1.4140625})\n",
       "             934 |  # node_Erf_3160\n",
       "                    %\"val_3169\"<FLOAT16,[batch,577,3072]> ⬅️ ::Erf(%\"val_3168\")\n",
       "             935 |  # node_Add_3162\n",
       "                    %\"val_3171\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3169\", %\"val_3099\"{1.0})\n",
       "             936 |  # node_Mul_3164\n",
       "                    %\"val_3173\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"val_3101\"{0.5}, %\"val_3171\")\n",
       "             937 |  # node_gelu_1\n",
       "                    %\"gelu_1\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"linear_6\", %\"val_3173\")\n",
       "             938 |  # node_MatMul_3166\n",
       "                    %\"val_3175\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"gelu_1\", %\"val_3174\"{...})\n",
       "             939 |  # node_linear_7\n",
       "                    %\"linear_7\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3175\", %\"pretrained.model.blocks.1.mlp.fc2.bias\"{...})\n",
       "             940 |  # node_add_1347\n",
       "                    %\"add_1347\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1318\", %\"linear_7\")\n",
       "             941 |  # node_layer_norm_4\n",
       "                    %\"layer_norm_4\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1347\", %\"pretrained.model.blocks.2.norm1.weight\"{...}, %\"pretrained.model.blocks.2.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "             942 |  # node_MatMul_3168\n",
       "                    %\"val_3179\"<FLOAT16,[batch,577,2304]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_3178\"{...})\n",
       "             943 |  # node_linear_8\n",
       "                    %\"linear_8\"<FLOAT16,[batch,577,2304]> ⬅️ ::Add(%\"val_3179\", %\"pretrained.model.blocks.2.attn.qkv.bias\"{...})\n",
       "             944 |  # node_view_111\n",
       "                    %\"view_111\"<FLOAT16,[batch,577,3,12,64]> ⬅️ ::Reshape(%\"linear_8\", %\"val_3041\") {allowzero=1}\n",
       "             945 |  # node_permute_4\n",
       "                    %\"permute_4\"<FLOAT16,[3,batch,12,577,64]> ⬅️ ::Transpose(%\"view_111\") {perm=(2, 0, 3, 1, 4)}\n",
       "             946 |  # node_Slice_3179\n",
       "                    %\"val_3190\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_4\", %\"val_5\"{[0]}, %\"val_8\"{[1]}, %\"val_5\"{[0]})\n",
       "             947 |  # node_unbind_2__0\n",
       "                    %\"getitem_162\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3190\", %\"val_5\"{[0]})\n",
       "             948 |  # node_Slice_3183\n",
       "                    %\"val_3194\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_4\", %\"val_8\"{[1]}, %\"val_3048\"{[2]}, %\"val_5\"{[0]})\n",
       "             949 |  # node_unbind_2__1\n",
       "                    %\"getitem_163\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3194\", %\"val_5\"{[0]})\n",
       "             950 |  # node_Slice_3187\n",
       "                    %\"val_3198\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_4\", %\"val_3048\"{[2]}, %\"val_3038\"{[3]}, %\"val_5\"{[0]})\n",
       "             951 |  # node_unbind_2__2\n",
       "                    %\"getitem_164\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3198\", %\"val_5\"{[0]})\n",
       "             952 |  # node_Shape_3197\n",
       "                    %\"val_3208\"<INT64,[4]> ⬅️ ::Shape(%\"getitem_163\") {start=0}\n",
       "             953 |  # node_Slice_3199\n",
       "                    %\"val_3210\"<INT64,[1]> ⬅️ ::Slice(%\"val_3208\", %\"val_3066\"{[-1]}, %\"val_19\"{[9223372036854775807]})\n",
       "             954 |  # node_Slice_3200\n",
       "                    %\"val_3211\"<INT64,[1]> ⬅️ ::Slice(%\"val_3208\", %\"val_3068\"{[-2]}, %\"val_3066\"{[-1]})\n",
       "             955 |  # node_Slice_3202\n",
       "                    %\"val_3213\"<INT64,[2]> ⬅️ ::Slice(%\"val_3208\", %\"val_3070\"{[-9223372036854775808]}, %\"val_3068\"{[-2]})\n",
       "             956 |  # node_Concat_3204\n",
       "                    %\"val_3215\"<INT64,[3]> ⬅️ ::Concat(%\"val_3066\"{[-1]}, %\"val_3211\", %\"val_3210\") {axis=0}\n",
       "             957 |  # node_Reshape_3205\n",
       "                    %\"val_3216\"<FLOAT16,[None,None,None]> ⬅️ ::Reshape(%\"getitem_163\", %\"val_3215\") {allowzero=0}\n",
       "             958 |  # node_Transpose_3206\n",
       "                    %\"val_3217\"<FLOAT16,[None,None,None]> ⬅️ ::Transpose(%\"val_3216\") {perm=(0, 2, 1)}\n",
       "             959 |  # node_Concat_3207\n",
       "                    %\"val_3218\"<INT64,[4]> ⬅️ ::Concat(%\"val_3213\", %\"val_3210\", %\"val_3211\") {axis=0}\n",
       "             960 |  # node_Reshape_3208\n",
       "                    %\"val_3219\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_3217\", %\"val_3218\") {allowzero=0}\n",
       "             961 |  # node_Mul_3210\n",
       "                    %\"val_3221\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Mul(%\"getitem_162\", %\"val_3078\"{[0.353515625]})\n",
       "             962 |  # node_Mul_3213\n",
       "                    %\"val_3224\"<FLOAT16,[None,None,None,None]> ⬅️ ::Mul(%\"val_3219\", %\"val_3078\"{[0.353515625]})\n",
       "             963 |  # node_MatMul_3214\n",
       "                    %\"val_3225\"<FLOAT16,[None,12,577,None]> ⬅️ ::MatMul(%\"val_3221\", %\"val_3224\")\n",
       "             964 |  # node_Softmax_3215\n",
       "                    %\"val_3226\"<FLOAT16,[None,12,577,None]> ⬅️ ::Softmax(%\"val_3225\") {axis=-1}\n",
       "             965 |  # node_scaled_dot_product_attention_2\n",
       "                    %\"scaled_dot_product_attention_2\"<FLOAT16,[batch,12,577,64]> ⬅️ ::MatMul(%\"val_3226\", %\"getitem_164\")\n",
       "             966 |  # node_transpose_3\n",
       "                    %\"transpose_3\"<FLOAT16,[batch,577,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_2\") {perm=(0, 2, 1, 3)}\n",
       "             967 |  # node_view_112\n",
       "                    %\"view_112\"<FLOAT16,[batch,577,768]> ⬅️ ::Reshape(%\"transpose_3\", %\"val_3089\") {allowzero=1}\n",
       "             968 |  # node_MatMul_3222\n",
       "                    %\"val_3233\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"view_112\", %\"val_3232\"{...})\n",
       "             969 |  # node_linear_9\n",
       "                    %\"linear_9\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3233\", %\"pretrained.model.blocks.2.attn.proj.bias\"{...})\n",
       "             970 |  # node_add_1408\n",
       "                    %\"add_1408\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1347\", %\"linear_9\")\n",
       "             971 |  # node_layer_norm_5\n",
       "                    %\"layer_norm_5\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1408\", %\"pretrained.model.blocks.2.norm2.weight\"{...}, %\"pretrained.model.blocks.2.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "             972 |  # node_MatMul_3224\n",
       "                    %\"val_3237\"<FLOAT16,[batch,577,3072]> ⬅️ ::MatMul(%\"layer_norm_5\", %\"val_3236\"{...})\n",
       "             973 |  # node_linear_10\n",
       "                    %\"linear_10\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3237\", %\"pretrained.model.blocks.2.mlp.fc1.bias\"{...})\n",
       "             974 |  # node_Div_3226\n",
       "                    %\"val_3239\"<FLOAT16,[batch,577,3072]> ⬅️ ::Div(%\"linear_10\", %\"val_3096\"{1.4140625})\n",
       "             975 |  # node_Erf_3227\n",
       "                    %\"val_3240\"<FLOAT16,[batch,577,3072]> ⬅️ ::Erf(%\"val_3239\")\n",
       "             976 |  # node_Add_3229\n",
       "                    %\"val_3242\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3240\", %\"val_3099\"{1.0})\n",
       "             977 |  # node_Mul_3231\n",
       "                    %\"val_3244\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"val_3101\"{0.5}, %\"val_3242\")\n",
       "             978 |  # node_gelu_2\n",
       "                    %\"gelu_2\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"linear_10\", %\"val_3244\")\n",
       "             979 |  # node_MatMul_3233\n",
       "                    %\"val_3246\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"gelu_2\", %\"val_3245\"{...})\n",
       "             980 |  # node_linear_11\n",
       "                    %\"linear_11\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3246\", %\"pretrained.model.blocks.2.mlp.fc2.bias\"{...})\n",
       "             981 |  # node_add_1437\n",
       "                    %\"add_1437\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1408\", %\"linear_11\")\n",
       "             982 |  # node_layer_norm_6\n",
       "                    %\"layer_norm_6\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1437\", %\"pretrained.model.blocks.3.norm1.weight\"{...}, %\"pretrained.model.blocks.3.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "             983 |  # node_MatMul_3235\n",
       "                    %\"val_3250\"<FLOAT16,[batch,577,2304]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_3249\"{...})\n",
       "             984 |  # node_linear_12\n",
       "                    %\"linear_12\"<FLOAT16,[batch,577,2304]> ⬅️ ::Add(%\"val_3250\", %\"pretrained.model.blocks.3.attn.qkv.bias\"{...})\n",
       "             985 |  # node_view_113\n",
       "                    %\"view_113\"<FLOAT16,[batch,577,3,12,64]> ⬅️ ::Reshape(%\"linear_12\", %\"val_3041\") {allowzero=1}\n",
       "             986 |  # node_permute_5\n",
       "                    %\"permute_5\"<FLOAT16,[3,batch,12,577,64]> ⬅️ ::Transpose(%\"view_113\") {perm=(2, 0, 3, 1, 4)}\n",
       "             987 |  # node_Slice_3246\n",
       "                    %\"val_3261\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_5\", %\"val_5\"{[0]}, %\"val_8\"{[1]}, %\"val_5\"{[0]})\n",
       "             988 |  # node_unbind_3__0\n",
       "                    %\"getitem_165\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3261\", %\"val_5\"{[0]})\n",
       "             989 |  # node_Slice_3250\n",
       "                    %\"val_3265\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_5\", %\"val_8\"{[1]}, %\"val_3048\"{[2]}, %\"val_5\"{[0]})\n",
       "             990 |  # node_unbind_3__1\n",
       "                    %\"getitem_166\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3265\", %\"val_5\"{[0]})\n",
       "             991 |  # node_Slice_3254\n",
       "                    %\"val_3269\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_5\", %\"val_3048\"{[2]}, %\"val_3038\"{[3]}, %\"val_5\"{[0]})\n",
       "             992 |  # node_unbind_3__2\n",
       "                    %\"getitem_167\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3269\", %\"val_5\"{[0]})\n",
       "             993 |  # node_Shape_3264\n",
       "                    %\"val_3279\"<INT64,[4]> ⬅️ ::Shape(%\"getitem_166\") {start=0}\n",
       "             994 |  # node_Slice_3266\n",
       "                    %\"val_3281\"<INT64,[1]> ⬅️ ::Slice(%\"val_3279\", %\"val_3066\"{[-1]}, %\"val_19\"{[9223372036854775807]})\n",
       "             995 |  # node_Slice_3267\n",
       "                    %\"val_3282\"<INT64,[1]> ⬅️ ::Slice(%\"val_3279\", %\"val_3068\"{[-2]}, %\"val_3066\"{[-1]})\n",
       "             996 |  # node_Slice_3269\n",
       "                    %\"val_3284\"<INT64,[2]> ⬅️ ::Slice(%\"val_3279\", %\"val_3070\"{[-9223372036854775808]}, %\"val_3068\"{[-2]})\n",
       "             997 |  # node_Concat_3271\n",
       "                    %\"val_3286\"<INT64,[3]> ⬅️ ::Concat(%\"val_3066\"{[-1]}, %\"val_3282\", %\"val_3281\") {axis=0}\n",
       "             998 |  # node_Reshape_3272\n",
       "                    %\"val_3287\"<FLOAT16,[None,None,None]> ⬅️ ::Reshape(%\"getitem_166\", %\"val_3286\") {allowzero=0}\n",
       "             999 |  # node_Transpose_3273\n",
       "                    %\"val_3288\"<FLOAT16,[None,None,None]> ⬅️ ::Transpose(%\"val_3287\") {perm=(0, 2, 1)}\n",
       "            1000 |  # node_Concat_3274\n",
       "                    %\"val_3289\"<INT64,[4]> ⬅️ ::Concat(%\"val_3284\", %\"val_3281\", %\"val_3282\") {axis=0}\n",
       "            1001 |  # node_Reshape_3275\n",
       "                    %\"val_3290\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_3288\", %\"val_3289\") {allowzero=0}\n",
       "            1002 |  # node_Mul_3277\n",
       "                    %\"val_3292\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Mul(%\"getitem_165\", %\"val_3078\"{[0.353515625]})\n",
       "            1003 |  # node_Mul_3280\n",
       "                    %\"val_3295\"<FLOAT16,[None,None,None,None]> ⬅️ ::Mul(%\"val_3290\", %\"val_3078\"{[0.353515625]})\n",
       "            1004 |  # node_MatMul_3281\n",
       "                    %\"val_3296\"<FLOAT16,[None,12,577,None]> ⬅️ ::MatMul(%\"val_3292\", %\"val_3295\")\n",
       "            1005 |  # node_Softmax_3282\n",
       "                    %\"val_3297\"<FLOAT16,[None,12,577,None]> ⬅️ ::Softmax(%\"val_3296\") {axis=-1}\n",
       "            1006 |  # node_scaled_dot_product_attention_3\n",
       "                    %\"scaled_dot_product_attention_3\"<FLOAT16,[batch,12,577,64]> ⬅️ ::MatMul(%\"val_3297\", %\"getitem_167\")\n",
       "            1007 |  # node_transpose_4\n",
       "                    %\"transpose_4\"<FLOAT16,[batch,577,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_3\") {perm=(0, 2, 1, 3)}\n",
       "            1008 |  # node_view_114\n",
       "                    %\"view_114\"<FLOAT16,[batch,577,768]> ⬅️ ::Reshape(%\"transpose_4\", %\"val_3089\") {allowzero=1}\n",
       "            1009 |  # node_MatMul_3289\n",
       "                    %\"val_3304\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"view_114\", %\"val_3303\"{...})\n",
       "            1010 |  # node_linear_13\n",
       "                    %\"linear_13\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3304\", %\"pretrained.model.blocks.3.attn.proj.bias\"{...})\n",
       "            1011 |  # node_add_1498\n",
       "                    %\"add_1498\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1437\", %\"linear_13\")\n",
       "            1012 |  # node_layer_norm_7\n",
       "                    %\"layer_norm_7\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1498\", %\"pretrained.model.blocks.3.norm2.weight\"{...}, %\"pretrained.model.blocks.3.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1013 |  # node_MatMul_3291\n",
       "                    %\"val_3308\"<FLOAT16,[batch,577,3072]> ⬅️ ::MatMul(%\"layer_norm_7\", %\"val_3307\"{...})\n",
       "            1014 |  # node_linear_14\n",
       "                    %\"linear_14\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3308\", %\"pretrained.model.blocks.3.mlp.fc1.bias\"{...})\n",
       "            1015 |  # node_Div_3293\n",
       "                    %\"val_3310\"<FLOAT16,[batch,577,3072]> ⬅️ ::Div(%\"linear_14\", %\"val_3096\"{1.4140625})\n",
       "            1016 |  # node_Erf_3294\n",
       "                    %\"val_3311\"<FLOAT16,[batch,577,3072]> ⬅️ ::Erf(%\"val_3310\")\n",
       "            1017 |  # node_Add_3296\n",
       "                    %\"val_3313\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3311\", %\"val_3099\"{1.0})\n",
       "            1018 |  # node_Mul_3298\n",
       "                    %\"val_3315\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"val_3101\"{0.5}, %\"val_3313\")\n",
       "            1019 |  # node_gelu_3\n",
       "                    %\"gelu_3\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"linear_14\", %\"val_3315\")\n",
       "            1020 |  # node_MatMul_3300\n",
       "                    %\"val_3317\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"gelu_3\", %\"val_3316\"{...})\n",
       "            1021 |  # node_linear_15\n",
       "                    %\"linear_15\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3317\", %\"pretrained.model.blocks.3.mlp.fc2.bias\"{...})\n",
       "            1022 |  # node_add_1527\n",
       "                    %\"add_1527\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1498\", %\"linear_15\")\n",
       "            1023 |  # node_layer_norm_8\n",
       "                    %\"layer_norm_8\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1527\", %\"pretrained.model.blocks.4.norm1.weight\"{...}, %\"pretrained.model.blocks.4.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1024 |  # node_MatMul_3302\n",
       "                    %\"val_3321\"<FLOAT16,[batch,577,2304]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_3320\"{...})\n",
       "            1025 |  # node_linear_16\n",
       "                    %\"linear_16\"<FLOAT16,[batch,577,2304]> ⬅️ ::Add(%\"val_3321\", %\"pretrained.model.blocks.4.attn.qkv.bias\"{...})\n",
       "            1026 |  # node_view_115\n",
       "                    %\"view_115\"<FLOAT16,[batch,577,3,12,64]> ⬅️ ::Reshape(%\"linear_16\", %\"val_3041\") {allowzero=1}\n",
       "            1027 |  # node_permute_6\n",
       "                    %\"permute_6\"<FLOAT16,[3,batch,12,577,64]> ⬅️ ::Transpose(%\"view_115\") {perm=(2, 0, 3, 1, 4)}\n",
       "            1028 |  # node_Slice_3313\n",
       "                    %\"val_3332\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_6\", %\"val_5\"{[0]}, %\"val_8\"{[1]}, %\"val_5\"{[0]})\n",
       "            1029 |  # node_unbind_4__0\n",
       "                    %\"getitem_168\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3332\", %\"val_5\"{[0]})\n",
       "            1030 |  # node_Slice_3317\n",
       "                    %\"val_3336\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_6\", %\"val_8\"{[1]}, %\"val_3048\"{[2]}, %\"val_5\"{[0]})\n",
       "            1031 |  # node_unbind_4__1\n",
       "                    %\"getitem_169\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3336\", %\"val_5\"{[0]})\n",
       "            1032 |  # node_Slice_3321\n",
       "                    %\"val_3340\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_6\", %\"val_3048\"{[2]}, %\"val_3038\"{[3]}, %\"val_5\"{[0]})\n",
       "            1033 |  # node_unbind_4__2\n",
       "                    %\"getitem_170\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3340\", %\"val_5\"{[0]})\n",
       "            1034 |  # node_Shape_3331\n",
       "                    %\"val_3350\"<INT64,[4]> ⬅️ ::Shape(%\"getitem_169\") {start=0}\n",
       "            1035 |  # node_Slice_3333\n",
       "                    %\"val_3352\"<INT64,[1]> ⬅️ ::Slice(%\"val_3350\", %\"val_3066\"{[-1]}, %\"val_19\"{[9223372036854775807]})\n",
       "            1036 |  # node_Slice_3334\n",
       "                    %\"val_3353\"<INT64,[1]> ⬅️ ::Slice(%\"val_3350\", %\"val_3068\"{[-2]}, %\"val_3066\"{[-1]})\n",
       "            1037 |  # node_Slice_3336\n",
       "                    %\"val_3355\"<INT64,[2]> ⬅️ ::Slice(%\"val_3350\", %\"val_3070\"{[-9223372036854775808]}, %\"val_3068\"{[-2]})\n",
       "            1038 |  # node_Concat_3338\n",
       "                    %\"val_3357\"<INT64,[3]> ⬅️ ::Concat(%\"val_3066\"{[-1]}, %\"val_3353\", %\"val_3352\") {axis=0}\n",
       "            1039 |  # node_Reshape_3339\n",
       "                    %\"val_3358\"<FLOAT16,[None,None,None]> ⬅️ ::Reshape(%\"getitem_169\", %\"val_3357\") {allowzero=0}\n",
       "            1040 |  # node_Transpose_3340\n",
       "                    %\"val_3359\"<FLOAT16,[None,None,None]> ⬅️ ::Transpose(%\"val_3358\") {perm=(0, 2, 1)}\n",
       "            1041 |  # node_Concat_3341\n",
       "                    %\"val_3360\"<INT64,[4]> ⬅️ ::Concat(%\"val_3355\", %\"val_3352\", %\"val_3353\") {axis=0}\n",
       "            1042 |  # node_Reshape_3342\n",
       "                    %\"val_3361\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_3359\", %\"val_3360\") {allowzero=0}\n",
       "            1043 |  # node_Mul_3344\n",
       "                    %\"val_3363\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Mul(%\"getitem_168\", %\"val_3078\"{[0.353515625]})\n",
       "            1044 |  # node_Mul_3347\n",
       "                    %\"val_3366\"<FLOAT16,[None,None,None,None]> ⬅️ ::Mul(%\"val_3361\", %\"val_3078\"{[0.353515625]})\n",
       "            1045 |  # node_MatMul_3348\n",
       "                    %\"val_3367\"<FLOAT16,[None,12,577,None]> ⬅️ ::MatMul(%\"val_3363\", %\"val_3366\")\n",
       "            1046 |  # node_Softmax_3349\n",
       "                    %\"val_3368\"<FLOAT16,[None,12,577,None]> ⬅️ ::Softmax(%\"val_3367\") {axis=-1}\n",
       "            1047 |  # node_scaled_dot_product_attention_4\n",
       "                    %\"scaled_dot_product_attention_4\"<FLOAT16,[batch,12,577,64]> ⬅️ ::MatMul(%\"val_3368\", %\"getitem_170\")\n",
       "            1048 |  # node_transpose_5\n",
       "                    %\"transpose_5\"<FLOAT16,[batch,577,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_4\") {perm=(0, 2, 1, 3)}\n",
       "            1049 |  # node_view_116\n",
       "                    %\"view_116\"<FLOAT16,[batch,577,768]> ⬅️ ::Reshape(%\"transpose_5\", %\"val_3089\") {allowzero=1}\n",
       "            1050 |  # node_MatMul_3356\n",
       "                    %\"val_3375\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"view_116\", %\"val_3374\"{...})\n",
       "            1051 |  # node_linear_17\n",
       "                    %\"linear_17\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3375\", %\"pretrained.model.blocks.4.attn.proj.bias\"{...})\n",
       "            1052 |  # node_add_1588\n",
       "                    %\"add_1588\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1527\", %\"linear_17\")\n",
       "            1053 |  # node_layer_norm_9\n",
       "                    %\"layer_norm_9\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1588\", %\"pretrained.model.blocks.4.norm2.weight\"{...}, %\"pretrained.model.blocks.4.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1054 |  # node_MatMul_3358\n",
       "                    %\"val_3379\"<FLOAT16,[batch,577,3072]> ⬅️ ::MatMul(%\"layer_norm_9\", %\"val_3378\"{...})\n",
       "            1055 |  # node_linear_18\n",
       "                    %\"linear_18\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3379\", %\"pretrained.model.blocks.4.mlp.fc1.bias\"{...})\n",
       "            1056 |  # node_Div_3360\n",
       "                    %\"val_3381\"<FLOAT16,[batch,577,3072]> ⬅️ ::Div(%\"linear_18\", %\"val_3096\"{1.4140625})\n",
       "            1057 |  # node_Erf_3361\n",
       "                    %\"val_3382\"<FLOAT16,[batch,577,3072]> ⬅️ ::Erf(%\"val_3381\")\n",
       "            1058 |  # node_Add_3363\n",
       "                    %\"val_3384\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3382\", %\"val_3099\"{1.0})\n",
       "            1059 |  # node_Mul_3365\n",
       "                    %\"val_3386\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"val_3101\"{0.5}, %\"val_3384\")\n",
       "            1060 |  # node_gelu_4\n",
       "                    %\"gelu_4\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"linear_18\", %\"val_3386\")\n",
       "            1061 |  # node_MatMul_3367\n",
       "                    %\"val_3388\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"gelu_4\", %\"val_3387\"{...})\n",
       "            1062 |  # node_linear_19\n",
       "                    %\"linear_19\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3388\", %\"pretrained.model.blocks.4.mlp.fc2.bias\"{...})\n",
       "            1063 |  # node_add_1617\n",
       "                    %\"add_1617\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1588\", %\"linear_19\")\n",
       "            1064 |  # node_layer_norm_10\n",
       "                    %\"layer_norm_10\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1617\", %\"pretrained.model.blocks.5.norm1.weight\"{...}, %\"pretrained.model.blocks.5.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1065 |  # node_MatMul_3369\n",
       "                    %\"val_3392\"<FLOAT16,[batch,577,2304]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_3391\"{...})\n",
       "            1066 |  # node_linear_20\n",
       "                    %\"linear_20\"<FLOAT16,[batch,577,2304]> ⬅️ ::Add(%\"val_3392\", %\"pretrained.model.blocks.5.attn.qkv.bias\"{...})\n",
       "            1067 |  # node_view_117\n",
       "                    %\"view_117\"<FLOAT16,[batch,577,3,12,64]> ⬅️ ::Reshape(%\"linear_20\", %\"val_3041\") {allowzero=1}\n",
       "            1068 |  # node_permute_7\n",
       "                    %\"permute_7\"<FLOAT16,[3,batch,12,577,64]> ⬅️ ::Transpose(%\"view_117\") {perm=(2, 0, 3, 1, 4)}\n",
       "            1069 |  # node_Slice_3380\n",
       "                    %\"val_3403\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_7\", %\"val_5\"{[0]}, %\"val_8\"{[1]}, %\"val_5\"{[0]})\n",
       "            1070 |  # node_unbind_5__0\n",
       "                    %\"getitem_171\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3403\", %\"val_5\"{[0]})\n",
       "            1071 |  # node_Slice_3384\n",
       "                    %\"val_3407\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_7\", %\"val_8\"{[1]}, %\"val_3048\"{[2]}, %\"val_5\"{[0]})\n",
       "            1072 |  # node_unbind_5__1\n",
       "                    %\"getitem_172\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3407\", %\"val_5\"{[0]})\n",
       "            1073 |  # node_Slice_3388\n",
       "                    %\"val_3411\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_7\", %\"val_3048\"{[2]}, %\"val_3038\"{[3]}, %\"val_5\"{[0]})\n",
       "            1074 |  # node_unbind_5__2\n",
       "                    %\"getitem_173\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3411\", %\"val_5\"{[0]})\n",
       "            1075 |  # node_Shape_3398\n",
       "                    %\"val_3421\"<INT64,[4]> ⬅️ ::Shape(%\"getitem_172\") {start=0}\n",
       "            1076 |  # node_Slice_3400\n",
       "                    %\"val_3423\"<INT64,[1]> ⬅️ ::Slice(%\"val_3421\", %\"val_3066\"{[-1]}, %\"val_19\"{[9223372036854775807]})\n",
       "            1077 |  # node_Slice_3401\n",
       "                    %\"val_3424\"<INT64,[1]> ⬅️ ::Slice(%\"val_3421\", %\"val_3068\"{[-2]}, %\"val_3066\"{[-1]})\n",
       "            1078 |  # node_Slice_3403\n",
       "                    %\"val_3426\"<INT64,[2]> ⬅️ ::Slice(%\"val_3421\", %\"val_3070\"{[-9223372036854775808]}, %\"val_3068\"{[-2]})\n",
       "            1079 |  # node_Concat_3405\n",
       "                    %\"val_3428\"<INT64,[3]> ⬅️ ::Concat(%\"val_3066\"{[-1]}, %\"val_3424\", %\"val_3423\") {axis=0}\n",
       "            1080 |  # node_Reshape_3406\n",
       "                    %\"val_3429\"<FLOAT16,[None,None,None]> ⬅️ ::Reshape(%\"getitem_172\", %\"val_3428\") {allowzero=0}\n",
       "            1081 |  # node_Transpose_3407\n",
       "                    %\"val_3430\"<FLOAT16,[None,None,None]> ⬅️ ::Transpose(%\"val_3429\") {perm=(0, 2, 1)}\n",
       "            1082 |  # node_Concat_3408\n",
       "                    %\"val_3431\"<INT64,[4]> ⬅️ ::Concat(%\"val_3426\", %\"val_3423\", %\"val_3424\") {axis=0}\n",
       "            1083 |  # node_Reshape_3409\n",
       "                    %\"val_3432\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_3430\", %\"val_3431\") {allowzero=0}\n",
       "            1084 |  # node_Mul_3411\n",
       "                    %\"val_3434\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Mul(%\"getitem_171\", %\"val_3078\"{[0.353515625]})\n",
       "            1085 |  # node_Mul_3414\n",
       "                    %\"val_3437\"<FLOAT16,[None,None,None,None]> ⬅️ ::Mul(%\"val_3432\", %\"val_3078\"{[0.353515625]})\n",
       "            1086 |  # node_MatMul_3415\n",
       "                    %\"val_3438\"<FLOAT16,[None,12,577,None]> ⬅️ ::MatMul(%\"val_3434\", %\"val_3437\")\n",
       "            1087 |  # node_Softmax_3416\n",
       "                    %\"val_3439\"<FLOAT16,[None,12,577,None]> ⬅️ ::Softmax(%\"val_3438\") {axis=-1}\n",
       "            1088 |  # node_scaled_dot_product_attention_5\n",
       "                    %\"scaled_dot_product_attention_5\"<FLOAT16,[batch,12,577,64]> ⬅️ ::MatMul(%\"val_3439\", %\"getitem_173\")\n",
       "            1089 |  # node_transpose_6\n",
       "                    %\"transpose_6\"<FLOAT16,[batch,577,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_5\") {perm=(0, 2, 1, 3)}\n",
       "            1090 |  # node_view_118\n",
       "                    %\"view_118\"<FLOAT16,[batch,577,768]> ⬅️ ::Reshape(%\"transpose_6\", %\"val_3089\") {allowzero=1}\n",
       "            1091 |  # node_MatMul_3423\n",
       "                    %\"val_3446\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"view_118\", %\"val_3445\"{...})\n",
       "            1092 |  # node_linear_21\n",
       "                    %\"linear_21\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3446\", %\"pretrained.model.blocks.5.attn.proj.bias\"{...})\n",
       "            1093 |  # node_add_1678\n",
       "                    %\"add_1678\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1617\", %\"linear_21\")\n",
       "            1094 |  # node_layer_norm_11\n",
       "                    %\"layer_norm_11\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1678\", %\"pretrained.model.blocks.5.norm2.weight\"{...}, %\"pretrained.model.blocks.5.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1095 |  # node_MatMul_3425\n",
       "                    %\"val_3450\"<FLOAT16,[batch,577,3072]> ⬅️ ::MatMul(%\"layer_norm_11\", %\"val_3449\"{...})\n",
       "            1096 |  # node_linear_22\n",
       "                    %\"linear_22\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3450\", %\"pretrained.model.blocks.5.mlp.fc1.bias\"{...})\n",
       "            1097 |  # node_Div_3427\n",
       "                    %\"val_3452\"<FLOAT16,[batch,577,3072]> ⬅️ ::Div(%\"linear_22\", %\"val_3096\"{1.4140625})\n",
       "            1098 |  # node_Erf_3428\n",
       "                    %\"val_3453\"<FLOAT16,[batch,577,3072]> ⬅️ ::Erf(%\"val_3452\")\n",
       "            1099 |  # node_Add_3430\n",
       "                    %\"val_3455\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3453\", %\"val_3099\"{1.0})\n",
       "            1100 |  # node_Mul_3432\n",
       "                    %\"val_3457\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"val_3101\"{0.5}, %\"val_3455\")\n",
       "            1101 |  # node_gelu_5\n",
       "                    %\"gelu_5\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"linear_22\", %\"val_3457\")\n",
       "            1102 |  # node_MatMul_3434\n",
       "                    %\"val_3459\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"gelu_5\", %\"val_3458\"{...})\n",
       "            1103 |  # node_linear_23\n",
       "                    %\"linear_23\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3459\", %\"pretrained.model.blocks.5.mlp.fc2.bias\"{...})\n",
       "            1104 |  # node_add_1707\n",
       "                    %\"add_1707\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1678\", %\"linear_23\")\n",
       "            1105 |  # node_layer_norm_12\n",
       "                    %\"layer_norm_12\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1707\", %\"pretrained.model.blocks.6.norm1.weight\"{...}, %\"pretrained.model.blocks.6.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1106 |  # node_MatMul_3436\n",
       "                    %\"val_3463\"<FLOAT16,[batch,577,2304]> ⬅️ ::MatMul(%\"layer_norm_12\", %\"val_3462\"{...})\n",
       "            1107 |  # node_linear_24\n",
       "                    %\"linear_24\"<FLOAT16,[batch,577,2304]> ⬅️ ::Add(%\"val_3463\", %\"pretrained.model.blocks.6.attn.qkv.bias\"{...})\n",
       "            1108 |  # node_view_119\n",
       "                    %\"view_119\"<FLOAT16,[batch,577,3,12,64]> ⬅️ ::Reshape(%\"linear_24\", %\"val_3041\") {allowzero=1}\n",
       "            1109 |  # node_permute_8\n",
       "                    %\"permute_8\"<FLOAT16,[3,batch,12,577,64]> ⬅️ ::Transpose(%\"view_119\") {perm=(2, 0, 3, 1, 4)}\n",
       "            1110 |  # node_Slice_3447\n",
       "                    %\"val_3474\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_8\", %\"val_5\"{[0]}, %\"val_8\"{[1]}, %\"val_5\"{[0]})\n",
       "            1111 |  # node_unbind_6__0\n",
       "                    %\"getitem_174\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3474\", %\"val_5\"{[0]})\n",
       "            1112 |  # node_Slice_3451\n",
       "                    %\"val_3478\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_8\", %\"val_8\"{[1]}, %\"val_3048\"{[2]}, %\"val_5\"{[0]})\n",
       "            1113 |  # node_unbind_6__1\n",
       "                    %\"getitem_175\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3478\", %\"val_5\"{[0]})\n",
       "            1114 |  # node_Slice_3455\n",
       "                    %\"val_3482\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_8\", %\"val_3048\"{[2]}, %\"val_3038\"{[3]}, %\"val_5\"{[0]})\n",
       "            1115 |  # node_unbind_6__2\n",
       "                    %\"getitem_176\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3482\", %\"val_5\"{[0]})\n",
       "            1116 |  # node_Shape_3465\n",
       "                    %\"val_3492\"<INT64,[4]> ⬅️ ::Shape(%\"getitem_175\") {start=0}\n",
       "            1117 |  # node_Slice_3467\n",
       "                    %\"val_3494\"<INT64,[1]> ⬅️ ::Slice(%\"val_3492\", %\"val_3066\"{[-1]}, %\"val_19\"{[9223372036854775807]})\n",
       "            1118 |  # node_Slice_3468\n",
       "                    %\"val_3495\"<INT64,[1]> ⬅️ ::Slice(%\"val_3492\", %\"val_3068\"{[-2]}, %\"val_3066\"{[-1]})\n",
       "            1119 |  # node_Slice_3470\n",
       "                    %\"val_3497\"<INT64,[2]> ⬅️ ::Slice(%\"val_3492\", %\"val_3070\"{[-9223372036854775808]}, %\"val_3068\"{[-2]})\n",
       "            1120 |  # node_Concat_3472\n",
       "                    %\"val_3499\"<INT64,[3]> ⬅️ ::Concat(%\"val_3066\"{[-1]}, %\"val_3495\", %\"val_3494\") {axis=0}\n",
       "            1121 |  # node_Reshape_3473\n",
       "                    %\"val_3500\"<FLOAT16,[None,None,None]> ⬅️ ::Reshape(%\"getitem_175\", %\"val_3499\") {allowzero=0}\n",
       "            1122 |  # node_Transpose_3474\n",
       "                    %\"val_3501\"<FLOAT16,[None,None,None]> ⬅️ ::Transpose(%\"val_3500\") {perm=(0, 2, 1)}\n",
       "            1123 |  # node_Concat_3475\n",
       "                    %\"val_3502\"<INT64,[4]> ⬅️ ::Concat(%\"val_3497\", %\"val_3494\", %\"val_3495\") {axis=0}\n",
       "            1124 |  # node_Reshape_3476\n",
       "                    %\"val_3503\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_3501\", %\"val_3502\") {allowzero=0}\n",
       "            1125 |  # node_Mul_3478\n",
       "                    %\"val_3505\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Mul(%\"getitem_174\", %\"val_3078\"{[0.353515625]})\n",
       "            1126 |  # node_Mul_3481\n",
       "                    %\"val_3508\"<FLOAT16,[None,None,None,None]> ⬅️ ::Mul(%\"val_3503\", %\"val_3078\"{[0.353515625]})\n",
       "            1127 |  # node_MatMul_3482\n",
       "                    %\"val_3509\"<FLOAT16,[None,12,577,None]> ⬅️ ::MatMul(%\"val_3505\", %\"val_3508\")\n",
       "            1128 |  # node_Softmax_3483\n",
       "                    %\"val_3510\"<FLOAT16,[None,12,577,None]> ⬅️ ::Softmax(%\"val_3509\") {axis=-1}\n",
       "            1129 |  # node_scaled_dot_product_attention_6\n",
       "                    %\"scaled_dot_product_attention_6\"<FLOAT16,[batch,12,577,64]> ⬅️ ::MatMul(%\"val_3510\", %\"getitem_176\")\n",
       "            1130 |  # node_transpose_7\n",
       "                    %\"transpose_7\"<FLOAT16,[batch,577,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_6\") {perm=(0, 2, 1, 3)}\n",
       "            1131 |  # node_view_120\n",
       "                    %\"view_120\"<FLOAT16,[batch,577,768]> ⬅️ ::Reshape(%\"transpose_7\", %\"val_3089\") {allowzero=1}\n",
       "            1132 |  # node_MatMul_3490\n",
       "                    %\"val_3517\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"view_120\", %\"val_3516\"{...})\n",
       "            1133 |  # node_linear_25\n",
       "                    %\"linear_25\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3517\", %\"pretrained.model.blocks.6.attn.proj.bias\"{...})\n",
       "            1134 |  # node_add_1768\n",
       "                    %\"add_1768\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1707\", %\"linear_25\")\n",
       "            1135 |  # node_layer_norm_13\n",
       "                    %\"layer_norm_13\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1768\", %\"pretrained.model.blocks.6.norm2.weight\"{...}, %\"pretrained.model.blocks.6.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1136 |  # node_MatMul_3492\n",
       "                    %\"val_3521\"<FLOAT16,[batch,577,3072]> ⬅️ ::MatMul(%\"layer_norm_13\", %\"val_3520\"{...})\n",
       "            1137 |  # node_linear_26\n",
       "                    %\"linear_26\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3521\", %\"pretrained.model.blocks.6.mlp.fc1.bias\"{...})\n",
       "            1138 |  # node_Div_3494\n",
       "                    %\"val_3523\"<FLOAT16,[batch,577,3072]> ⬅️ ::Div(%\"linear_26\", %\"val_3096\"{1.4140625})\n",
       "            1139 |  # node_Erf_3495\n",
       "                    %\"val_3524\"<FLOAT16,[batch,577,3072]> ⬅️ ::Erf(%\"val_3523\")\n",
       "            1140 |  # node_Add_3497\n",
       "                    %\"val_3526\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3524\", %\"val_3099\"{1.0})\n",
       "            1141 |  # node_Mul_3499\n",
       "                    %\"val_3528\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"val_3101\"{0.5}, %\"val_3526\")\n",
       "            1142 |  # node_gelu_6\n",
       "                    %\"gelu_6\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"linear_26\", %\"val_3528\")\n",
       "            1143 |  # node_MatMul_3501\n",
       "                    %\"val_3530\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"gelu_6\", %\"val_3529\"{...})\n",
       "            1144 |  # node_linear_27\n",
       "                    %\"linear_27\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3530\", %\"pretrained.model.blocks.6.mlp.fc2.bias\"{...})\n",
       "            1145 |  # node_add_1797\n",
       "                    %\"add_1797\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1768\", %\"linear_27\")\n",
       "            1146 |  # node_layer_norm_14\n",
       "                    %\"layer_norm_14\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1797\", %\"pretrained.model.blocks.7.norm1.weight\"{...}, %\"pretrained.model.blocks.7.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1147 |  # node_MatMul_3503\n",
       "                    %\"val_3534\"<FLOAT16,[batch,577,2304]> ⬅️ ::MatMul(%\"layer_norm_14\", %\"val_3533\"{...})\n",
       "            1148 |  # node_linear_28\n",
       "                    %\"linear_28\"<FLOAT16,[batch,577,2304]> ⬅️ ::Add(%\"val_3534\", %\"pretrained.model.blocks.7.attn.qkv.bias\"{...})\n",
       "            1149 |  # node_view_121\n",
       "                    %\"view_121\"<FLOAT16,[batch,577,3,12,64]> ⬅️ ::Reshape(%\"linear_28\", %\"val_3041\") {allowzero=1}\n",
       "            1150 |  # node_permute_9\n",
       "                    %\"permute_9\"<FLOAT16,[3,batch,12,577,64]> ⬅️ ::Transpose(%\"view_121\") {perm=(2, 0, 3, 1, 4)}\n",
       "            1151 |  # node_Slice_3514\n",
       "                    %\"val_3545\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_9\", %\"val_5\"{[0]}, %\"val_8\"{[1]}, %\"val_5\"{[0]})\n",
       "            1152 |  # node_unbind_7__0\n",
       "                    %\"getitem_177\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3545\", %\"val_5\"{[0]})\n",
       "            1153 |  # node_Slice_3518\n",
       "                    %\"val_3549\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_9\", %\"val_8\"{[1]}, %\"val_3048\"{[2]}, %\"val_5\"{[0]})\n",
       "            1154 |  # node_unbind_7__1\n",
       "                    %\"getitem_178\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3549\", %\"val_5\"{[0]})\n",
       "            1155 |  # node_Slice_3522\n",
       "                    %\"val_3553\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_9\", %\"val_3048\"{[2]}, %\"val_3038\"{[3]}, %\"val_5\"{[0]})\n",
       "            1156 |  # node_unbind_7__2\n",
       "                    %\"getitem_179\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3553\", %\"val_5\"{[0]})\n",
       "            1157 |  # node_Shape_3532\n",
       "                    %\"val_3563\"<INT64,[4]> ⬅️ ::Shape(%\"getitem_178\") {start=0}\n",
       "            1158 |  # node_Slice_3534\n",
       "                    %\"val_3565\"<INT64,[1]> ⬅️ ::Slice(%\"val_3563\", %\"val_3066\"{[-1]}, %\"val_19\"{[9223372036854775807]})\n",
       "            1159 |  # node_Slice_3535\n",
       "                    %\"val_3566\"<INT64,[1]> ⬅️ ::Slice(%\"val_3563\", %\"val_3068\"{[-2]}, %\"val_3066\"{[-1]})\n",
       "            1160 |  # node_Slice_3537\n",
       "                    %\"val_3568\"<INT64,[2]> ⬅️ ::Slice(%\"val_3563\", %\"val_3070\"{[-9223372036854775808]}, %\"val_3068\"{[-2]})\n",
       "            1161 |  # node_Concat_3539\n",
       "                    %\"val_3570\"<INT64,[3]> ⬅️ ::Concat(%\"val_3066\"{[-1]}, %\"val_3566\", %\"val_3565\") {axis=0}\n",
       "            1162 |  # node_Reshape_3540\n",
       "                    %\"val_3571\"<FLOAT16,[None,None,None]> ⬅️ ::Reshape(%\"getitem_178\", %\"val_3570\") {allowzero=0}\n",
       "            1163 |  # node_Transpose_3541\n",
       "                    %\"val_3572\"<FLOAT16,[None,None,None]> ⬅️ ::Transpose(%\"val_3571\") {perm=(0, 2, 1)}\n",
       "            1164 |  # node_Concat_3542\n",
       "                    %\"val_3573\"<INT64,[4]> ⬅️ ::Concat(%\"val_3568\", %\"val_3565\", %\"val_3566\") {axis=0}\n",
       "            1165 |  # node_Reshape_3543\n",
       "                    %\"val_3574\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_3572\", %\"val_3573\") {allowzero=0}\n",
       "            1166 |  # node_Mul_3545\n",
       "                    %\"val_3576\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Mul(%\"getitem_177\", %\"val_3078\"{[0.353515625]})\n",
       "            1167 |  # node_Mul_3548\n",
       "                    %\"val_3579\"<FLOAT16,[None,None,None,None]> ⬅️ ::Mul(%\"val_3574\", %\"val_3078\"{[0.353515625]})\n",
       "            1168 |  # node_MatMul_3549\n",
       "                    %\"val_3580\"<FLOAT16,[None,12,577,None]> ⬅️ ::MatMul(%\"val_3576\", %\"val_3579\")\n",
       "            1169 |  # node_Softmax_3550\n",
       "                    %\"val_3581\"<FLOAT16,[None,12,577,None]> ⬅️ ::Softmax(%\"val_3580\") {axis=-1}\n",
       "            1170 |  # node_scaled_dot_product_attention_7\n",
       "                    %\"scaled_dot_product_attention_7\"<FLOAT16,[batch,12,577,64]> ⬅️ ::MatMul(%\"val_3581\", %\"getitem_179\")\n",
       "            1171 |  # node_transpose_8\n",
       "                    %\"transpose_8\"<FLOAT16,[batch,577,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_7\") {perm=(0, 2, 1, 3)}\n",
       "            1172 |  # node_view_122\n",
       "                    %\"view_122\"<FLOAT16,[batch,577,768]> ⬅️ ::Reshape(%\"transpose_8\", %\"val_3089\") {allowzero=1}\n",
       "            1173 |  # node_MatMul_3557\n",
       "                    %\"val_3588\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"view_122\", %\"val_3587\"{...})\n",
       "            1174 |  # node_linear_29\n",
       "                    %\"linear_29\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3588\", %\"pretrained.model.blocks.7.attn.proj.bias\"{...})\n",
       "            1175 |  # node_add_1858\n",
       "                    %\"add_1858\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1797\", %\"linear_29\")\n",
       "            1176 |  # node_layer_norm_15\n",
       "                    %\"layer_norm_15\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1858\", %\"pretrained.model.blocks.7.norm2.weight\"{...}, %\"pretrained.model.blocks.7.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1177 |  # node_MatMul_3559\n",
       "                    %\"val_3592\"<FLOAT16,[batch,577,3072]> ⬅️ ::MatMul(%\"layer_norm_15\", %\"val_3591\"{...})\n",
       "            1178 |  # node_linear_30\n",
       "                    %\"linear_30\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3592\", %\"pretrained.model.blocks.7.mlp.fc1.bias\"{...})\n",
       "            1179 |  # node_Div_3561\n",
       "                    %\"val_3594\"<FLOAT16,[batch,577,3072]> ⬅️ ::Div(%\"linear_30\", %\"val_3096\"{1.4140625})\n",
       "            1180 |  # node_Erf_3562\n",
       "                    %\"val_3595\"<FLOAT16,[batch,577,3072]> ⬅️ ::Erf(%\"val_3594\")\n",
       "            1181 |  # node_Add_3564\n",
       "                    %\"val_3597\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3595\", %\"val_3099\"{1.0})\n",
       "            1182 |  # node_Mul_3566\n",
       "                    %\"val_3599\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"val_3101\"{0.5}, %\"val_3597\")\n",
       "            1183 |  # node_gelu_7\n",
       "                    %\"gelu_7\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"linear_30\", %\"val_3599\")\n",
       "            1184 |  # node_MatMul_3568\n",
       "                    %\"val_3601\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"gelu_7\", %\"val_3600\"{...})\n",
       "            1185 |  # node_linear_31\n",
       "                    %\"linear_31\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3601\", %\"pretrained.model.blocks.7.mlp.fc2.bias\"{...})\n",
       "            1186 |  # node_add_1887\n",
       "                    %\"add_1887\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1858\", %\"linear_31\")\n",
       "            1187 |  # node_layer_norm_16\n",
       "                    %\"layer_norm_16\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1887\", %\"pretrained.model.blocks.8.norm1.weight\"{...}, %\"pretrained.model.blocks.8.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1188 |  # node_MatMul_3570\n",
       "                    %\"val_3605\"<FLOAT16,[batch,577,2304]> ⬅️ ::MatMul(%\"layer_norm_16\", %\"val_3604\"{...})\n",
       "            1189 |  # node_linear_32\n",
       "                    %\"linear_32\"<FLOAT16,[batch,577,2304]> ⬅️ ::Add(%\"val_3605\", %\"pretrained.model.blocks.8.attn.qkv.bias\"{...})\n",
       "            1190 |  # node_view_123\n",
       "                    %\"view_123\"<FLOAT16,[batch,577,3,12,64]> ⬅️ ::Reshape(%\"linear_32\", %\"val_3041\") {allowzero=1}\n",
       "            1191 |  # node_permute_10\n",
       "                    %\"permute_10\"<FLOAT16,[3,batch,12,577,64]> ⬅️ ::Transpose(%\"view_123\") {perm=(2, 0, 3, 1, 4)}\n",
       "            1192 |  # node_Slice_3581\n",
       "                    %\"val_3616\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_10\", %\"val_5\"{[0]}, %\"val_8\"{[1]}, %\"val_5\"{[0]})\n",
       "            1193 |  # node_unbind_8__0\n",
       "                    %\"getitem_180\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3616\", %\"val_5\"{[0]})\n",
       "            1194 |  # node_Slice_3585\n",
       "                    %\"val_3620\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_10\", %\"val_8\"{[1]}, %\"val_3048\"{[2]}, %\"val_5\"{[0]})\n",
       "            1195 |  # node_unbind_8__1\n",
       "                    %\"getitem_181\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3620\", %\"val_5\"{[0]})\n",
       "            1196 |  # node_Slice_3589\n",
       "                    %\"val_3624\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_10\", %\"val_3048\"{[2]}, %\"val_3038\"{[3]}, %\"val_5\"{[0]})\n",
       "            1197 |  # node_unbind_8__2\n",
       "                    %\"getitem_182\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3624\", %\"val_5\"{[0]})\n",
       "            1198 |  # node_Shape_3599\n",
       "                    %\"val_3634\"<INT64,[4]> ⬅️ ::Shape(%\"getitem_181\") {start=0}\n",
       "            1199 |  # node_Slice_3601\n",
       "                    %\"val_3636\"<INT64,[1]> ⬅️ ::Slice(%\"val_3634\", %\"val_3066\"{[-1]}, %\"val_19\"{[9223372036854775807]})\n",
       "            1200 |  # node_Slice_3602\n",
       "                    %\"val_3637\"<INT64,[1]> ⬅️ ::Slice(%\"val_3634\", %\"val_3068\"{[-2]}, %\"val_3066\"{[-1]})\n",
       "            1201 |  # node_Slice_3604\n",
       "                    %\"val_3639\"<INT64,[2]> ⬅️ ::Slice(%\"val_3634\", %\"val_3070\"{[-9223372036854775808]}, %\"val_3068\"{[-2]})\n",
       "            1202 |  # node_Concat_3606\n",
       "                    %\"val_3641\"<INT64,[3]> ⬅️ ::Concat(%\"val_3066\"{[-1]}, %\"val_3637\", %\"val_3636\") {axis=0}\n",
       "            1203 |  # node_Reshape_3607\n",
       "                    %\"val_3642\"<FLOAT16,[None,None,None]> ⬅️ ::Reshape(%\"getitem_181\", %\"val_3641\") {allowzero=0}\n",
       "            1204 |  # node_Transpose_3608\n",
       "                    %\"val_3643\"<FLOAT16,[None,None,None]> ⬅️ ::Transpose(%\"val_3642\") {perm=(0, 2, 1)}\n",
       "            1205 |  # node_Concat_3609\n",
       "                    %\"val_3644\"<INT64,[4]> ⬅️ ::Concat(%\"val_3639\", %\"val_3636\", %\"val_3637\") {axis=0}\n",
       "            1206 |  # node_Reshape_3610\n",
       "                    %\"val_3645\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_3643\", %\"val_3644\") {allowzero=0}\n",
       "            1207 |  # node_Mul_3612\n",
       "                    %\"val_3647\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Mul(%\"getitem_180\", %\"val_3078\"{[0.353515625]})\n",
       "            1208 |  # node_Mul_3615\n",
       "                    %\"val_3650\"<FLOAT16,[None,None,None,None]> ⬅️ ::Mul(%\"val_3645\", %\"val_3078\"{[0.353515625]})\n",
       "            1209 |  # node_MatMul_3616\n",
       "                    %\"val_3651\"<FLOAT16,[None,12,577,None]> ⬅️ ::MatMul(%\"val_3647\", %\"val_3650\")\n",
       "            1210 |  # node_Softmax_3617\n",
       "                    %\"val_3652\"<FLOAT16,[None,12,577,None]> ⬅️ ::Softmax(%\"val_3651\") {axis=-1}\n",
       "            1211 |  # node_scaled_dot_product_attention_8\n",
       "                    %\"scaled_dot_product_attention_8\"<FLOAT16,[batch,12,577,64]> ⬅️ ::MatMul(%\"val_3652\", %\"getitem_182\")\n",
       "            1212 |  # node_transpose_9\n",
       "                    %\"transpose_9\"<FLOAT16,[batch,577,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_8\") {perm=(0, 2, 1, 3)}\n",
       "            1213 |  # node_view_124\n",
       "                    %\"view_124\"<FLOAT16,[batch,577,768]> ⬅️ ::Reshape(%\"transpose_9\", %\"val_3089\") {allowzero=1}\n",
       "            1214 |  # node_MatMul_3624\n",
       "                    %\"val_3659\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"view_124\", %\"val_3658\"{...})\n",
       "            1215 |  # node_linear_33\n",
       "                    %\"linear_33\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3659\", %\"pretrained.model.blocks.8.attn.proj.bias\"{...})\n",
       "            1216 |  # node_add_1948\n",
       "                    %\"add_1948\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1887\", %\"linear_33\")\n",
       "            1217 |  # node_layer_norm_17\n",
       "                    %\"layer_norm_17\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1948\", %\"pretrained.model.blocks.8.norm2.weight\"{...}, %\"pretrained.model.blocks.8.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1218 |  # node_MatMul_3626\n",
       "                    %\"val_3663\"<FLOAT16,[batch,577,3072]> ⬅️ ::MatMul(%\"layer_norm_17\", %\"val_3662\"{...})\n",
       "            1219 |  # node_linear_34\n",
       "                    %\"linear_34\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3663\", %\"pretrained.model.blocks.8.mlp.fc1.bias\"{...})\n",
       "            1220 |  # node_Div_3628\n",
       "                    %\"val_3665\"<FLOAT16,[batch,577,3072]> ⬅️ ::Div(%\"linear_34\", %\"val_3096\"{1.4140625})\n",
       "            1221 |  # node_Erf_3629\n",
       "                    %\"val_3666\"<FLOAT16,[batch,577,3072]> ⬅️ ::Erf(%\"val_3665\")\n",
       "            1222 |  # node_Add_3631\n",
       "                    %\"val_3668\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3666\", %\"val_3099\"{1.0})\n",
       "            1223 |  # node_Mul_3633\n",
       "                    %\"val_3670\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"val_3101\"{0.5}, %\"val_3668\")\n",
       "            1224 |  # node_gelu_8\n",
       "                    %\"gelu_8\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"linear_34\", %\"val_3670\")\n",
       "            1225 |  # node_MatMul_3635\n",
       "                    %\"val_3672\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"gelu_8\", %\"val_3671\"{...})\n",
       "            1226 |  # node_linear_35\n",
       "                    %\"linear_35\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3672\", %\"pretrained.model.blocks.8.mlp.fc2.bias\"{...})\n",
       "            1227 |  # node_add_1977\n",
       "                    %\"add_1977\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1948\", %\"linear_35\")\n",
       "            1228 |  # node_layer_norm_18\n",
       "                    %\"layer_norm_18\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_1977\", %\"pretrained.model.blocks.9.norm1.weight\"{...}, %\"pretrained.model.blocks.9.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1229 |  # node_MatMul_3637\n",
       "                    %\"val_3676\"<FLOAT16,[batch,577,2304]> ⬅️ ::MatMul(%\"layer_norm_18\", %\"val_3675\"{...})\n",
       "            1230 |  # node_linear_36\n",
       "                    %\"linear_36\"<FLOAT16,[batch,577,2304]> ⬅️ ::Add(%\"val_3676\", %\"pretrained.model.blocks.9.attn.qkv.bias\"{...})\n",
       "            1231 |  # node_view_125\n",
       "                    %\"view_125\"<FLOAT16,[batch,577,3,12,64]> ⬅️ ::Reshape(%\"linear_36\", %\"val_3041\") {allowzero=1}\n",
       "            1232 |  # node_permute_11\n",
       "                    %\"permute_11\"<FLOAT16,[3,batch,12,577,64]> ⬅️ ::Transpose(%\"view_125\") {perm=(2, 0, 3, 1, 4)}\n",
       "            1233 |  # node_Slice_3648\n",
       "                    %\"val_3687\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_11\", %\"val_5\"{[0]}, %\"val_8\"{[1]}, %\"val_5\"{[0]})\n",
       "            1234 |  # node_unbind_9__0\n",
       "                    %\"getitem_183\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3687\", %\"val_5\"{[0]})\n",
       "            1235 |  # node_Slice_3652\n",
       "                    %\"val_3691\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_11\", %\"val_8\"{[1]}, %\"val_3048\"{[2]}, %\"val_5\"{[0]})\n",
       "            1236 |  # node_unbind_9__1\n",
       "                    %\"getitem_184\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3691\", %\"val_5\"{[0]})\n",
       "            1237 |  # node_Slice_3656\n",
       "                    %\"val_3695\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_11\", %\"val_3048\"{[2]}, %\"val_3038\"{[3]}, %\"val_5\"{[0]})\n",
       "            1238 |  # node_unbind_9__2\n",
       "                    %\"getitem_185\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3695\", %\"val_5\"{[0]})\n",
       "            1239 |  # node_Shape_3666\n",
       "                    %\"val_3705\"<INT64,[4]> ⬅️ ::Shape(%\"getitem_184\") {start=0}\n",
       "            1240 |  # node_Slice_3668\n",
       "                    %\"val_3707\"<INT64,[1]> ⬅️ ::Slice(%\"val_3705\", %\"val_3066\"{[-1]}, %\"val_19\"{[9223372036854775807]})\n",
       "            1241 |  # node_Slice_3669\n",
       "                    %\"val_3708\"<INT64,[1]> ⬅️ ::Slice(%\"val_3705\", %\"val_3068\"{[-2]}, %\"val_3066\"{[-1]})\n",
       "            1242 |  # node_Slice_3671\n",
       "                    %\"val_3710\"<INT64,[2]> ⬅️ ::Slice(%\"val_3705\", %\"val_3070\"{[-9223372036854775808]}, %\"val_3068\"{[-2]})\n",
       "            1243 |  # node_Concat_3673\n",
       "                    %\"val_3712\"<INT64,[3]> ⬅️ ::Concat(%\"val_3066\"{[-1]}, %\"val_3708\", %\"val_3707\") {axis=0}\n",
       "            1244 |  # node_Reshape_3674\n",
       "                    %\"val_3713\"<FLOAT16,[None,None,None]> ⬅️ ::Reshape(%\"getitem_184\", %\"val_3712\") {allowzero=0}\n",
       "            1245 |  # node_Transpose_3675\n",
       "                    %\"val_3714\"<FLOAT16,[None,None,None]> ⬅️ ::Transpose(%\"val_3713\") {perm=(0, 2, 1)}\n",
       "            1246 |  # node_Concat_3676\n",
       "                    %\"val_3715\"<INT64,[4]> ⬅️ ::Concat(%\"val_3710\", %\"val_3707\", %\"val_3708\") {axis=0}\n",
       "            1247 |  # node_Reshape_3677\n",
       "                    %\"val_3716\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_3714\", %\"val_3715\") {allowzero=0}\n",
       "            1248 |  # node_Mul_3679\n",
       "                    %\"val_3718\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Mul(%\"getitem_183\", %\"val_3078\"{[0.353515625]})\n",
       "            1249 |  # node_Mul_3682\n",
       "                    %\"val_3721\"<FLOAT16,[None,None,None,None]> ⬅️ ::Mul(%\"val_3716\", %\"val_3078\"{[0.353515625]})\n",
       "            1250 |  # node_MatMul_3683\n",
       "                    %\"val_3722\"<FLOAT16,[None,12,577,None]> ⬅️ ::MatMul(%\"val_3718\", %\"val_3721\")\n",
       "            1251 |  # node_Softmax_3684\n",
       "                    %\"val_3723\"<FLOAT16,[None,12,577,None]> ⬅️ ::Softmax(%\"val_3722\") {axis=-1}\n",
       "            1252 |  # node_scaled_dot_product_attention_9\n",
       "                    %\"scaled_dot_product_attention_9\"<FLOAT16,[batch,12,577,64]> ⬅️ ::MatMul(%\"val_3723\", %\"getitem_185\")\n",
       "            1253 |  # node_transpose_10\n",
       "                    %\"transpose_10\"<FLOAT16,[batch,577,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_9\") {perm=(0, 2, 1, 3)}\n",
       "            1254 |  # node_view_126\n",
       "                    %\"view_126\"<FLOAT16,[batch,577,768]> ⬅️ ::Reshape(%\"transpose_10\", %\"val_3089\") {allowzero=1}\n",
       "            1255 |  # node_MatMul_3691\n",
       "                    %\"val_3730\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"view_126\", %\"val_3729\"{...})\n",
       "            1256 |  # node_linear_37\n",
       "                    %\"linear_37\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3730\", %\"pretrained.model.blocks.9.attn.proj.bias\"{...})\n",
       "            1257 |  # node_add_2038\n",
       "                    %\"add_2038\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_1977\", %\"linear_37\")\n",
       "            1258 |  # node_layer_norm_19\n",
       "                    %\"layer_norm_19\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_2038\", %\"pretrained.model.blocks.9.norm2.weight\"{...}, %\"pretrained.model.blocks.9.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1259 |  # node_MatMul_3693\n",
       "                    %\"val_3734\"<FLOAT16,[batch,577,3072]> ⬅️ ::MatMul(%\"layer_norm_19\", %\"val_3733\"{...})\n",
       "            1260 |  # node_linear_38\n",
       "                    %\"linear_38\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3734\", %\"pretrained.model.blocks.9.mlp.fc1.bias\"{...})\n",
       "            1261 |  # node_Div_3695\n",
       "                    %\"val_3736\"<FLOAT16,[batch,577,3072]> ⬅️ ::Div(%\"linear_38\", %\"val_3096\"{1.4140625})\n",
       "            1262 |  # node_Erf_3696\n",
       "                    %\"val_3737\"<FLOAT16,[batch,577,3072]> ⬅️ ::Erf(%\"val_3736\")\n",
       "            1263 |  # node_Add_3698\n",
       "                    %\"val_3739\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3737\", %\"val_3099\"{1.0})\n",
       "            1264 |  # node_Mul_3700\n",
       "                    %\"val_3741\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"val_3101\"{0.5}, %\"val_3739\")\n",
       "            1265 |  # node_gelu_9\n",
       "                    %\"gelu_9\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"linear_38\", %\"val_3741\")\n",
       "            1266 |  # node_MatMul_3702\n",
       "                    %\"val_3743\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"gelu_9\", %\"val_3742\"{...})\n",
       "            1267 |  # node_linear_39\n",
       "                    %\"linear_39\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3743\", %\"pretrained.model.blocks.9.mlp.fc2.bias\"{...})\n",
       "            1268 |  # node_add_2067\n",
       "                    %\"add_2067\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_2038\", %\"linear_39\")\n",
       "            1269 |  # node_layer_norm_20\n",
       "                    %\"layer_norm_20\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_2067\", %\"pretrained.model.blocks.10.norm1.weight\"{...}, %\"pretrained.model.blocks.10.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1270 |  # node_MatMul_3704\n",
       "                    %\"val_3747\"<FLOAT16,[batch,577,2304]> ⬅️ ::MatMul(%\"layer_norm_20\", %\"val_3746\"{...})\n",
       "            1271 |  # node_linear_40\n",
       "                    %\"linear_40\"<FLOAT16,[batch,577,2304]> ⬅️ ::Add(%\"val_3747\", %\"pretrained.model.blocks.10.attn.qkv.bias\"{...})\n",
       "            1272 |  # node_view_127\n",
       "                    %\"view_127\"<FLOAT16,[batch,577,3,12,64]> ⬅️ ::Reshape(%\"linear_40\", %\"val_3041\") {allowzero=1}\n",
       "            1273 |  # node_permute_12\n",
       "                    %\"permute_12\"<FLOAT16,[3,batch,12,577,64]> ⬅️ ::Transpose(%\"view_127\") {perm=(2, 0, 3, 1, 4)}\n",
       "            1274 |  # node_Slice_3715\n",
       "                    %\"val_3758\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_12\", %\"val_5\"{[0]}, %\"val_8\"{[1]}, %\"val_5\"{[0]})\n",
       "            1275 |  # node_unbind_10__0\n",
       "                    %\"getitem_186\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3758\", %\"val_5\"{[0]})\n",
       "            1276 |  # node_Slice_3719\n",
       "                    %\"val_3762\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_12\", %\"val_8\"{[1]}, %\"val_3048\"{[2]}, %\"val_5\"{[0]})\n",
       "            1277 |  # node_unbind_10__1\n",
       "                    %\"getitem_187\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3762\", %\"val_5\"{[0]})\n",
       "            1278 |  # node_Slice_3723\n",
       "                    %\"val_3766\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_12\", %\"val_3048\"{[2]}, %\"val_3038\"{[3]}, %\"val_5\"{[0]})\n",
       "            1279 |  # node_unbind_10__2\n",
       "                    %\"getitem_188\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3766\", %\"val_5\"{[0]})\n",
       "            1280 |  # node_Shape_3733\n",
       "                    %\"val_3776\"<INT64,[4]> ⬅️ ::Shape(%\"getitem_187\") {start=0}\n",
       "            1281 |  # node_Slice_3735\n",
       "                    %\"val_3778\"<INT64,[1]> ⬅️ ::Slice(%\"val_3776\", %\"val_3066\"{[-1]}, %\"val_19\"{[9223372036854775807]})\n",
       "            1282 |  # node_Slice_3736\n",
       "                    %\"val_3779\"<INT64,[1]> ⬅️ ::Slice(%\"val_3776\", %\"val_3068\"{[-2]}, %\"val_3066\"{[-1]})\n",
       "            1283 |  # node_Slice_3738\n",
       "                    %\"val_3781\"<INT64,[2]> ⬅️ ::Slice(%\"val_3776\", %\"val_3070\"{[-9223372036854775808]}, %\"val_3068\"{[-2]})\n",
       "            1284 |  # node_Concat_3740\n",
       "                    %\"val_3783\"<INT64,[3]> ⬅️ ::Concat(%\"val_3066\"{[-1]}, %\"val_3779\", %\"val_3778\") {axis=0}\n",
       "            1285 |  # node_Reshape_3741\n",
       "                    %\"val_3784\"<FLOAT16,[None,None,None]> ⬅️ ::Reshape(%\"getitem_187\", %\"val_3783\") {allowzero=0}\n",
       "            1286 |  # node_Transpose_3742\n",
       "                    %\"val_3785\"<FLOAT16,[None,None,None]> ⬅️ ::Transpose(%\"val_3784\") {perm=(0, 2, 1)}\n",
       "            1287 |  # node_Concat_3743\n",
       "                    %\"val_3786\"<INT64,[4]> ⬅️ ::Concat(%\"val_3781\", %\"val_3778\", %\"val_3779\") {axis=0}\n",
       "            1288 |  # node_Reshape_3744\n",
       "                    %\"val_3787\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_3785\", %\"val_3786\") {allowzero=0}\n",
       "            1289 |  # node_Mul_3746\n",
       "                    %\"val_3789\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Mul(%\"getitem_186\", %\"val_3078\"{[0.353515625]})\n",
       "            1290 |  # node_Mul_3749\n",
       "                    %\"val_3792\"<FLOAT16,[None,None,None,None]> ⬅️ ::Mul(%\"val_3787\", %\"val_3078\"{[0.353515625]})\n",
       "            1291 |  # node_MatMul_3750\n",
       "                    %\"val_3793\"<FLOAT16,[None,12,577,None]> ⬅️ ::MatMul(%\"val_3789\", %\"val_3792\")\n",
       "            1292 |  # node_Softmax_3751\n",
       "                    %\"val_3794\"<FLOAT16,[None,12,577,None]> ⬅️ ::Softmax(%\"val_3793\") {axis=-1}\n",
       "            1293 |  # node_scaled_dot_product_attention_10\n",
       "                    %\"scaled_dot_product_attention_10\"<FLOAT16,[batch,12,577,64]> ⬅️ ::MatMul(%\"val_3794\", %\"getitem_188\")\n",
       "            1294 |  # node_transpose_11\n",
       "                    %\"transpose_11\"<FLOAT16,[batch,577,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_10\") {perm=(0, 2, 1, 3)}\n",
       "            1295 |  # node_view_128\n",
       "                    %\"view_128\"<FLOAT16,[batch,577,768]> ⬅️ ::Reshape(%\"transpose_11\", %\"val_3089\") {allowzero=1}\n",
       "            1296 |  # node_MatMul_3758\n",
       "                    %\"val_3801\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"view_128\", %\"val_3800\"{...})\n",
       "            1297 |  # node_linear_41\n",
       "                    %\"linear_41\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3801\", %\"pretrained.model.blocks.10.attn.proj.bias\"{...})\n",
       "            1298 |  # node_add_2128\n",
       "                    %\"add_2128\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_2067\", %\"linear_41\")\n",
       "            1299 |  # node_layer_norm_21\n",
       "                    %\"layer_norm_21\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_2128\", %\"pretrained.model.blocks.10.norm2.weight\"{...}, %\"pretrained.model.blocks.10.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1300 |  # node_MatMul_3760\n",
       "                    %\"val_3805\"<FLOAT16,[batch,577,3072]> ⬅️ ::MatMul(%\"layer_norm_21\", %\"val_3804\"{...})\n",
       "            1301 |  # node_linear_42\n",
       "                    %\"linear_42\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3805\", %\"pretrained.model.blocks.10.mlp.fc1.bias\"{...})\n",
       "            1302 |  # node_Div_3762\n",
       "                    %\"val_3807\"<FLOAT16,[batch,577,3072]> ⬅️ ::Div(%\"linear_42\", %\"val_3096\"{1.4140625})\n",
       "            1303 |  # node_Erf_3763\n",
       "                    %\"val_3808\"<FLOAT16,[batch,577,3072]> ⬅️ ::Erf(%\"val_3807\")\n",
       "            1304 |  # node_Add_3765\n",
       "                    %\"val_3810\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3808\", %\"val_3099\"{1.0})\n",
       "            1305 |  # node_Mul_3767\n",
       "                    %\"val_3812\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"val_3101\"{0.5}, %\"val_3810\")\n",
       "            1306 |  # node_gelu_10\n",
       "                    %\"gelu_10\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"linear_42\", %\"val_3812\")\n",
       "            1307 |  # node_MatMul_3769\n",
       "                    %\"val_3814\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"gelu_10\", %\"val_3813\"{...})\n",
       "            1308 |  # node_linear_43\n",
       "                    %\"linear_43\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3814\", %\"pretrained.model.blocks.10.mlp.fc2.bias\"{...})\n",
       "            1309 |  # node_add_2157\n",
       "                    %\"add_2157\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_2128\", %\"linear_43\")\n",
       "            1310 |  # node_layer_norm_22\n",
       "                    %\"layer_norm_22\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_2157\", %\"pretrained.model.blocks.11.norm1.weight\"{...}, %\"pretrained.model.blocks.11.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1311 |  # node_MatMul_3771\n",
       "                    %\"val_3818\"<FLOAT16,[batch,577,2304]> ⬅️ ::MatMul(%\"layer_norm_22\", %\"val_3817\"{...})\n",
       "            1312 |  # node_linear_44\n",
       "                    %\"linear_44\"<FLOAT16,[batch,577,2304]> ⬅️ ::Add(%\"val_3818\", %\"pretrained.model.blocks.11.attn.qkv.bias\"{...})\n",
       "            1313 |  # node_view_129\n",
       "                    %\"view_129\"<FLOAT16,[batch,577,3,12,64]> ⬅️ ::Reshape(%\"linear_44\", %\"val_3041\") {allowzero=1}\n",
       "            1314 |  # node_permute_13\n",
       "                    %\"permute_13\"<FLOAT16,[3,batch,12,577,64]> ⬅️ ::Transpose(%\"view_129\") {perm=(2, 0, 3, 1, 4)}\n",
       "            1315 |  # node_Slice_3782\n",
       "                    %\"val_3829\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_13\", %\"val_5\"{[0]}, %\"val_8\"{[1]}, %\"val_5\"{[0]})\n",
       "            1316 |  # node_unbind_11__0\n",
       "                    %\"getitem_189\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3829\", %\"val_5\"{[0]})\n",
       "            1317 |  # node_Slice_3786\n",
       "                    %\"val_3833\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_13\", %\"val_8\"{[1]}, %\"val_3048\"{[2]}, %\"val_5\"{[0]})\n",
       "            1318 |  # node_unbind_11__1\n",
       "                    %\"getitem_190\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3833\", %\"val_5\"{[0]})\n",
       "            1319 |  # node_Slice_3790\n",
       "                    %\"val_3837\"<FLOAT16,[1,batch,12,577,64]> ⬅️ ::Slice(%\"permute_13\", %\"val_3048\"{[2]}, %\"val_3038\"{[3]}, %\"val_5\"{[0]})\n",
       "            1320 |  # node_unbind_11__2\n",
       "                    %\"getitem_191\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Squeeze(%\"val_3837\", %\"val_5\"{[0]})\n",
       "            1321 |  # node_Shape_3800\n",
       "                    %\"val_3847\"<INT64,[4]> ⬅️ ::Shape(%\"getitem_190\") {start=0}\n",
       "            1322 |  # node_Slice_3802\n",
       "                    %\"val_3849\"<INT64,[1]> ⬅️ ::Slice(%\"val_3847\", %\"val_3066\"{[-1]}, %\"val_19\"{[9223372036854775807]})\n",
       "            1323 |  # node_Slice_3803\n",
       "                    %\"val_3850\"<INT64,[1]> ⬅️ ::Slice(%\"val_3847\", %\"val_3068\"{[-2]}, %\"val_3066\"{[-1]})\n",
       "            1324 |  # node_Slice_3805\n",
       "                    %\"val_3852\"<INT64,[2]> ⬅️ ::Slice(%\"val_3847\", %\"val_3070\"{[-9223372036854775808]}, %\"val_3068\"{[-2]})\n",
       "            1325 |  # node_Concat_3807\n",
       "                    %\"val_3854\"<INT64,[3]> ⬅️ ::Concat(%\"val_3066\"{[-1]}, %\"val_3850\", %\"val_3849\") {axis=0}\n",
       "            1326 |  # node_Reshape_3808\n",
       "                    %\"val_3855\"<FLOAT16,[None,None,None]> ⬅️ ::Reshape(%\"getitem_190\", %\"val_3854\") {allowzero=0}\n",
       "            1327 |  # node_Transpose_3809\n",
       "                    %\"val_3856\"<FLOAT16,[None,None,None]> ⬅️ ::Transpose(%\"val_3855\") {perm=(0, 2, 1)}\n",
       "            1328 |  # node_Concat_3810\n",
       "                    %\"val_3857\"<INT64,[4]> ⬅️ ::Concat(%\"val_3852\", %\"val_3849\", %\"val_3850\") {axis=0}\n",
       "            1329 |  # node_Reshape_3811\n",
       "                    %\"val_3858\"<FLOAT16,[None,None,None,None]> ⬅️ ::Reshape(%\"val_3856\", %\"val_3857\") {allowzero=0}\n",
       "            1330 |  # node_Mul_3813\n",
       "                    %\"val_3860\"<FLOAT16,[batch,12,577,64]> ⬅️ ::Mul(%\"getitem_189\", %\"val_3078\"{[0.353515625]})\n",
       "            1331 |  # node_Mul_3816\n",
       "                    %\"val_3863\"<FLOAT16,[None,None,None,None]> ⬅️ ::Mul(%\"val_3858\", %\"val_3078\"{[0.353515625]})\n",
       "            1332 |  # node_MatMul_3817\n",
       "                    %\"val_3864\"<FLOAT16,[None,12,577,None]> ⬅️ ::MatMul(%\"val_3860\", %\"val_3863\")\n",
       "            1333 |  # node_Softmax_3818\n",
       "                    %\"val_3865\"<FLOAT16,[None,12,577,None]> ⬅️ ::Softmax(%\"val_3864\") {axis=-1}\n",
       "            1334 |  # node_scaled_dot_product_attention_11\n",
       "                    %\"scaled_dot_product_attention_11\"<FLOAT16,[batch,12,577,64]> ⬅️ ::MatMul(%\"val_3865\", %\"getitem_191\")\n",
       "            1335 |  # node_transpose_12\n",
       "                    %\"transpose_12\"<FLOAT16,[batch,577,12,64]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_11\") {perm=(0, 2, 1, 3)}\n",
       "            1336 |  # node_view_130\n",
       "                    %\"view_130\"<FLOAT16,[batch,577,768]> ⬅️ ::Reshape(%\"transpose_12\", %\"val_3089\") {allowzero=1}\n",
       "            1337 |  # node_MatMul_3825\n",
       "                    %\"val_3872\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"view_130\", %\"val_3871\"{...})\n",
       "            1338 |  # node_linear_45\n",
       "                    %\"linear_45\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3872\", %\"pretrained.model.blocks.11.attn.proj.bias\"{...})\n",
       "            1339 |  # node_add_2218\n",
       "                    %\"add_2218\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_2157\", %\"linear_45\")\n",
       "            1340 |  # node_layer_norm_23\n",
       "                    %\"layer_norm_23\"<FLOAT16,[batch,577,768]> ⬅️ ::LayerNormalization(%\"add_2218\", %\"pretrained.model.blocks.11.norm2.weight\"{...}, %\"pretrained.model.blocks.11.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            1341 |  # node_MatMul_3827\n",
       "                    %\"val_3876\"<FLOAT16,[batch,577,3072]> ⬅️ ::MatMul(%\"layer_norm_23\", %\"val_3875\"{...})\n",
       "            1342 |  # node_linear_46\n",
       "                    %\"linear_46\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3876\", %\"pretrained.model.blocks.11.mlp.fc1.bias\"{...})\n",
       "            1343 |  # node_Div_3829\n",
       "                    %\"val_3878\"<FLOAT16,[batch,577,3072]> ⬅️ ::Div(%\"linear_46\", %\"val_3096\"{1.4140625})\n",
       "            1344 |  # node_Erf_3830\n",
       "                    %\"val_3879\"<FLOAT16,[batch,577,3072]> ⬅️ ::Erf(%\"val_3878\")\n",
       "            1345 |  # node_Add_3832\n",
       "                    %\"val_3881\"<FLOAT16,[batch,577,3072]> ⬅️ ::Add(%\"val_3879\", %\"val_3099\"{1.0})\n",
       "            1346 |  # node_Mul_3834\n",
       "                    %\"val_3883\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"val_3101\"{0.5}, %\"val_3881\")\n",
       "            1347 |  # node_gelu_11\n",
       "                    %\"gelu_11\"<FLOAT16,[batch,577,3072]> ⬅️ ::Mul(%\"linear_46\", %\"val_3883\")\n",
       "            1348 |  # node_MatMul_3836\n",
       "                    %\"val_3885\"<FLOAT16,[batch,577,768]> ⬅️ ::MatMul(%\"gelu_11\", %\"val_3884\"{...})\n",
       "            1349 |  # node_linear_47\n",
       "                    %\"linear_47\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"val_3885\", %\"pretrained.model.blocks.11.mlp.fc2.bias\"{...})\n",
       "            1350 |  # node_add_2247\n",
       "                    %\"add_2247\"<FLOAT16,[batch,577,768]> ⬅️ ::Add(%\"add_2218\", %\"linear_47\")\n",
       "            1351 |  # node_select_1\n",
       "                    %\"select_1\"<FLOAT16,[batch,768]> ⬅️ ::Gather(%\"add_1977\", %\"val_2\"{0}) {axis=1}\n",
       "            1352 |  # node_unsqueeze\n",
       "                    %\"unsqueeze\"<FLOAT16,[batch,1,768]> ⬅️ ::Unsqueeze(%\"select_1\", %\"val_8\"{[1]})\n",
       "            1353 |  # node_Concat_3862\n",
       "                    %\"val_3911\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_3024\"{[576]}, %\"val_3023\"{[768]}) {axis=0}\n",
       "            1354 |  # node_expand_1\n",
       "                    %\"expand_1\"<FLOAT16,[batch,576,768]> ⬅️ ::Expand(%\"unsqueeze\", %\"val_3911\")\n",
       "            1355 |  # node_slice_7\n",
       "                    %\"slice_7\"<FLOAT16,[batch,576,768]> ⬅️ ::Slice(%\"add_1977\", %\"val_8\"{[1]}, %\"val_19\"{[9223372036854775807]}, %\"val_8\"{[1]}, %\"val_8\"{[1]})\n",
       "            1356 |  # node_cat_2\n",
       "                    %\"cat_2\"<FLOAT16,[batch,576,1536]> ⬅️ ::Concat(%\"slice_7\", %\"expand_1\") {axis=-1}\n",
       "            1357 |  # node_MatMul_3884\n",
       "                    %\"val_3933\"<FLOAT16,[batch,576,768]> ⬅️ ::MatMul(%\"cat_2\", %\"val_3932\"{...})\n",
       "            1358 |  # node_linear_48\n",
       "                    %\"linear_48\"<FLOAT16,[batch,576,768]> ⬅️ ::Add(%\"val_3933\", %\"pretrained.act_postprocess3.0.project.0.bias\"{...})\n",
       "            1359 |  # node_Div_3886\n",
       "                    %\"val_3935\"<FLOAT16,[batch,576,768]> ⬅️ ::Div(%\"linear_48\", %\"val_3096\"{1.4140625})\n",
       "            1360 |  # node_Erf_3887\n",
       "                    %\"val_3936\"<FLOAT16,[batch,576,768]> ⬅️ ::Erf(%\"val_3935\")\n",
       "            1361 |  # node_Add_3889\n",
       "                    %\"val_3938\"<FLOAT16,[batch,576,768]> ⬅️ ::Add(%\"val_3936\", %\"val_3099\"{1.0})\n",
       "            1362 |  # node_Mul_3891\n",
       "                    %\"val_3940\"<FLOAT16,[batch,576,768]> ⬅️ ::Mul(%\"val_3101\"{0.5}, %\"val_3938\")\n",
       "            1363 |  # node_gelu_12\n",
       "                    %\"gelu_12\"<FLOAT16,[batch,576,768]> ⬅️ ::Mul(%\"linear_48\", %\"val_3940\")\n",
       "            1364 |  # node_transpose_13\n",
       "                    %\"transpose_13\"<FLOAT16,[batch,768,576]> ⬅️ ::Transpose(%\"gelu_12\") {perm=(0, 2, 1)}\n",
       "            1365 |  # node_select_2\n",
       "                    %\"select_2\"<FLOAT16,[batch,768]> ⬅️ ::Gather(%\"add_2247\", %\"val_2\"{0}) {axis=1}\n",
       "            1366 |  # node_unsqueeze_1\n",
       "                    %\"unsqueeze_1\"<FLOAT16,[batch,1,768]> ⬅️ ::Unsqueeze(%\"select_2\", %\"val_8\"{[1]})\n",
       "            1367 |  # node_expand_2\n",
       "                    %\"expand_2\"<FLOAT16,[batch,576,768]> ⬅️ ::Expand(%\"unsqueeze_1\", %\"val_3911\")\n",
       "            1368 |  # node_slice_12\n",
       "                    %\"slice_12\"<FLOAT16,[batch,576,768]> ⬅️ ::Slice(%\"add_2247\", %\"val_8\"{[1]}, %\"val_19\"{[9223372036854775807]}, %\"val_8\"{[1]}, %\"val_8\"{[1]})\n",
       "            1369 |  # node_cat_3\n",
       "                    %\"cat_3\"<FLOAT16,[batch,576,1536]> ⬅️ ::Concat(%\"slice_12\", %\"expand_2\") {axis=-1}\n",
       "            1370 |  # node_MatMul_3938\n",
       "                    %\"val_3987\"<FLOAT16,[batch,576,768]> ⬅️ ::MatMul(%\"cat_3\", %\"val_3986\"{...})\n",
       "            1371 |  # node_linear_49\n",
       "                    %\"linear_49\"<FLOAT16,[batch,576,768]> ⬅️ ::Add(%\"val_3987\", %\"pretrained.act_postprocess4.0.project.0.bias\"{...})\n",
       "            1372 |  # node_Div_3940\n",
       "                    %\"val_3989\"<FLOAT16,[batch,576,768]> ⬅️ ::Div(%\"linear_49\", %\"val_3096\"{1.4140625})\n",
       "            1373 |  # node_Erf_3941\n",
       "                    %\"val_3990\"<FLOAT16,[batch,576,768]> ⬅️ ::Erf(%\"val_3989\")\n",
       "            1374 |  # node_Add_3943\n",
       "                    %\"val_3992\"<FLOAT16,[batch,576,768]> ⬅️ ::Add(%\"val_3990\", %\"val_3099\"{1.0})\n",
       "            1375 |  # node_Mul_3945\n",
       "                    %\"val_3994\"<FLOAT16,[batch,576,768]> ⬅️ ::Mul(%\"val_3101\"{0.5}, %\"val_3992\")\n",
       "            1376 |  # node_gelu_13\n",
       "                    %\"gelu_13\"<FLOAT16,[batch,576,768]> ⬅️ ::Mul(%\"linear_49\", %\"val_3994\")\n",
       "            1377 |  # node_transpose_14\n",
       "                    %\"transpose_14\"<FLOAT16,[batch,768,576]> ⬅️ ::Transpose(%\"gelu_13\") {perm=(0, 2, 1)}\n",
       "            1378 |  # node_Concat_3951\n",
       "                    %\"val_4000\"<INT64,[4]> ⬅️ ::Concat(%\"val_0\", %\"val_3023\"{[768]}, %\"val_3998\"{[24]}, %\"val_3998\"{[24]}) {axis=0}\n",
       "            1379 |  # node_view_131\n",
       "                    %\"view_131\"<FLOAT16,[batch,768,24,24]> ⬅️ ::Reshape(%\"transpose_13\", %\"val_4000\") {allowzero=1}\n",
       "            1380 |  # node_view_132\n",
       "                    %\"view_132\"<FLOAT16,[batch,768,24,24]> ⬅️ ::Reshape(%\"transpose_14\", %\"val_4000\") {allowzero=1}\n",
       "            1381 |  # node_conv2d_53\n",
       "                    %\"conv2d_53\"<FLOAT16,[batch,768,24,24]> ⬅️ ::Conv(%\"view_131\", %\"pretrained.act_postprocess3.3.weight\"{...}, %\"pretrained.act_postprocess3.3.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1382 |  # node_conv2d_54\n",
       "                    %\"conv2d_54\"<FLOAT16,[batch,768,24,24]> ⬅️ ::Conv(%\"view_132\", %\"pretrained.act_postprocess4.3.weight\"{...}, %\"pretrained.act_postprocess4.3.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1383 |  # node_conv2d_55\n",
       "                    %\"conv2d_55\"<FLOAT16,[batch,768,12,12]> ⬅️ ::Conv(%\"conv2d_54\", %\"pretrained.act_postprocess4.4.weight\"{...}, %\"pretrained.act_postprocess4.4.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "            1384 |  # node_Conv_4750\n",
       "                    %\"conv2d_56\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Conv(%\"relu_9\", %\"scratch.layer1_rn.weight\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1385 |  # node_Conv_4751\n",
       "                    %\"conv2d_57\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Conv(%\"relu_21\", %\"scratch.layer2_rn.weight\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1386 |  # node_Conv_4752\n",
       "                    %\"conv2d_58\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"conv2d_53\", %\"scratch.layer3_rn.weight\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1387 |  # node_Conv_4753\n",
       "                    %\"conv2d_59\"<FLOAT16,[batch,256,12,12]> ⬅️ ::Conv(%\"conv2d_55\", %\"scratch.layer4_rn.weight\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1388 |  # node_relu_49\n",
       "                    %\"relu_49\"<FLOAT16,[batch,256,12,12]> ⬅️ ::Relu(%\"conv2d_59\")\n",
       "            1389 |  # node_conv2d_60\n",
       "                    %\"conv2d_60\"<FLOAT16,[batch,256,12,12]> ⬅️ ::Conv(%\"relu_49\", %\"scratch.refinenet4.resConfUnit2.conv1.weight\"{...}, %\"scratch.refinenet4.resConfUnit2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1390 |  # node_relu_50\n",
       "                    %\"relu_50\"<FLOAT16,[batch,256,12,12]> ⬅️ ::Relu(%\"conv2d_60\")\n",
       "            1391 |  # node_conv2d_61\n",
       "                    %\"conv2d_61\"<FLOAT16,[batch,256,12,12]> ⬅️ ::Conv(%\"relu_50\", %\"scratch.refinenet4.resConfUnit2.conv2.weight\"{...}, %\"scratch.refinenet4.resConfUnit2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1392 |  # node_add_2415\n",
       "                    %\"add_2415\"<FLOAT16,[batch,256,12,12]> ⬅️ ::Add(%\"conv2d_61\", %\"conv2d_59\")\n",
       "            1393 |  # node_Shape_3978\n",
       "                    %\"val_4027\"<INT64,[2]> ⬅️ ::Shape(%\"add_2415\") {end=2, start=0}\n",
       "            1394 |  # node_Concat_3980\n",
       "                    %\"val_4029\"<INT64,[4]> ⬅️ ::Concat(%\"val_4027\", %\"val_4028\"{[24, 24]}) {axis=0}\n",
       "            1395 |  # node_upsample_bilinear2d_1\n",
       "                    %\"upsample_bilinear2d_1\"<FLOAT16,[1,256,24,24]> ⬅️ ::Resize(%\"add_2415\", None, None, %\"val_4029\") {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='align_corners', cubic_coeff_a=-0.75, mode='linear'}\n",
       "            1396 |  # node_conv2d_62\n",
       "                    %\"conv2d_62\"<FLOAT16,[1,256,24,24]> ⬅️ ::Conv(%\"upsample_bilinear2d_1\", %\"scratch.refinenet4.out_conv.weight\"{...}, %\"scratch.refinenet4.out_conv.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1397 |  # node_relu_51\n",
       "                    %\"relu_51\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"conv2d_58\")\n",
       "            1398 |  # node_conv2d_63\n",
       "                    %\"conv2d_63\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_51\", %\"scratch.refinenet3.resConfUnit1.conv1.weight\"{...}, %\"scratch.refinenet3.resConfUnit1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1399 |  # node_relu_52\n",
       "                    %\"relu_52\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"conv2d_63\")\n",
       "            1400 |  # node_conv2d_64\n",
       "                    %\"conv2d_64\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_52\", %\"scratch.refinenet3.resConfUnit1.conv2.weight\"{...}, %\"scratch.refinenet3.resConfUnit1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1401 |  # node_add_2441\n",
       "                    %\"add_2441\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"conv2d_64\", %\"conv2d_58\")\n",
       "            1402 |  # node_add_2447\n",
       "                    %\"add_2447\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"conv2d_62\", %\"add_2441\")\n",
       "            1403 |  # node_relu_53\n",
       "                    %\"relu_53\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"add_2447\")\n",
       "            1404 |  # node_conv2d_65\n",
       "                    %\"conv2d_65\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_53\", %\"scratch.refinenet3.resConfUnit2.conv1.weight\"{...}, %\"scratch.refinenet3.resConfUnit2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1405 |  # node_relu_54\n",
       "                    %\"relu_54\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Relu(%\"conv2d_65\")\n",
       "            1406 |  # node_conv2d_66\n",
       "                    %\"conv2d_66\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Conv(%\"relu_54\", %\"scratch.refinenet3.resConfUnit2.conv2.weight\"{...}, %\"scratch.refinenet3.resConfUnit2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1407 |  # node_add_2473\n",
       "                    %\"add_2473\"<FLOAT16,[batch,256,24,24]> ⬅️ ::Add(%\"conv2d_66\", %\"add_2447\")\n",
       "            1408 |  # node_Shape_3981\n",
       "                    %\"val_4030\"<INT64,[2]> ⬅️ ::Shape(%\"add_2473\") {end=2, start=0}\n",
       "            1409 |  # node_Concat_3984\n",
       "                    %\"val_4033\"<INT64,[4]> ⬅️ ::Concat(%\"val_4030\", %\"val_4032\"{[48, 48]}) {axis=0}\n",
       "            1410 |  # node_upsample_bilinear2d_2\n",
       "                    %\"upsample_bilinear2d_2\"<FLOAT16,[1,256,48,48]> ⬅️ ::Resize(%\"add_2473\", None, None, %\"val_4033\") {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='align_corners', cubic_coeff_a=-0.75, mode='linear'}\n",
       "            1411 |  # node_conv2d_67\n",
       "                    %\"conv2d_67\"<FLOAT16,[1,256,48,48]> ⬅️ ::Conv(%\"upsample_bilinear2d_2\", %\"scratch.refinenet3.out_conv.weight\"{...}, %\"scratch.refinenet3.out_conv.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1412 |  # node_relu_55\n",
       "                    %\"relu_55\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Relu(%\"conv2d_57\")\n",
       "            1413 |  # node_conv2d_68\n",
       "                    %\"conv2d_68\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Conv(%\"relu_55\", %\"scratch.refinenet2.resConfUnit1.conv1.weight\"{...}, %\"scratch.refinenet2.resConfUnit1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1414 |  # node_relu_56\n",
       "                    %\"relu_56\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Relu(%\"conv2d_68\")\n",
       "            1415 |  # node_conv2d_69\n",
       "                    %\"conv2d_69\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Conv(%\"relu_56\", %\"scratch.refinenet2.resConfUnit1.conv2.weight\"{...}, %\"scratch.refinenet2.resConfUnit1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1416 |  # node_add_2499\n",
       "                    %\"add_2499\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Add(%\"conv2d_69\", %\"conv2d_57\")\n",
       "            1417 |  # node_add_2505\n",
       "                    %\"add_2505\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Add(%\"conv2d_67\", %\"add_2499\")\n",
       "            1418 |  # node_relu_57\n",
       "                    %\"relu_57\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Relu(%\"add_2505\")\n",
       "            1419 |  # node_conv2d_70\n",
       "                    %\"conv2d_70\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Conv(%\"relu_57\", %\"scratch.refinenet2.resConfUnit2.conv1.weight\"{...}, %\"scratch.refinenet2.resConfUnit2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1420 |  # node_relu_58\n",
       "                    %\"relu_58\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Relu(%\"conv2d_70\")\n",
       "            1421 |  # node_conv2d_71\n",
       "                    %\"conv2d_71\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Conv(%\"relu_58\", %\"scratch.refinenet2.resConfUnit2.conv2.weight\"{...}, %\"scratch.refinenet2.resConfUnit2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1422 |  # node_add_2531\n",
       "                    %\"add_2531\"<FLOAT16,[batch,256,48,48]> ⬅️ ::Add(%\"conv2d_71\", %\"add_2505\")\n",
       "            1423 |  # node_Shape_3985\n",
       "                    %\"val_4034\"<INT64,[2]> ⬅️ ::Shape(%\"add_2531\") {end=2, start=0}\n",
       "            1424 |  # node_Concat_3988\n",
       "                    %\"val_4037\"<INT64,[4]> ⬅️ ::Concat(%\"val_4034\", %\"val_4036\"{[96, 96]}) {axis=0}\n",
       "            1425 |  # node_upsample_bilinear2d_3\n",
       "                    %\"upsample_bilinear2d_3\"<FLOAT16,[1,256,96,96]> ⬅️ ::Resize(%\"add_2531\", None, None, %\"val_4037\") {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='align_corners', cubic_coeff_a=-0.75, mode='linear'}\n",
       "            1426 |  # node_conv2d_72\n",
       "                    %\"conv2d_72\"<FLOAT16,[1,256,96,96]> ⬅️ ::Conv(%\"upsample_bilinear2d_3\", %\"scratch.refinenet2.out_conv.weight\"{...}, %\"scratch.refinenet2.out_conv.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1427 |  # node_relu_59\n",
       "                    %\"relu_59\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Relu(%\"conv2d_56\")\n",
       "            1428 |  # node_conv2d_73\n",
       "                    %\"conv2d_73\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Conv(%\"relu_59\", %\"scratch.refinenet1.resConfUnit1.conv1.weight\"{...}, %\"scratch.refinenet1.resConfUnit1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1429 |  # node_relu_60\n",
       "                    %\"relu_60\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Relu(%\"conv2d_73\")\n",
       "            1430 |  # node_conv2d_74\n",
       "                    %\"conv2d_74\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Conv(%\"relu_60\", %\"scratch.refinenet1.resConfUnit1.conv2.weight\"{...}, %\"scratch.refinenet1.resConfUnit1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1431 |  # node_add_2557\n",
       "                    %\"add_2557\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Add(%\"conv2d_74\", %\"conv2d_56\")\n",
       "            1432 |  # node_add_2563\n",
       "                    %\"add_2563\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Add(%\"conv2d_72\", %\"add_2557\")\n",
       "            1433 |  # node_relu_61\n",
       "                    %\"relu_61\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Relu(%\"add_2563\")\n",
       "            1434 |  # node_conv2d_75\n",
       "                    %\"conv2d_75\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Conv(%\"relu_61\", %\"scratch.refinenet1.resConfUnit2.conv1.weight\"{...}, %\"scratch.refinenet1.resConfUnit2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1435 |  # node_relu_62\n",
       "                    %\"relu_62\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Relu(%\"conv2d_75\")\n",
       "            1436 |  # node_conv2d_76\n",
       "                    %\"conv2d_76\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Conv(%\"relu_62\", %\"scratch.refinenet1.resConfUnit2.conv2.weight\"{...}, %\"scratch.refinenet1.resConfUnit2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1437 |  # node_add_2589\n",
       "                    %\"add_2589\"<FLOAT16,[batch,256,96,96]> ⬅️ ::Add(%\"conv2d_76\", %\"add_2563\")\n",
       "            1438 |  # node_upsample_bilinear2d_4\n",
       "                    %\"upsample_bilinear2d_4\"<FLOAT16,[1,256,192,192]> ⬅️ ::Resize(%\"add_2589\", None, %\"val_4038\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='align_corners', cubic_coeff_a=-0.75, mode='linear'}\n",
       "            1439 |  # node_conv2d_77\n",
       "                    %\"conv2d_77\"<FLOAT16,[1,256,192,192]> ⬅️ ::Conv(%\"upsample_bilinear2d_4\", %\"scratch.refinenet1.out_conv.weight\"{...}, %\"scratch.refinenet1.out_conv.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1440 |  # node_conv2d_78\n",
       "                    %\"conv2d_78\"<FLOAT16,[1,128,192,192]> ⬅️ ::Conv(%\"conv2d_77\", %\"scratch.output_conv.0.weight\"{...}, %\"scratch.output_conv.0.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1441 |  # node_upsample_bilinear2d_5\n",
       "                    %\"upsample_bilinear2d_5\"<FLOAT16,[1,128,384,384]> ⬅️ ::Resize(%\"conv2d_78\", None, %\"val_4038\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='align_corners', cubic_coeff_a=-0.75, mode='linear'}\n",
       "            1442 |  # node_conv2d_79\n",
       "                    %\"conv2d_79\"<FLOAT16,[1,32,384,384]> ⬅️ ::Conv(%\"upsample_bilinear2d_5\", %\"scratch.output_conv.2.weight\"{...}, %\"scratch.output_conv.2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1443 |  # node_relu_63\n",
       "                    %\"relu_63\"<FLOAT16,[1,32,384,384]> ⬅️ ::Relu(%\"conv2d_79\")\n",
       "            1444 |  # node_conv2d_80\n",
       "                    %\"conv2d_80\"<FLOAT16,[1,1,384,384]> ⬅️ ::Conv(%\"relu_63\", %\"scratch.output_conv.4.weight\"{...}, %\"scratch.output_conv.4.bias\"{[0.492919921875]}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            1445 |  # node_relu_64\n",
       "                    %\"relu_64\"<FLOAT16,[1,1,384,384]> ⬅️ ::Relu(%\"conv2d_80\")\n",
       "            1446 |  # node_squeeze_1\n",
       "                    %\"depth\"<FLOAT16,[1,384,384]> ⬅️ ::Squeeze(%\"relu_64\", %\"val_8\"{[1]})\n",
       "            return %\"depth\"<FLOAT16,[1,384,384]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_pretrained_model_cls_token: \"f16[1, 1, 768]\", p_pretrained_model_pos_embed: \"f16[1, 577, 768]\", p_pretrained_model_patch_embed_backbone_stem_conv_weight: \"f16[64, 3, 7, 7]\", p_pretrained_model_patch_embed_backbone_stem_norm_weight: \"f16[64]\", p_pretrained_model_patch_embed_backbone_stem_norm_bias: \"f16[64]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_downsample_conv_weight: \"f16[256, 64, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_downsample_norm_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_downsample_norm_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_conv1_weight: \"f16[64, 64, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm1_weight: \"f16[64]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm1_bias: \"f16[64]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_conv2_weight: \"f16[64, 64, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm2_weight: \"f16[64]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm2_bias: \"f16[64]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_conv3_weight: \"f16[256, 64, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm3_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm3_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_conv1_weight: \"f16[64, 256, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm1_weight: \"f16[64]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm1_bias: \"f16[64]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_conv2_weight: \"f16[64, 64, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm2_weight: \"f16[64]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm2_bias: \"f16[64]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_conv3_weight: \"f16[256, 64, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm3_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm3_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_conv1_weight: \"f16[64, 256, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm1_weight: \"f16[64]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm1_bias: \"f16[64]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_conv2_weight: \"f16[64, 64, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm2_weight: \"f16[64]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm2_bias: \"f16[64]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_conv3_weight: \"f16[256, 64, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm3_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm3_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_downsample_conv_weight: \"f16[512, 256, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_downsample_norm_weight: \"f16[512]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_downsample_norm_bias: \"f16[512]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_conv1_weight: \"f16[128, 256, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm1_weight: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm1_bias: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_conv2_weight: \"f16[128, 128, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm2_weight: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm2_bias: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_conv3_weight: \"f16[512, 128, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm3_weight: \"f16[512]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm3_bias: \"f16[512]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_conv1_weight: \"f16[128, 512, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm1_weight: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm1_bias: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_conv2_weight: \"f16[128, 128, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm2_weight: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm2_bias: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_conv3_weight: \"f16[512, 128, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm3_weight: \"f16[512]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm3_bias: \"f16[512]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_conv1_weight: \"f16[128, 512, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm1_weight: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm1_bias: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_conv2_weight: \"f16[128, 128, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm2_weight: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm2_bias: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_conv3_weight: \"f16[512, 128, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm3_weight: \"f16[512]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm3_bias: \"f16[512]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_conv1_weight: \"f16[128, 512, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm1_weight: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm1_bias: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_conv2_weight: \"f16[128, 128, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm2_weight: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm2_bias: \"f16[128]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_conv3_weight: \"f16[512, 128, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm3_weight: \"f16[512]\", p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm3_bias: \"f16[512]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_downsample_conv_weight: \"f16[1024, 512, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_downsample_norm_weight: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_downsample_norm_bias: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_conv1_weight: \"f16[256, 512, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm1_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm1_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_conv2_weight: \"f16[256, 256, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm2_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm2_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_conv3_weight: \"f16[1024, 256, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm3_weight: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm3_bias: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_conv1_weight: \"f16[256, 1024, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm1_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm1_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_conv2_weight: \"f16[256, 256, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm2_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm2_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_conv3_weight: \"f16[1024, 256, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm3_weight: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm3_bias: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_conv1_weight: \"f16[256, 1024, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm1_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm1_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_conv2_weight: \"f16[256, 256, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm2_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm2_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_conv3_weight: \"f16[1024, 256, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm3_weight: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm3_bias: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_conv1_weight: \"f16[256, 1024, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm1_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm1_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_conv2_weight: \"f16[256, 256, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm2_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm2_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_conv3_weight: \"f16[1024, 256, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm3_weight: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm3_bias: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_conv1_weight: \"f16[256, 1024, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm1_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm1_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_conv2_weight: \"f16[256, 256, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm2_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm2_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_conv3_weight: \"f16[1024, 256, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm3_weight: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm3_bias: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_conv1_weight: \"f16[256, 1024, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm1_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm1_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_conv2_weight: \"f16[256, 256, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm2_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm2_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_conv3_weight: \"f16[1024, 256, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm3_weight: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm3_bias: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_conv1_weight: \"f16[256, 1024, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm1_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm1_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_conv2_weight: \"f16[256, 256, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm2_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm2_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_conv3_weight: \"f16[1024, 256, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm3_weight: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm3_bias: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_conv1_weight: \"f16[256, 1024, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm1_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm1_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_conv2_weight: \"f16[256, 256, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm2_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm2_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_conv3_weight: \"f16[1024, 256, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm3_weight: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm3_bias: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_conv1_weight: \"f16[256, 1024, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm1_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm1_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_conv2_weight: \"f16[256, 256, 3, 3]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm2_weight: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm2_bias: \"f16[256]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_conv3_weight: \"f16[1024, 256, 1, 1]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm3_weight: \"f16[1024]\", p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm3_bias: \"f16[1024]\", p_pretrained_model_patch_embed_proj_weight: \"f16[768, 1024, 1, 1]\", p_pretrained_model_patch_embed_proj_bias: \"f16[768]\", p_pretrained_model_blocks_0_norm1_weight: \"f16[768]\", p_pretrained_model_blocks_0_norm1_bias: \"f16[768]\", p_pretrained_model_blocks_0_attn_qkv_weight: \"f16[2304, 768]\", p_pretrained_model_blocks_0_attn_qkv_bias: \"f16[2304]\", p_pretrained_model_blocks_0_attn_proj_weight: \"f16[768, 768]\", p_pretrained_model_blocks_0_attn_proj_bias: \"f16[768]\", p_pretrained_model_blocks_0_norm2_weight: \"f16[768]\", p_pretrained_model_blocks_0_norm2_bias: \"f16[768]\", p_pretrained_model_blocks_0_mlp_fc1_weight: \"f16[3072, 768]\", p_pretrained_model_blocks_0_mlp_fc1_bias: \"f16[3072]\", p_pretrained_model_blocks_0_mlp_fc2_weight: \"f16[768, 3072]\", p_pretrained_model_blocks_0_mlp_fc2_bias: \"f16[768]\", p_pretrained_model_blocks_1_norm1_weight: \"f16[768]\", p_pretrained_model_blocks_1_norm1_bias: \"f16[768]\", p_pretrained_model_blocks_1_attn_qkv_weight: \"f16[2304, 768]\", p_pretrained_model_blocks_1_attn_qkv_bias: \"f16[2304]\", p_pretrained_model_blocks_1_attn_proj_weight: \"f16[768, 768]\", p_pretrained_model_blocks_1_attn_proj_bias: \"f16[768]\", p_pretrained_model_blocks_1_norm2_weight: \"f16[768]\", p_pretrained_model_blocks_1_norm2_bias: \"f16[768]\", p_pretrained_model_blocks_1_mlp_fc1_weight: \"f16[3072, 768]\", p_pretrained_model_blocks_1_mlp_fc1_bias: \"f16[3072]\", p_pretrained_model_blocks_1_mlp_fc2_weight: \"f16[768, 3072]\", p_pretrained_model_blocks_1_mlp_fc2_bias: \"f16[768]\", p_pretrained_model_blocks_2_norm1_weight: \"f16[768]\", p_pretrained_model_blocks_2_norm1_bias: \"f16[768]\", p_pretrained_model_blocks_2_attn_qkv_weight: \"f16[2304, 768]\", p_pretrained_model_blocks_2_attn_qkv_bias: \"f16[2304]\", p_pretrained_model_blocks_2_attn_proj_weight: \"f16[768, 768]\", p_pretrained_model_blocks_2_attn_proj_bias: \"f16[768]\", p_pretrained_model_blocks_2_norm2_weight: \"f16[768]\", p_pretrained_model_blocks_2_norm2_bias: \"f16[768]\", p_pretrained_model_blocks_2_mlp_fc1_weight: \"f16[3072, 768]\", p_pretrained_model_blocks_2_mlp_fc1_bias: \"f16[3072]\", p_pretrained_model_blocks_2_mlp_fc2_weight: \"f16[768, 3072]\", p_pretrained_model_blocks_2_mlp_fc2_bias: \"f16[768]\", p_pretrained_model_blocks_3_norm1_weight: \"f16[768]\", p_pretrained_model_blocks_3_norm1_bias: \"f16[768]\", p_pretrained_model_blocks_3_attn_qkv_weight: \"f16[2304, 768]\", p_pretrained_model_blocks_3_attn_qkv_bias: \"f16[2304]\", p_pretrained_model_blocks_3_attn_proj_weight: \"f16[768, 768]\", p_pretrained_model_blocks_3_attn_proj_bias: \"f16[768]\", p_pretrained_model_blocks_3_norm2_weight: \"f16[768]\", p_pretrained_model_blocks_3_norm2_bias: \"f16[768]\", p_pretrained_model_blocks_3_mlp_fc1_weight: \"f16[3072, 768]\", p_pretrained_model_blocks_3_mlp_fc1_bias: \"f16[3072]\", p_pretrained_model_blocks_3_mlp_fc2_weight: \"f16[768, 3072]\", p_pretrained_model_blocks_3_mlp_fc2_bias: \"f16[768]\", p_pretrained_model_blocks_4_norm1_weight: \"f16[768]\", p_pretrained_model_blocks_4_norm1_bias: \"f16[768]\", p_pretrained_model_blocks_4_attn_qkv_weight: \"f16[2304, 768]\", p_pretrained_model_blocks_4_attn_qkv_bias: \"f16[2304]\", p_pretrained_model_blocks_4_attn_proj_weight: \"f16[768, 768]\", p_pretrained_model_blocks_4_attn_proj_bias: \"f16[768]\", p_pretrained_model_blocks_4_norm2_weight: \"f16[768]\", p_pretrained_model_blocks_4_norm2_bias: \"f16[768]\", p_pretrained_model_blocks_4_mlp_fc1_weight: \"f16[3072, 768]\", p_pretrained_model_blocks_4_mlp_fc1_bias: \"f16[3072]\", p_pretrained_model_blocks_4_mlp_fc2_weight: \"f16[768, 3072]\", p_pretrained_model_blocks_4_mlp_fc2_bias: \"f16[768]\", p_pretrained_model_blocks_5_norm1_weight: \"f16[768]\", p_pretrained_model_blocks_5_norm1_bias: \"f16[768]\", p_pretrained_model_blocks_5_attn_qkv_weight: \"f16[2304, 768]\", p_pretrained_model_blocks_5_attn_qkv_bias: \"f16[2304]\", p_pretrained_model_blocks_5_attn_proj_weight: \"f16[768, 768]\", p_pretrained_model_blocks_5_attn_proj_bias: \"f16[768]\", p_pretrained_model_blocks_5_norm2_weight: \"f16[768]\", p_pretrained_model_blocks_5_norm2_bias: \"f16[768]\", p_pretrained_model_blocks_5_mlp_fc1_weight: \"f16[3072, 768]\", p_pretrained_model_blocks_5_mlp_fc1_bias: \"f16[3072]\", p_pretrained_model_blocks_5_mlp_fc2_weight: \"f16[768, 3072]\", p_pretrained_model_blocks_5_mlp_fc2_bias: \"f16[768]\", p_pretrained_model_blocks_6_norm1_weight: \"f16[768]\", p_pretrained_model_blocks_6_norm1_bias: \"f16[768]\", p_pretrained_model_blocks_6_attn_qkv_weight: \"f16[2304, 768]\", p_pretrained_model_blocks_6_attn_qkv_bias: \"f16[2304]\", p_pretrained_model_blocks_6_attn_proj_weight: \"f16[768, 768]\", p_pretrained_model_blocks_6_attn_proj_bias: \"f16[768]\", p_pretrained_model_blocks_6_norm2_weight: \"f16[768]\", p_pretrained_model_blocks_6_norm2_bias: \"f16[768]\", p_pretrained_model_blocks_6_mlp_fc1_weight: \"f16[3072, 768]\", p_pretrained_model_blocks_6_mlp_fc1_bias: \"f16[3072]\", p_pretrained_model_blocks_6_mlp_fc2_weight: \"f16[768, 3072]\", p_pretrained_model_blocks_6_mlp_fc2_bias: \"f16[768]\", p_pretrained_model_blocks_7_norm1_weight: \"f16[768]\", p_pretrained_model_blocks_7_norm1_bias: \"f16[768]\", p_pretrained_model_blocks_7_attn_qkv_weight: \"f16[2304, 768]\", p_pretrained_model_blocks_7_attn_qkv_bias: \"f16[2304]\", p_pretrained_model_blocks_7_attn_proj_weight: \"f16[768, 768]\", p_pretrained_model_blocks_7_attn_proj_bias: \"f16[768]\", p_pretrained_model_blocks_7_norm2_weight: \"f16[768]\", p_pretrained_model_blocks_7_norm2_bias: \"f16[768]\", p_pretrained_model_blocks_7_mlp_fc1_weight: \"f16[3072, 768]\", p_pretrained_model_blocks_7_mlp_fc1_bias: \"f16[3072]\", p_pretrained_model_blocks_7_mlp_fc2_weight: \"f16[768, 3072]\", p_pretrained_model_blocks_7_mlp_fc2_bias: \"f16[768]\", p_pretrained_model_blocks_8_norm1_weight: \"f16[768]\", p_pretrained_model_blocks_8_norm1_bias: \"f16[768]\", p_pretrained_model_blocks_8_attn_qkv_weight: \"f16[2304, 768]\", p_pretrained_model_blocks_8_attn_qkv_bias: \"f16[2304]\", p_pretrained_model_blocks_8_attn_proj_weight: \"f16[768, 768]\", p_pretrained_model_blocks_8_attn_proj_bias: \"f16[768]\", p_pretrained_model_blocks_8_norm2_weight: \"f16[768]\", p_pretrained_model_blocks_8_norm2_bias: \"f16[768]\", p_pretrained_model_blocks_8_mlp_fc1_weight: \"f16[3072, 768]\", p_pretrained_model_blocks_8_mlp_fc1_bias: \"f16[3072]\", p_pretrained_model_blocks_8_mlp_fc2_weight: \"f16[768, 3072]\", p_pretrained_model_blocks_8_mlp_fc2_bias: \"f16[768]\", p_pretrained_model_blocks_9_norm1_weight: \"f16[768]\", p_pretrained_model_blocks_9_norm1_bias: \"f16[768]\", p_pretrained_model_blocks_9_attn_qkv_weight: \"f16[2304, 768]\", p_pretrained_model_blocks_9_attn_qkv_bias: \"f16[2304]\", p_pretrained_model_blocks_9_attn_proj_weight: \"f16[768, 768]\", p_pretrained_model_blocks_9_attn_proj_bias: \"f16[768]\", p_pretrained_model_blocks_9_norm2_weight: \"f16[768]\", p_pretrained_model_blocks_9_norm2_bias: \"f16[768]\", p_pretrained_model_blocks_9_mlp_fc1_weight: \"f16[3072, 768]\", p_pretrained_model_blocks_9_mlp_fc1_bias: \"f16[3072]\", p_pretrained_model_blocks_9_mlp_fc2_weight: \"f16[768, 3072]\", p_pretrained_model_blocks_9_mlp_fc2_bias: \"f16[768]\", p_pretrained_model_blocks_10_norm1_weight: \"f16[768]\", p_pretrained_model_blocks_10_norm1_bias: \"f16[768]\", p_pretrained_model_blocks_10_attn_qkv_weight: \"f16[2304, 768]\", p_pretrained_model_blocks_10_attn_qkv_bias: \"f16[2304]\", p_pretrained_model_blocks_10_attn_proj_weight: \"f16[768, 768]\", p_pretrained_model_blocks_10_attn_proj_bias: \"f16[768]\", p_pretrained_model_blocks_10_norm2_weight: \"f16[768]\", p_pretrained_model_blocks_10_norm2_bias: \"f16[768]\", p_pretrained_model_blocks_10_mlp_fc1_weight: \"f16[3072, 768]\", p_pretrained_model_blocks_10_mlp_fc1_bias: \"f16[3072]\", p_pretrained_model_blocks_10_mlp_fc2_weight: \"f16[768, 3072]\", p_pretrained_model_blocks_10_mlp_fc2_bias: \"f16[768]\", p_pretrained_model_blocks_11_norm1_weight: \"f16[768]\", p_pretrained_model_blocks_11_norm1_bias: \"f16[768]\", p_pretrained_model_blocks_11_attn_qkv_weight: \"f16[2304, 768]\", p_pretrained_model_blocks_11_attn_qkv_bias: \"f16[2304]\", p_pretrained_model_blocks_11_attn_proj_weight: \"f16[768, 768]\", p_pretrained_model_blocks_11_attn_proj_bias: \"f16[768]\", p_pretrained_model_blocks_11_norm2_weight: \"f16[768]\", p_pretrained_model_blocks_11_norm2_bias: \"f16[768]\", p_pretrained_model_blocks_11_mlp_fc1_weight: \"f16[3072, 768]\", p_pretrained_model_blocks_11_mlp_fc1_bias: \"f16[3072]\", p_pretrained_model_blocks_11_mlp_fc2_weight: \"f16[768, 3072]\", p_pretrained_model_blocks_11_mlp_fc2_bias: \"f16[768]\", p_pretrained_model_norm_weight: \"f16[768]\", p_pretrained_model_norm_bias: \"f16[768]\", p_pretrained_model_head_weight: \"f16[1000, 768]\", p_pretrained_model_head_bias: \"f16[1000]\", p_pretrained_act_postprocess3_0_project_0_weight: \"f16[768, 1536]\", p_pretrained_act_postprocess3_0_project_0_bias: \"f16[768]\", p_pretrained_act_postprocess3_3_weight: \"f16[768, 768, 1, 1]\", p_pretrained_act_postprocess3_3_bias: \"f16[768]\", p_pretrained_act_postprocess4_0_project_0_weight: \"f16[768, 1536]\", p_pretrained_act_postprocess4_0_project_0_bias: \"f16[768]\", p_pretrained_act_postprocess4_3_weight: \"f16[768, 768, 1, 1]\", p_pretrained_act_postprocess4_3_bias: \"f16[768]\", p_pretrained_act_postprocess4_4_weight: \"f16[768, 768, 3, 3]\", p_pretrained_act_postprocess4_4_bias: \"f16[768]\", p_scratch_layer1_rn_weight: \"f16[256, 256, 3, 3]\", p_scratch_layer2_rn_weight: \"f16[256, 512, 3, 3]\", p_scratch_layer3_rn_weight: \"f16[256, 768, 3, 3]\", p_scratch_layer4_rn_weight: \"f16[256, 768, 3, 3]\", p_scratch_refinenet1_out_conv_weight: \"f16[256, 256, 1, 1]\", p_scratch_refinenet1_out_conv_bias: \"f16[256]\", p_scratch_refinenet1_resconfunit1_conv1_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet1_resconfunit1_conv1_bias: \"f16[256]\", p_scratch_refinenet1_resconfunit1_conv2_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet1_resconfunit1_conv2_bias: \"f16[256]\", p_scratch_refinenet1_resconfunit2_conv1_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet1_resconfunit2_conv1_bias: \"f16[256]\", p_scratch_refinenet1_resconfunit2_conv2_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet1_resconfunit2_conv2_bias: \"f16[256]\", p_scratch_refinenet2_out_conv_weight: \"f16[256, 256, 1, 1]\", p_scratch_refinenet2_out_conv_bias: \"f16[256]\", p_scratch_refinenet2_resconfunit1_conv1_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet2_resconfunit1_conv1_bias: \"f16[256]\", p_scratch_refinenet2_resconfunit1_conv2_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet2_resconfunit1_conv2_bias: \"f16[256]\", p_scratch_refinenet2_resconfunit2_conv1_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet2_resconfunit2_conv1_bias: \"f16[256]\", p_scratch_refinenet2_resconfunit2_conv2_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet2_resconfunit2_conv2_bias: \"f16[256]\", p_scratch_refinenet3_out_conv_weight: \"f16[256, 256, 1, 1]\", p_scratch_refinenet3_out_conv_bias: \"f16[256]\", p_scratch_refinenet3_resconfunit1_conv1_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet3_resconfunit1_conv1_bias: \"f16[256]\", p_scratch_refinenet3_resconfunit1_conv2_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet3_resconfunit1_conv2_bias: \"f16[256]\", p_scratch_refinenet3_resconfunit2_conv1_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet3_resconfunit2_conv1_bias: \"f16[256]\", p_scratch_refinenet3_resconfunit2_conv2_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet3_resconfunit2_conv2_bias: \"f16[256]\", p_scratch_refinenet4_out_conv_weight: \"f16[256, 256, 1, 1]\", p_scratch_refinenet4_out_conv_bias: \"f16[256]\", p_scratch_refinenet4_resconfunit1_conv1_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet4_resconfunit1_conv1_bias: \"f16[256]\", p_scratch_refinenet4_resconfunit1_conv2_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet4_resconfunit1_conv2_bias: \"f16[256]\", p_scratch_refinenet4_resconfunit2_conv1_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet4_resconfunit2_conv1_bias: \"f16[256]\", p_scratch_refinenet4_resconfunit2_conv2_weight: \"f16[256, 256, 3, 3]\", p_scratch_refinenet4_resconfunit2_conv2_bias: \"f16[256]\", p_scratch_output_conv_0_weight: \"f16[128, 256, 3, 3]\", p_scratch_output_conv_0_bias: \"f16[128]\", p_scratch_output_conv_2_weight: \"f16[32, 128, 3, 3]\", p_scratch_output_conv_2_bias: \"f16[32]\", p_scratch_output_conv_4_weight: \"f16[1, 32, 1, 1]\", p_scratch_output_conv_4_bias: \"f16[1]\", x: \"f16[s77, 3, 384, 384]\"):\n",
       "                    # No stacktrace found for following nodes\n",
       "                    sym_size_int_7: \"Sym(s77)\" = torch.ops.aten.sym_size.int(x, 0)\n",
       "                    le_2: \"Sym(s77 <= 1)\" = sym_size_int_7 <= 1;  le_2 = None\n",
       "                    eq_595: \"Sym(Eq(s77, 1))\" = sym_size_int_7 == 1;  eq_595 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/dpt_depth.py:114 in forward, code: layers = self.forward_transformer(self.pretrained, x)\n",
       "                    slice_1: \"f16[1, 1, 768]\" = torch.ops.aten.slice.Tensor(p_pretrained_model_pos_embed, 1, 0, 1)\n",
       "                    select: \"f16[577, 768]\" = torch.ops.aten.select.int(p_pretrained_model_pos_embed, 0, 0);  p_pretrained_model_pos_embed = None\n",
       "                    slice_2: \"f16[576, 768]\" = torch.ops.aten.slice.Tensor(select, 0, 1, 9223372036854775807);  select = None\n",
       "                    view: \"f16[1, 24, 24, 768]\" = torch.ops.aten.view.default(slice_2, [1, 24, 24, -1]);  slice_2 = None\n",
       "                    permute: \"f16[1, 768, 24, 24]\" = torch.ops.aten.permute.default(view, [0, 3, 1, 2]);  view = None\n",
       "                    upsample_bilinear2d: \"f16[1, 768, 24, 24]\" = torch.ops.aten.upsample_bilinear2d.vec(permute, [24, 24], False, None);  permute = None\n",
       "                    permute_1: \"f16[1, 24, 24, 768]\" = torch.ops.aten.permute.default(upsample_bilinear2d, [0, 2, 3, 1]);  upsample_bilinear2d = None\n",
       "                    view_1: \"f16[1, 576, 768]\" = torch.ops.aten.view.default(permute_1, [1, 576, -1]);  permute_1 = None\n",
       "                    cat: \"f16[1, 577, 768]\" = torch.ops.aten.cat.default([slice_1, view_1], 1);  slice_1 = view_1 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:99 in forward, code: x = pad_same(x, self.kernel_size, self.stride, self.dilation)\n",
       "                    pad: \"f16[s77, 3, 389, 389]\" = torch.ops.aten.pad.default(x, [2, 3, 2, 3], 'constant', 0.0);  x = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_2: \"f16[1, 64, 147]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stem_conv_weight, [1, 64, -1]);  p_pretrained_model_patch_embed_backbone_stem_conv_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit = torch.ops.aten._native_batch_norm_legit.no_stats(view_2, None, None, True, 0.0, 1e-08);  view_2 = None\n",
       "                    getitem: \"f16[1, 64, 147]\" = _native_batch_norm_legit[0];  _native_batch_norm_legit = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_3: \"f16[64, 3, 7, 7]\" = torch.ops.aten.view.default(getitem, [64, 3, 7, 7]);  getitem = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d: \"f16[s77, 64, 192, 192]\" = torch.ops.aten.conv2d.default(pad, view_3, None, [2, 2]);  pad = view_3 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm: \"f16[s77, 64, 192, 192]\" = torch.ops.aten.group_norm.default(conv2d, 32, p_pretrained_model_patch_embed_backbone_stem_norm_weight, p_pretrained_model_patch_embed_backbone_stem_norm_bias);  conv2d = p_pretrained_model_patch_embed_backbone_stem_norm_weight = p_pretrained_model_patch_embed_backbone_stem_norm_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu: \"f16[s77, 64, 192, 192]\" = torch.ops.aten.relu.default(group_norm);  group_norm = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/pool2d_same.py:80 in forward, code: x = pad_same(x, self.kernel_size, self.stride, value=-float('inf'))\n",
       "                    pad_1: \"f16[s77, 64, 193, 193]\" = torch.ops.aten.pad.default(relu, [0, 1, 0, 1], 'constant', -inf);  relu = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/pool2d_same.py:81 in forward, code: return F.max_pool2d(x, self.kernel_size, self.stride, (0, 0), self.dilation, self.ceil_mode)\n",
       "                    max_pool2d: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.max_pool2d.default(pad_1, [3, 3], [2, 2]);  pad_1 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_4: \"f16[1, 256, 64]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_downsample_conv_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_downsample_conv_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_1 = torch.ops.aten._native_batch_norm_legit.no_stats(view_4, None, None, True, 0.0, 1e-08);  view_4 = None\n",
       "                    getitem_3: \"f16[1, 256, 64]\" = _native_batch_norm_legit_1[0];  _native_batch_norm_legit_1 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_5: \"f16[256, 64, 1, 1]\" = torch.ops.aten.view.default(getitem_3, [256, 64, 1, 1]);  getitem_3 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_1: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.conv2d.default(max_pool2d, view_5);  view_5 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_1: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.group_norm.default(conv2d_1, 32, p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_downsample_norm_weight, p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_downsample_norm_bias);  conv2d_1 = p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_downsample_norm_weight = p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_downsample_norm_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_6: \"f16[1, 64, 64]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_conv1_weight, [1, 64, -1]);  p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_2 = torch.ops.aten._native_batch_norm_legit.no_stats(view_6, None, None, True, 0.0, 1e-08);  view_6 = None\n",
       "                    getitem_6: \"f16[1, 64, 64]\" = _native_batch_norm_legit_2[0];  _native_batch_norm_legit_2 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_7: \"f16[64, 64, 1, 1]\" = torch.ops.aten.view.default(getitem_6, [64, 64, 1, 1]);  getitem_6 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_2: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.conv2d.default(max_pool2d, view_7);  max_pool2d = view_7 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_2: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.group_norm.default(conv2d_2, 32, p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm1_bias);  conv2d_2 = p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_1: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.relu.default(group_norm_2);  group_norm_2 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_8: \"f16[1, 64, 576]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_conv2_weight, [1, 64, -1]);  p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_3 = torch.ops.aten._native_batch_norm_legit.no_stats(view_8, None, None, True, 0.0, 1e-08);  view_8 = None\n",
       "                    getitem_9: \"f16[1, 64, 576]\" = _native_batch_norm_legit_3[0];  _native_batch_norm_legit_3 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_9: \"f16[64, 64, 3, 3]\" = torch.ops.aten.view.default(getitem_9, [64, 64, 3, 3]);  getitem_9 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_3: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.conv2d.default(relu_1, view_9, None, [1, 1], [1, 1]);  relu_1 = view_9 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_3: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.group_norm.default(conv2d_3, 32, p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm2_bias);  conv2d_3 = p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_2: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.relu.default(group_norm_3);  group_norm_3 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_10: \"f16[1, 256, 64]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_conv3_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_4 = torch.ops.aten._native_batch_norm_legit.no_stats(view_10, None, None, True, 0.0, 1e-08);  view_10 = None\n",
       "                    getitem_12: \"f16[1, 256, 64]\" = _native_batch_norm_legit_4[0];  _native_batch_norm_legit_4 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_11: \"f16[256, 64, 1, 1]\" = torch.ops.aten.view.default(getitem_12, [256, 64, 1, 1]);  getitem_12 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_4: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.conv2d.default(relu_2, view_11);  relu_2 = view_11 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_4: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.group_norm.default(conv2d_4, 32, p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm3_bias);  conv2d_4 = p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_95: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.add.Tensor(group_norm_4, group_norm_1);  group_norm_4 = group_norm_1 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_3: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.relu.default(add_95);  add_95 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_12: \"f16[1, 64, 256]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_conv1_weight, [1, 64, -1]);  p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_5 = torch.ops.aten._native_batch_norm_legit.no_stats(view_12, None, None, True, 0.0, 1e-08);  view_12 = None\n",
       "                    getitem_15: \"f16[1, 64, 256]\" = _native_batch_norm_legit_5[0];  _native_batch_norm_legit_5 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_13: \"f16[64, 256, 1, 1]\" = torch.ops.aten.view.default(getitem_15, [64, 256, 1, 1]);  getitem_15 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_5: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.conv2d.default(relu_3, view_13);  view_13 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_5: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.group_norm.default(conv2d_5, 32, p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm1_bias);  conv2d_5 = p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_4: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.relu.default(group_norm_5);  group_norm_5 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_14: \"f16[1, 64, 576]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_conv2_weight, [1, 64, -1]);  p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_6 = torch.ops.aten._native_batch_norm_legit.no_stats(view_14, None, None, True, 0.0, 1e-08);  view_14 = None\n",
       "                    getitem_18: \"f16[1, 64, 576]\" = _native_batch_norm_legit_6[0];  _native_batch_norm_legit_6 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_15: \"f16[64, 64, 3, 3]\" = torch.ops.aten.view.default(getitem_18, [64, 64, 3, 3]);  getitem_18 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_6: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.conv2d.default(relu_4, view_15, None, [1, 1], [1, 1]);  relu_4 = view_15 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_6: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.group_norm.default(conv2d_6, 32, p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm2_bias);  conv2d_6 = p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_5: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.relu.default(group_norm_6);  group_norm_6 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_16: \"f16[1, 256, 64]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_conv3_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_7 = torch.ops.aten._native_batch_norm_legit.no_stats(view_16, None, None, True, 0.0, 1e-08);  view_16 = None\n",
       "                    getitem_21: \"f16[1, 256, 64]\" = _native_batch_norm_legit_7[0];  _native_batch_norm_legit_7 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_17: \"f16[256, 64, 1, 1]\" = torch.ops.aten.view.default(getitem_21, [256, 64, 1, 1]);  getitem_21 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_7: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.conv2d.default(relu_5, view_17);  relu_5 = view_17 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_7: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.group_norm.default(conv2d_7, 32, p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm3_bias);  conv2d_7 = p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_161: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.add.Tensor(group_norm_7, relu_3);  group_norm_7 = relu_3 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_6: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.relu.default(add_161);  add_161 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_18: \"f16[1, 64, 256]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_conv1_weight, [1, 64, -1]);  p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_8 = torch.ops.aten._native_batch_norm_legit.no_stats(view_18, None, None, True, 0.0, 1e-08);  view_18 = None\n",
       "                    getitem_24: \"f16[1, 64, 256]\" = _native_batch_norm_legit_8[0];  _native_batch_norm_legit_8 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_19: \"f16[64, 256, 1, 1]\" = torch.ops.aten.view.default(getitem_24, [64, 256, 1, 1]);  getitem_24 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_8: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.conv2d.default(relu_6, view_19);  view_19 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_8: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.group_norm.default(conv2d_8, 32, p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm1_bias);  conv2d_8 = p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_7: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.relu.default(group_norm_8);  group_norm_8 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_20: \"f16[1, 64, 576]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_conv2_weight, [1, 64, -1]);  p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_9 = torch.ops.aten._native_batch_norm_legit.no_stats(view_20, None, None, True, 0.0, 1e-08);  view_20 = None\n",
       "                    getitem_27: \"f16[1, 64, 576]\" = _native_batch_norm_legit_9[0];  _native_batch_norm_legit_9 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_21: \"f16[64, 64, 3, 3]\" = torch.ops.aten.view.default(getitem_27, [64, 64, 3, 3]);  getitem_27 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_9: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.conv2d.default(relu_7, view_21, None, [1, 1], [1, 1]);  relu_7 = view_21 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_9: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.group_norm.default(conv2d_9, 32, p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm2_bias);  conv2d_9 = p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_8: \"f16[s77, 64, 96, 96]\" = torch.ops.aten.relu.default(group_norm_9);  group_norm_9 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_22: \"f16[1, 256, 64]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_conv3_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_10 = torch.ops.aten._native_batch_norm_legit.no_stats(view_22, None, None, True, 0.0, 1e-08);  view_22 = None\n",
       "                    getitem_30: \"f16[1, 256, 64]\" = _native_batch_norm_legit_10[0];  _native_batch_norm_legit_10 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_23: \"f16[256, 64, 1, 1]\" = torch.ops.aten.view.default(getitem_30, [256, 64, 1, 1]);  getitem_30 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_10: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.conv2d.default(relu_8, view_23);  relu_8 = view_23 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_10: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.group_norm.default(conv2d_10, 32, p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm3_bias);  conv2d_10 = p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_227: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.add.Tensor(group_norm_10, relu_6);  group_norm_10 = relu_6 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_9: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.relu.default(add_227);  add_227 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:99 in forward, code: x = pad_same(x, self.kernel_size, self.stride, self.dilation)\n",
       "                    pad_2: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.pad.default(relu_9, [0, 0, 0, 0], 'constant', 0.0)\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_24: \"f16[1, 512, 256]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_downsample_conv_weight, [1, 512, -1]);  p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_downsample_conv_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_11 = torch.ops.aten._native_batch_norm_legit.no_stats(view_24, None, None, True, 0.0, 1e-08);  view_24 = None\n",
       "                    getitem_33: \"f16[1, 512, 256]\" = _native_batch_norm_legit_11[0];  _native_batch_norm_legit_11 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_25: \"f16[512, 256, 1, 1]\" = torch.ops.aten.view.default(getitem_33, [512, 256, 1, 1]);  getitem_33 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_11: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.conv2d.default(pad_2, view_25, None, [2, 2]);  pad_2 = view_25 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_11: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.group_norm.default(conv2d_11, 32, p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_downsample_norm_weight, p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_downsample_norm_bias);  conv2d_11 = p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_downsample_norm_weight = p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_downsample_norm_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_26: \"f16[1, 128, 256]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_conv1_weight, [1, 128, -1]);  p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_12 = torch.ops.aten._native_batch_norm_legit.no_stats(view_26, None, None, True, 0.0, 1e-08);  view_26 = None\n",
       "                    getitem_36: \"f16[1, 128, 256]\" = _native_batch_norm_legit_12[0];  _native_batch_norm_legit_12 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_27: \"f16[128, 256, 1, 1]\" = torch.ops.aten.view.default(getitem_36, [128, 256, 1, 1]);  getitem_36 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_12: \"f16[s77, 128, 96, 96]\" = torch.ops.aten.conv2d.default(relu_9, view_27);  view_27 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_12: \"f16[s77, 128, 96, 96]\" = torch.ops.aten.group_norm.default(conv2d_12, 32, p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm1_bias);  conv2d_12 = p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_10: \"f16[s77, 128, 96, 96]\" = torch.ops.aten.relu.default(group_norm_12);  group_norm_12 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:99 in forward, code: x = pad_same(x, self.kernel_size, self.stride, self.dilation)\n",
       "                    pad_3: \"f16[s77, 128, 97, 97]\" = torch.ops.aten.pad.default(relu_10, [0, 1, 0, 1], 'constant', 0.0);  relu_10 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_28: \"f16[1, 128, 1152]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_conv2_weight, [1, 128, -1]);  p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_13 = torch.ops.aten._native_batch_norm_legit.no_stats(view_28, None, None, True, 0.0, 1e-08);  view_28 = None\n",
       "                    getitem_39: \"f16[1, 128, 1152]\" = _native_batch_norm_legit_13[0];  _native_batch_norm_legit_13 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_29: \"f16[128, 128, 3, 3]\" = torch.ops.aten.view.default(getitem_39, [128, 128, 3, 3]);  getitem_39 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_13: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.conv2d.default(pad_3, view_29, None, [2, 2]);  pad_3 = view_29 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_13: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.group_norm.default(conv2d_13, 32, p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm2_bias);  conv2d_13 = p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_11: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.relu.default(group_norm_13);  group_norm_13 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_30: \"f16[1, 512, 128]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_conv3_weight, [1, 512, -1]);  p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_14 = torch.ops.aten._native_batch_norm_legit.no_stats(view_30, None, None, True, 0.0, 1e-08);  view_30 = None\n",
       "                    getitem_42: \"f16[1, 512, 128]\" = _native_batch_norm_legit_14[0];  _native_batch_norm_legit_14 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_31: \"f16[512, 128, 1, 1]\" = torch.ops.aten.view.default(getitem_42, [512, 128, 1, 1]);  getitem_42 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_14: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.conv2d.default(relu_11, view_31);  relu_11 = view_31 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_14: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.group_norm.default(conv2d_14, 32, p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm3_bias);  conv2d_14 = p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_313: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.add.Tensor(group_norm_14, group_norm_11);  group_norm_14 = group_norm_11 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_12: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.relu.default(add_313);  add_313 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_32: \"f16[1, 128, 512]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_conv1_weight, [1, 128, -1]);  p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_15 = torch.ops.aten._native_batch_norm_legit.no_stats(view_32, None, None, True, 0.0, 1e-08);  view_32 = None\n",
       "                    getitem_45: \"f16[1, 128, 512]\" = _native_batch_norm_legit_15[0];  _native_batch_norm_legit_15 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_33: \"f16[128, 512, 1, 1]\" = torch.ops.aten.view.default(getitem_45, [128, 512, 1, 1]);  getitem_45 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_15: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.conv2d.default(relu_12, view_33);  view_33 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_15: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.group_norm.default(conv2d_15, 32, p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm1_bias);  conv2d_15 = p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_13: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.relu.default(group_norm_15);  group_norm_15 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_34: \"f16[1, 128, 1152]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_conv2_weight, [1, 128, -1]);  p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_16 = torch.ops.aten._native_batch_norm_legit.no_stats(view_34, None, None, True, 0.0, 1e-08);  view_34 = None\n",
       "                    getitem_48: \"f16[1, 128, 1152]\" = _native_batch_norm_legit_16[0];  _native_batch_norm_legit_16 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_35: \"f16[128, 128, 3, 3]\" = torch.ops.aten.view.default(getitem_48, [128, 128, 3, 3]);  getitem_48 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_16: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.conv2d.default(relu_13, view_35, None, [1, 1], [1, 1]);  relu_13 = view_35 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_16: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.group_norm.default(conv2d_16, 32, p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm2_bias);  conv2d_16 = p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_14: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.relu.default(group_norm_16);  group_norm_16 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_36: \"f16[1, 512, 128]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_conv3_weight, [1, 512, -1]);  p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_17 = torch.ops.aten._native_batch_norm_legit.no_stats(view_36, None, None, True, 0.0, 1e-08);  view_36 = None\n",
       "                    getitem_51: \"f16[1, 512, 128]\" = _native_batch_norm_legit_17[0];  _native_batch_norm_legit_17 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_37: \"f16[512, 128, 1, 1]\" = torch.ops.aten.view.default(getitem_51, [512, 128, 1, 1]);  getitem_51 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_17: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.conv2d.default(relu_14, view_37);  relu_14 = view_37 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_17: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.group_norm.default(conv2d_17, 32, p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm3_bias);  conv2d_17 = p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_379: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.add.Tensor(group_norm_17, relu_12);  group_norm_17 = relu_12 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_15: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.relu.default(add_379);  add_379 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_38: \"f16[1, 128, 512]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_conv1_weight, [1, 128, -1]);  p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_18 = torch.ops.aten._native_batch_norm_legit.no_stats(view_38, None, None, True, 0.0, 1e-08);  view_38 = None\n",
       "                    getitem_54: \"f16[1, 128, 512]\" = _native_batch_norm_legit_18[0];  _native_batch_norm_legit_18 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_39: \"f16[128, 512, 1, 1]\" = torch.ops.aten.view.default(getitem_54, [128, 512, 1, 1]);  getitem_54 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_18: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.conv2d.default(relu_15, view_39);  view_39 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_18: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.group_norm.default(conv2d_18, 32, p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm1_bias);  conv2d_18 = p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_16: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.relu.default(group_norm_18);  group_norm_18 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_40: \"f16[1, 128, 1152]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_conv2_weight, [1, 128, -1]);  p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_19 = torch.ops.aten._native_batch_norm_legit.no_stats(view_40, None, None, True, 0.0, 1e-08);  view_40 = None\n",
       "                    getitem_57: \"f16[1, 128, 1152]\" = _native_batch_norm_legit_19[0];  _native_batch_norm_legit_19 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_41: \"f16[128, 128, 3, 3]\" = torch.ops.aten.view.default(getitem_57, [128, 128, 3, 3]);  getitem_57 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_19: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.conv2d.default(relu_16, view_41, None, [1, 1], [1, 1]);  relu_16 = view_41 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_19: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.group_norm.default(conv2d_19, 32, p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm2_bias);  conv2d_19 = p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_17: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.relu.default(group_norm_19);  group_norm_19 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_42: \"f16[1, 512, 128]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_conv3_weight, [1, 512, -1]);  p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_20 = torch.ops.aten._native_batch_norm_legit.no_stats(view_42, None, None, True, 0.0, 1e-08);  view_42 = None\n",
       "                    getitem_60: \"f16[1, 512, 128]\" = _native_batch_norm_legit_20[0];  _native_batch_norm_legit_20 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_43: \"f16[512, 128, 1, 1]\" = torch.ops.aten.view.default(getitem_60, [512, 128, 1, 1]);  getitem_60 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_20: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.conv2d.default(relu_17, view_43);  relu_17 = view_43 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_20: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.group_norm.default(conv2d_20, 32, p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm3_bias);  conv2d_20 = p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_445: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.add.Tensor(group_norm_20, relu_15);  group_norm_20 = relu_15 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_18: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.relu.default(add_445);  add_445 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_44: \"f16[1, 128, 512]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_conv1_weight, [1, 128, -1]);  p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_21 = torch.ops.aten._native_batch_norm_legit.no_stats(view_44, None, None, True, 0.0, 1e-08);  view_44 = None\n",
       "                    getitem_63: \"f16[1, 128, 512]\" = _native_batch_norm_legit_21[0];  _native_batch_norm_legit_21 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_45: \"f16[128, 512, 1, 1]\" = torch.ops.aten.view.default(getitem_63, [128, 512, 1, 1]);  getitem_63 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_21: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.conv2d.default(relu_18, view_45);  view_45 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_21: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.group_norm.default(conv2d_21, 32, p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm1_bias);  conv2d_21 = p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_19: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.relu.default(group_norm_21);  group_norm_21 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_46: \"f16[1, 128, 1152]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_conv2_weight, [1, 128, -1]);  p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_22 = torch.ops.aten._native_batch_norm_legit.no_stats(view_46, None, None, True, 0.0, 1e-08);  view_46 = None\n",
       "                    getitem_66: \"f16[1, 128, 1152]\" = _native_batch_norm_legit_22[0];  _native_batch_norm_legit_22 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_47: \"f16[128, 128, 3, 3]\" = torch.ops.aten.view.default(getitem_66, [128, 128, 3, 3]);  getitem_66 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_22: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.conv2d.default(relu_19, view_47, None, [1, 1], [1, 1]);  relu_19 = view_47 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_22: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.group_norm.default(conv2d_22, 32, p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm2_bias);  conv2d_22 = p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_20: \"f16[s77, 128, 48, 48]\" = torch.ops.aten.relu.default(group_norm_22);  group_norm_22 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_48: \"f16[1, 512, 128]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_conv3_weight, [1, 512, -1]);  p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_23 = torch.ops.aten._native_batch_norm_legit.no_stats(view_48, None, None, True, 0.0, 1e-08);  view_48 = None\n",
       "                    getitem_69: \"f16[1, 512, 128]\" = _native_batch_norm_legit_23[0];  _native_batch_norm_legit_23 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_49: \"f16[512, 128, 1, 1]\" = torch.ops.aten.view.default(getitem_69, [512, 128, 1, 1]);  getitem_69 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_23: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.conv2d.default(relu_20, view_49);  relu_20 = view_49 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_23: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.group_norm.default(conv2d_23, 32, p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm3_bias);  conv2d_23 = p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_511: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.add.Tensor(group_norm_23, relu_18);  group_norm_23 = relu_18 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_21: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.relu.default(add_511);  add_511 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:99 in forward, code: x = pad_same(x, self.kernel_size, self.stride, self.dilation)\n",
       "                    pad_4: \"f16[s77, 512, 48, 48]\" = torch.ops.aten.pad.default(relu_21, [0, 0, 0, 0], 'constant', 0.0)\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_50: \"f16[1, 1024, 512]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_downsample_conv_weight, [1, 1024, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_downsample_conv_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_24 = torch.ops.aten._native_batch_norm_legit.no_stats(view_50, None, None, True, 0.0, 1e-08);  view_50 = None\n",
       "                    getitem_72: \"f16[1, 1024, 512]\" = _native_batch_norm_legit_24[0];  _native_batch_norm_legit_24 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_51: \"f16[1024, 512, 1, 1]\" = torch.ops.aten.view.default(getitem_72, [1024, 512, 1, 1]);  getitem_72 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_24: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.conv2d.default(pad_4, view_51, None, [2, 2]);  pad_4 = view_51 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_24: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_24, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_downsample_norm_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_downsample_norm_bias);  conv2d_24 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_downsample_norm_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_downsample_norm_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_52: \"f16[1, 256, 512]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_conv1_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_25 = torch.ops.aten._native_batch_norm_legit.no_stats(view_52, None, None, True, 0.0, 1e-08);  view_52 = None\n",
       "                    getitem_75: \"f16[1, 256, 512]\" = _native_batch_norm_legit_25[0];  _native_batch_norm_legit_25 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_53: \"f16[256, 512, 1, 1]\" = torch.ops.aten.view.default(getitem_75, [256, 512, 1, 1]);  getitem_75 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_25: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.conv2d.default(relu_21, view_53);  view_53 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_25: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.group_norm.default(conv2d_25, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm1_bias);  conv2d_25 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_22: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.relu.default(group_norm_25);  group_norm_25 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:99 in forward, code: x = pad_same(x, self.kernel_size, self.stride, self.dilation)\n",
       "                    pad_5: \"f16[s77, 256, 49, 49]\" = torch.ops.aten.pad.default(relu_22, [0, 1, 0, 1], 'constant', 0.0);  relu_22 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_54: \"f16[1, 256, 2304]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_conv2_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_26 = torch.ops.aten._native_batch_norm_legit.no_stats(view_54, None, None, True, 0.0, 1e-08);  view_54 = None\n",
       "                    getitem_78: \"f16[1, 256, 2304]\" = _native_batch_norm_legit_26[0];  _native_batch_norm_legit_26 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_55: \"f16[256, 256, 3, 3]\" = torch.ops.aten.view.default(getitem_78, [256, 256, 3, 3]);  getitem_78 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_26: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(pad_5, view_55, None, [2, 2]);  pad_5 = view_55 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_26: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_26, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm2_bias);  conv2d_26 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_23: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_26);  group_norm_26 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_56: \"f16[1, 1024, 256]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_conv3_weight, [1, 1024, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_27 = torch.ops.aten._native_batch_norm_legit.no_stats(view_56, None, None, True, 0.0, 1e-08);  view_56 = None\n",
       "                    getitem_81: \"f16[1, 1024, 256]\" = _native_batch_norm_legit_27[0];  _native_batch_norm_legit_27 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_57: \"f16[1024, 256, 1, 1]\" = torch.ops.aten.view.default(getitem_81, [1024, 256, 1, 1]);  getitem_81 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_27: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.conv2d.default(relu_23, view_57);  relu_23 = view_57 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_27: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_27, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm3_bias);  conv2d_27 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_597: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.add.Tensor(group_norm_27, group_norm_24);  group_norm_27 = group_norm_24 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_24: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.relu.default(add_597);  add_597 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_58: \"f16[1, 256, 1024]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_conv1_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_28 = torch.ops.aten._native_batch_norm_legit.no_stats(view_58, None, None, True, 0.0, 1e-08);  view_58 = None\n",
       "                    getitem_84: \"f16[1, 256, 1024]\" = _native_batch_norm_legit_28[0];  _native_batch_norm_legit_28 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_59: \"f16[256, 1024, 1, 1]\" = torch.ops.aten.view.default(getitem_84, [256, 1024, 1, 1]);  getitem_84 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_28: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_24, view_59);  view_59 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_28: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_28, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm1_bias);  conv2d_28 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_25: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_28);  group_norm_28 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_60: \"f16[1, 256, 2304]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_conv2_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_29 = torch.ops.aten._native_batch_norm_legit.no_stats(view_60, None, None, True, 0.0, 1e-08);  view_60 = None\n",
       "                    getitem_87: \"f16[1, 256, 2304]\" = _native_batch_norm_legit_29[0];  _native_batch_norm_legit_29 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_61: \"f16[256, 256, 3, 3]\" = torch.ops.aten.view.default(getitem_87, [256, 256, 3, 3]);  getitem_87 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_29: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_25, view_61, None, [1, 1], [1, 1]);  relu_25 = view_61 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_29: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_29, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm2_bias);  conv2d_29 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_26: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_29);  group_norm_29 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_62: \"f16[1, 1024, 256]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_conv3_weight, [1, 1024, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_30 = torch.ops.aten._native_batch_norm_legit.no_stats(view_62, None, None, True, 0.0, 1e-08);  view_62 = None\n",
       "                    getitem_90: \"f16[1, 1024, 256]\" = _native_batch_norm_legit_30[0];  _native_batch_norm_legit_30 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_63: \"f16[1024, 256, 1, 1]\" = torch.ops.aten.view.default(getitem_90, [1024, 256, 1, 1]);  getitem_90 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_30: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.conv2d.default(relu_26, view_63);  relu_26 = view_63 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_30: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_30, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm3_bias);  conv2d_30 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_663: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.add.Tensor(group_norm_30, relu_24);  group_norm_30 = relu_24 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_27: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.relu.default(add_663);  add_663 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_64: \"f16[1, 256, 1024]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_conv1_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_31 = torch.ops.aten._native_batch_norm_legit.no_stats(view_64, None, None, True, 0.0, 1e-08);  view_64 = None\n",
       "                    getitem_93: \"f16[1, 256, 1024]\" = _native_batch_norm_legit_31[0];  _native_batch_norm_legit_31 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_65: \"f16[256, 1024, 1, 1]\" = torch.ops.aten.view.default(getitem_93, [256, 1024, 1, 1]);  getitem_93 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_31: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_27, view_65);  view_65 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_31: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_31, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm1_bias);  conv2d_31 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_28: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_31);  group_norm_31 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_66: \"f16[1, 256, 2304]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_conv2_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_32 = torch.ops.aten._native_batch_norm_legit.no_stats(view_66, None, None, True, 0.0, 1e-08);  view_66 = None\n",
       "                    getitem_96: \"f16[1, 256, 2304]\" = _native_batch_norm_legit_32[0];  _native_batch_norm_legit_32 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_67: \"f16[256, 256, 3, 3]\" = torch.ops.aten.view.default(getitem_96, [256, 256, 3, 3]);  getitem_96 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_32: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_28, view_67, None, [1, 1], [1, 1]);  relu_28 = view_67 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_32: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_32, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm2_bias);  conv2d_32 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_29: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_32);  group_norm_32 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_68: \"f16[1, 1024, 256]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_conv3_weight, [1, 1024, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_33 = torch.ops.aten._native_batch_norm_legit.no_stats(view_68, None, None, True, 0.0, 1e-08);  view_68 = None\n",
       "                    getitem_99: \"f16[1, 1024, 256]\" = _native_batch_norm_legit_33[0];  _native_batch_norm_legit_33 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_69: \"f16[1024, 256, 1, 1]\" = torch.ops.aten.view.default(getitem_99, [1024, 256, 1, 1]);  getitem_99 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_33: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.conv2d.default(relu_29, view_69);  relu_29 = view_69 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_33: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_33, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm3_bias);  conv2d_33 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_729: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.add.Tensor(group_norm_33, relu_27);  group_norm_33 = relu_27 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_30: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.relu.default(add_729);  add_729 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_70: \"f16[1, 256, 1024]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_conv1_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_34 = torch.ops.aten._native_batch_norm_legit.no_stats(view_70, None, None, True, 0.0, 1e-08);  view_70 = None\n",
       "                    getitem_102: \"f16[1, 256, 1024]\" = _native_batch_norm_legit_34[0];  _native_batch_norm_legit_34 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_71: \"f16[256, 1024, 1, 1]\" = torch.ops.aten.view.default(getitem_102, [256, 1024, 1, 1]);  getitem_102 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_34: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_30, view_71);  view_71 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_34: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_34, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm1_bias);  conv2d_34 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_31: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_34);  group_norm_34 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_72: \"f16[1, 256, 2304]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_conv2_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_35 = torch.ops.aten._native_batch_norm_legit.no_stats(view_72, None, None, True, 0.0, 1e-08);  view_72 = None\n",
       "                    getitem_105: \"f16[1, 256, 2304]\" = _native_batch_norm_legit_35[0];  _native_batch_norm_legit_35 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_73: \"f16[256, 256, 3, 3]\" = torch.ops.aten.view.default(getitem_105, [256, 256, 3, 3]);  getitem_105 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_35: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_31, view_73, None, [1, 1], [1, 1]);  relu_31 = view_73 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_35: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_35, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm2_bias);  conv2d_35 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_32: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_35);  group_norm_35 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_74: \"f16[1, 1024, 256]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_conv3_weight, [1, 1024, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_36 = torch.ops.aten._native_batch_norm_legit.no_stats(view_74, None, None, True, 0.0, 1e-08);  view_74 = None\n",
       "                    getitem_108: \"f16[1, 1024, 256]\" = _native_batch_norm_legit_36[0];  _native_batch_norm_legit_36 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_75: \"f16[1024, 256, 1, 1]\" = torch.ops.aten.view.default(getitem_108, [1024, 256, 1, 1]);  getitem_108 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_36: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.conv2d.default(relu_32, view_75);  relu_32 = view_75 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_36: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_36, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm3_bias);  conv2d_36 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_795: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.add.Tensor(group_norm_36, relu_30);  group_norm_36 = relu_30 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_33: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.relu.default(add_795);  add_795 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_76: \"f16[1, 256, 1024]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_conv1_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_37 = torch.ops.aten._native_batch_norm_legit.no_stats(view_76, None, None, True, 0.0, 1e-08);  view_76 = None\n",
       "                    getitem_111: \"f16[1, 256, 1024]\" = _native_batch_norm_legit_37[0];  _native_batch_norm_legit_37 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_77: \"f16[256, 1024, 1, 1]\" = torch.ops.aten.view.default(getitem_111, [256, 1024, 1, 1]);  getitem_111 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_37: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_33, view_77);  view_77 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_37: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_37, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm1_bias);  conv2d_37 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_34: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_37);  group_norm_37 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_78: \"f16[1, 256, 2304]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_conv2_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_38 = torch.ops.aten._native_batch_norm_legit.no_stats(view_78, None, None, True, 0.0, 1e-08);  view_78 = None\n",
       "                    getitem_114: \"f16[1, 256, 2304]\" = _native_batch_norm_legit_38[0];  _native_batch_norm_legit_38 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_79: \"f16[256, 256, 3, 3]\" = torch.ops.aten.view.default(getitem_114, [256, 256, 3, 3]);  getitem_114 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_38: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_34, view_79, None, [1, 1], [1, 1]);  relu_34 = view_79 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_38: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_38, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm2_bias);  conv2d_38 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_35: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_38);  group_norm_38 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_80: \"f16[1, 1024, 256]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_conv3_weight, [1, 1024, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_39 = torch.ops.aten._native_batch_norm_legit.no_stats(view_80, None, None, True, 0.0, 1e-08);  view_80 = None\n",
       "                    getitem_117: \"f16[1, 1024, 256]\" = _native_batch_norm_legit_39[0];  _native_batch_norm_legit_39 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_81: \"f16[1024, 256, 1, 1]\" = torch.ops.aten.view.default(getitem_117, [1024, 256, 1, 1]);  getitem_117 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_39: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.conv2d.default(relu_35, view_81);  relu_35 = view_81 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_39: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_39, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm3_bias);  conv2d_39 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_861: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.add.Tensor(group_norm_39, relu_33);  group_norm_39 = relu_33 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_36: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.relu.default(add_861);  add_861 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_82: \"f16[1, 256, 1024]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_conv1_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_40 = torch.ops.aten._native_batch_norm_legit.no_stats(view_82, None, None, True, 0.0, 1e-08);  view_82 = None\n",
       "                    getitem_120: \"f16[1, 256, 1024]\" = _native_batch_norm_legit_40[0];  _native_batch_norm_legit_40 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_83: \"f16[256, 1024, 1, 1]\" = torch.ops.aten.view.default(getitem_120, [256, 1024, 1, 1]);  getitem_120 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_40: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_36, view_83);  view_83 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_40: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_40, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm1_bias);  conv2d_40 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_37: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_40);  group_norm_40 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_84: \"f16[1, 256, 2304]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_conv2_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_41 = torch.ops.aten._native_batch_norm_legit.no_stats(view_84, None, None, True, 0.0, 1e-08);  view_84 = None\n",
       "                    getitem_123: \"f16[1, 256, 2304]\" = _native_batch_norm_legit_41[0];  _native_batch_norm_legit_41 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_85: \"f16[256, 256, 3, 3]\" = torch.ops.aten.view.default(getitem_123, [256, 256, 3, 3]);  getitem_123 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_41: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_37, view_85, None, [1, 1], [1, 1]);  relu_37 = view_85 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_41: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_41, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm2_bias);  conv2d_41 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_38: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_41);  group_norm_41 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_86: \"f16[1, 1024, 256]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_conv3_weight, [1, 1024, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_42 = torch.ops.aten._native_batch_norm_legit.no_stats(view_86, None, None, True, 0.0, 1e-08);  view_86 = None\n",
       "                    getitem_126: \"f16[1, 1024, 256]\" = _native_batch_norm_legit_42[0];  _native_batch_norm_legit_42 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_87: \"f16[1024, 256, 1, 1]\" = torch.ops.aten.view.default(getitem_126, [1024, 256, 1, 1]);  getitem_126 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_42: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.conv2d.default(relu_38, view_87);  relu_38 = view_87 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_42: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_42, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm3_bias);  conv2d_42 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_927: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.add.Tensor(group_norm_42, relu_36);  group_norm_42 = relu_36 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_39: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.relu.default(add_927);  add_927 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_88: \"f16[1, 256, 1024]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_conv1_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_43 = torch.ops.aten._native_batch_norm_legit.no_stats(view_88, None, None, True, 0.0, 1e-08);  view_88 = None\n",
       "                    getitem_129: \"f16[1, 256, 1024]\" = _native_batch_norm_legit_43[0];  _native_batch_norm_legit_43 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_89: \"f16[256, 1024, 1, 1]\" = torch.ops.aten.view.default(getitem_129, [256, 1024, 1, 1]);  getitem_129 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_43: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_39, view_89);  view_89 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_43: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_43, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm1_bias);  conv2d_43 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_40: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_43);  group_norm_43 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_90: \"f16[1, 256, 2304]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_conv2_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_44 = torch.ops.aten._native_batch_norm_legit.no_stats(view_90, None, None, True, 0.0, 1e-08);  view_90 = None\n",
       "                    getitem_132: \"f16[1, 256, 2304]\" = _native_batch_norm_legit_44[0];  _native_batch_norm_legit_44 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_91: \"f16[256, 256, 3, 3]\" = torch.ops.aten.view.default(getitem_132, [256, 256, 3, 3]);  getitem_132 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_44: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_40, view_91, None, [1, 1], [1, 1]);  relu_40 = view_91 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_44: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_44, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm2_bias);  conv2d_44 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_41: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_44);  group_norm_44 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_92: \"f16[1, 1024, 256]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_conv3_weight, [1, 1024, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_45 = torch.ops.aten._native_batch_norm_legit.no_stats(view_92, None, None, True, 0.0, 1e-08);  view_92 = None\n",
       "                    getitem_135: \"f16[1, 1024, 256]\" = _native_batch_norm_legit_45[0];  _native_batch_norm_legit_45 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_93: \"f16[1024, 256, 1, 1]\" = torch.ops.aten.view.default(getitem_135, [1024, 256, 1, 1]);  getitem_135 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_45: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.conv2d.default(relu_41, view_93);  relu_41 = view_93 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_45: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_45, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm3_bias);  conv2d_45 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_993: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.add.Tensor(group_norm_45, relu_39);  group_norm_45 = relu_39 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_42: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.relu.default(add_993);  add_993 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_94: \"f16[1, 256, 1024]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_conv1_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_46 = torch.ops.aten._native_batch_norm_legit.no_stats(view_94, None, None, True, 0.0, 1e-08);  view_94 = None\n",
       "                    getitem_138: \"f16[1, 256, 1024]\" = _native_batch_norm_legit_46[0];  _native_batch_norm_legit_46 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_95: \"f16[256, 1024, 1, 1]\" = torch.ops.aten.view.default(getitem_138, [256, 1024, 1, 1]);  getitem_138 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_46: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_42, view_95);  view_95 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_46: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_46, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm1_bias);  conv2d_46 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_43: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_46);  group_norm_46 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_96: \"f16[1, 256, 2304]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_conv2_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_47 = torch.ops.aten._native_batch_norm_legit.no_stats(view_96, None, None, True, 0.0, 1e-08);  view_96 = None\n",
       "                    getitem_141: \"f16[1, 256, 2304]\" = _native_batch_norm_legit_47[0];  _native_batch_norm_legit_47 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_97: \"f16[256, 256, 3, 3]\" = torch.ops.aten.view.default(getitem_141, [256, 256, 3, 3]);  getitem_141 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_47: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_43, view_97, None, [1, 1], [1, 1]);  relu_43 = view_97 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_47: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_47, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm2_bias);  conv2d_47 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_44: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_47);  group_norm_47 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_98: \"f16[1, 1024, 256]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_conv3_weight, [1, 1024, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_48 = torch.ops.aten._native_batch_norm_legit.no_stats(view_98, None, None, True, 0.0, 1e-08);  view_98 = None\n",
       "                    getitem_144: \"f16[1, 1024, 256]\" = _native_batch_norm_legit_48[0];  _native_batch_norm_legit_48 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_99: \"f16[1024, 256, 1, 1]\" = torch.ops.aten.view.default(getitem_144, [1024, 256, 1, 1]);  getitem_144 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_48: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.conv2d.default(relu_44, view_99);  relu_44 = view_99 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_48: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_48, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm3_bias);  conv2d_48 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_1059: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.add.Tensor(group_norm_48, relu_42);  group_norm_48 = relu_42 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_45: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.relu.default(add_1059);  add_1059 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_100: \"f16[1, 256, 1024]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_conv1_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_49 = torch.ops.aten._native_batch_norm_legit.no_stats(view_100, None, None, True, 0.0, 1e-08);  view_100 = None\n",
       "                    getitem_147: \"f16[1, 256, 1024]\" = _native_batch_norm_legit_49[0];  _native_batch_norm_legit_49 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_101: \"f16[256, 1024, 1, 1]\" = torch.ops.aten.view.default(getitem_147, [256, 1024, 1, 1]);  getitem_147 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_49: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_45, view_101);  view_101 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_49: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_49, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm1_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm1_bias);  conv2d_49 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm1_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_46: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_49);  group_norm_49 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_102: \"f16[1, 256, 2304]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_conv2_weight, [1, 256, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_50 = torch.ops.aten._native_batch_norm_legit.no_stats(view_102, None, None, True, 0.0, 1e-08);  view_102 = None\n",
       "                    getitem_150: \"f16[1, 256, 2304]\" = _native_batch_norm_legit_50[0];  _native_batch_norm_legit_50 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_103: \"f16[256, 256, 3, 3]\" = torch.ops.aten.view.default(getitem_150, [256, 256, 3, 3]);  getitem_150 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_50: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_46, view_103, None, [1, 1], [1, 1]);  relu_46 = view_103 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_50: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_50, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm2_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm2_bias);  conv2d_50 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm2_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_47: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(group_norm_50);  group_norm_50 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:101 in forward, code: self.weight.reshape(1, self.out_channels, -1),\n",
       "                    view_104: \"f16[1, 1024, 256]\" = torch.ops.aten.view.default(p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_conv3_weight, [1, 1024, -1]);  p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:100 in forward, code: weight = F.batch_norm(\n",
       "                    _native_batch_norm_legit_51 = torch.ops.aten._native_batch_norm_legit.no_stats(view_104, None, None, True, 0.0, 1e-08);  view_104 = None\n",
       "                    getitem_153: \"f16[1, 1024, 256]\" = _native_batch_norm_legit_51[0];  _native_batch_norm_legit_51 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:107 in forward, code: ).reshape_as(self.weight)\n",
       "                    view_105: \"f16[1024, 256, 1, 1]\" = torch.ops.aten.view.default(getitem_153, [1024, 256, 1, 1]);  getitem_153 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/std_conv.py:108 in forward, code: x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
       "                    conv2d_51: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.conv2d.default(relu_47, view_105);  relu_47 = view_105 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm_act.py:406 in forward, code: x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n",
       "                    group_norm_51: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.group_norm.default(conv2d_51, 32, p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm3_weight, p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm3_bias);  conv2d_51 = p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm3_weight = p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm3_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/resnetv2.py:322 in forward, code: x = self.act3(x + shortcut)\n",
       "                    add_1125: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.add.Tensor(group_norm_51, relu_45);  group_norm_51 = relu_45 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_48: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.relu.default(add_1125);  add_1125 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone: \"f16[s77, 1024, 24, 24]\" = torch.ops.aten.clone.default(relu_48);  relu_48 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_52: \"f16[s77, 768, 24, 24]\" = torch.ops.aten.conv2d.default(clone, p_pretrained_model_patch_embed_proj_weight, p_pretrained_model_patch_embed_proj_bias);  clone = p_pretrained_model_patch_embed_proj_weight = p_pretrained_model_patch_embed_proj_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/dpt_depth.py:114 in forward, code: layers = self.forward_transformer(self.pretrained, x)\n",
       "                    view_106: \"f16[s77, 768, 576]\" = torch.ops.aten.view.default(conv2d_52, [sym_size_int_7, 768, 576]);  conv2d_52 = None\n",
       "                    transpose: \"f16[s77, 576, 768]\" = torch.ops.aten.transpose.int(view_106, 1, 2);  view_106 = None\n",
       "                    expand: \"f16[s77, 1, 768]\" = torch.ops.aten.expand.default(p_pretrained_model_cls_token, [sym_size_int_7, -1, -1]);  p_pretrained_model_cls_token = None\n",
       "                    cat_1: \"f16[s77, 577, 768]\" = torch.ops.aten.cat.default([expand, transpose], 1);  expand = transpose = None\n",
       "                    add_1163: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(cat_1, cat);  cat_1 = cat = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_1: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(add_1163);  add_1163 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(clone_1, [768], p_pretrained_model_blocks_0_norm1_weight, p_pretrained_model_blocks_0_norm1_bias, 1e-06);  p_pretrained_model_blocks_0_norm1_weight = p_pretrained_model_blocks_0_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear: \"f16[s77, 577, 2304]\" = torch.ops.aten.linear.default(layer_norm, p_pretrained_model_blocks_0_attn_qkv_weight, p_pretrained_model_blocks_0_attn_qkv_bias);  layer_norm = p_pretrained_model_blocks_0_attn_qkv_weight = p_pretrained_model_blocks_0_attn_qkv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:89 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
       "                    view_107: \"f16[s77, 577, 3, 12, 64]\" = torch.ops.aten.view.default(linear, [sym_size_int_7, 577, 3, 12, 64]);  linear = None\n",
       "                    permute_2: \"f16[3, s77, 12, 577, 64]\" = torch.ops.aten.permute.default(view_107, [2, 0, 3, 1, 4]);  view_107 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:90 in forward, code: q, k, v = qkv.unbind(0)\n",
       "                    unbind = torch.ops.aten.unbind.int(permute_2);  permute_2 = None\n",
       "                    getitem_156: \"f16[s77, 12, 577, 64]\" = unbind[0]\n",
       "                    getitem_157: \"f16[s77, 12, 577, 64]\" = unbind[1]\n",
       "                    getitem_158: \"f16[s77, 12, 577, 64]\" = unbind[2];  unbind = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:94 in forward, code: x = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention: \"f16[s77, 12, 577, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_156, getitem_157, getitem_158);  getitem_156 = getitem_157 = getitem_158 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:107 in forward, code: x = x.transpose(1, 2).reshape(B, N, self.attn_dim)\n",
       "                    transpose_1: \"f16[s77, 577, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None\n",
       "                    view_108: \"f16[s77, 577, 768]\" = torch.ops.aten.view.default(transpose_1, [sym_size_int_7, 577, 768]);  transpose_1 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_1: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(view_108, p_pretrained_model_blocks_0_attn_proj_weight, p_pretrained_model_blocks_0_attn_proj_bias);  view_108 = p_pretrained_model_blocks_0_attn_proj_weight = p_pretrained_model_blocks_0_attn_proj_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_2: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_1);  linear_1 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:205 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
       "                    add_1228: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(clone_1, clone_2);  clone_1 = clone_2 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_1: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1228, [768], p_pretrained_model_blocks_0_norm2_weight, p_pretrained_model_blocks_0_norm2_bias, 1e-06);  p_pretrained_model_blocks_0_norm2_weight = p_pretrained_model_blocks_0_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_2: \"f16[s77, 577, 3072]\" = torch.ops.aten.linear.default(layer_norm_1, p_pretrained_model_blocks_0_mlp_fc1_weight, p_pretrained_model_blocks_0_mlp_fc1_bias);  layer_norm_1 = p_pretrained_model_blocks_0_mlp_fc1_weight = p_pretrained_model_blocks_0_mlp_fc1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:815 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu: \"f16[s77, 577, 3072]\" = torch.ops.aten.gelu.default(linear_2);  linear_2 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_3: \"f16[s77, 577, 3072]\" = torch.ops.aten.clone.default(gelu);  gelu = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_3: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(clone_3, p_pretrained_model_blocks_0_mlp_fc2_weight, p_pretrained_model_blocks_0_mlp_fc2_bias);  clone_3 = p_pretrained_model_blocks_0_mlp_fc2_weight = p_pretrained_model_blocks_0_mlp_fc2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_4: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_3);  linear_3 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:206 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_1257: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1228, clone_4);  add_1228 = clone_4 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_2: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1257, [768], p_pretrained_model_blocks_1_norm1_weight, p_pretrained_model_blocks_1_norm1_bias, 1e-06);  p_pretrained_model_blocks_1_norm1_weight = p_pretrained_model_blocks_1_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_4: \"f16[s77, 577, 2304]\" = torch.ops.aten.linear.default(layer_norm_2, p_pretrained_model_blocks_1_attn_qkv_weight, p_pretrained_model_blocks_1_attn_qkv_bias);  layer_norm_2 = p_pretrained_model_blocks_1_attn_qkv_weight = p_pretrained_model_blocks_1_attn_qkv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:89 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
       "                    view_109: \"f16[s77, 577, 3, 12, 64]\" = torch.ops.aten.view.default(linear_4, [sym_size_int_7, 577, 3, 12, 64]);  linear_4 = None\n",
       "                    permute_3: \"f16[3, s77, 12, 577, 64]\" = torch.ops.aten.permute.default(view_109, [2, 0, 3, 1, 4]);  view_109 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:90 in forward, code: q, k, v = qkv.unbind(0)\n",
       "                    unbind_1 = torch.ops.aten.unbind.int(permute_3);  permute_3 = None\n",
       "                    getitem_159: \"f16[s77, 12, 577, 64]\" = unbind_1[0]\n",
       "                    getitem_160: \"f16[s77, 12, 577, 64]\" = unbind_1[1]\n",
       "                    getitem_161: \"f16[s77, 12, 577, 64]\" = unbind_1[2];  unbind_1 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:94 in forward, code: x = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_1: \"f16[s77, 12, 577, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_159, getitem_160, getitem_161);  getitem_159 = getitem_160 = getitem_161 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:107 in forward, code: x = x.transpose(1, 2).reshape(B, N, self.attn_dim)\n",
       "                    transpose_2: \"f16[s77, 577, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None\n",
       "                    view_110: \"f16[s77, 577, 768]\" = torch.ops.aten.view.default(transpose_2, [sym_size_int_7, 577, 768]);  transpose_2 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_5: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(view_110, p_pretrained_model_blocks_1_attn_proj_weight, p_pretrained_model_blocks_1_attn_proj_bias);  view_110 = p_pretrained_model_blocks_1_attn_proj_weight = p_pretrained_model_blocks_1_attn_proj_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_5: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_5);  linear_5 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:205 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
       "                    add_1318: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1257, clone_5);  add_1257 = clone_5 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_3: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1318, [768], p_pretrained_model_blocks_1_norm2_weight, p_pretrained_model_blocks_1_norm2_bias, 1e-06);  p_pretrained_model_blocks_1_norm2_weight = p_pretrained_model_blocks_1_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_6: \"f16[s77, 577, 3072]\" = torch.ops.aten.linear.default(layer_norm_3, p_pretrained_model_blocks_1_mlp_fc1_weight, p_pretrained_model_blocks_1_mlp_fc1_bias);  layer_norm_3 = p_pretrained_model_blocks_1_mlp_fc1_weight = p_pretrained_model_blocks_1_mlp_fc1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:815 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_1: \"f16[s77, 577, 3072]\" = torch.ops.aten.gelu.default(linear_6);  linear_6 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_6: \"f16[s77, 577, 3072]\" = torch.ops.aten.clone.default(gelu_1);  gelu_1 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_7: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(clone_6, p_pretrained_model_blocks_1_mlp_fc2_weight, p_pretrained_model_blocks_1_mlp_fc2_bias);  clone_6 = p_pretrained_model_blocks_1_mlp_fc2_weight = p_pretrained_model_blocks_1_mlp_fc2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_7: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_7);  linear_7 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:206 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_1347: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1318, clone_7);  add_1318 = clone_7 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_4: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1347, [768], p_pretrained_model_blocks_2_norm1_weight, p_pretrained_model_blocks_2_norm1_bias, 1e-06);  p_pretrained_model_blocks_2_norm1_weight = p_pretrained_model_blocks_2_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_8: \"f16[s77, 577, 2304]\" = torch.ops.aten.linear.default(layer_norm_4, p_pretrained_model_blocks_2_attn_qkv_weight, p_pretrained_model_blocks_2_attn_qkv_bias);  layer_norm_4 = p_pretrained_model_blocks_2_attn_qkv_weight = p_pretrained_model_blocks_2_attn_qkv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:89 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
       "                    view_111: \"f16[s77, 577, 3, 12, 64]\" = torch.ops.aten.view.default(linear_8, [sym_size_int_7, 577, 3, 12, 64]);  linear_8 = None\n",
       "                    permute_4: \"f16[3, s77, 12, 577, 64]\" = torch.ops.aten.permute.default(view_111, [2, 0, 3, 1, 4]);  view_111 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:90 in forward, code: q, k, v = qkv.unbind(0)\n",
       "                    unbind_2 = torch.ops.aten.unbind.int(permute_4);  permute_4 = None\n",
       "                    getitem_162: \"f16[s77, 12, 577, 64]\" = unbind_2[0]\n",
       "                    getitem_163: \"f16[s77, 12, 577, 64]\" = unbind_2[1]\n",
       "                    getitem_164: \"f16[s77, 12, 577, 64]\" = unbind_2[2];  unbind_2 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:94 in forward, code: x = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_2: \"f16[s77, 12, 577, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_162, getitem_163, getitem_164);  getitem_162 = getitem_163 = getitem_164 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:107 in forward, code: x = x.transpose(1, 2).reshape(B, N, self.attn_dim)\n",
       "                    transpose_3: \"f16[s77, 577, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_2, 1, 2);  scaled_dot_product_attention_2 = None\n",
       "                    view_112: \"f16[s77, 577, 768]\" = torch.ops.aten.view.default(transpose_3, [sym_size_int_7, 577, 768]);  transpose_3 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_9: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(view_112, p_pretrained_model_blocks_2_attn_proj_weight, p_pretrained_model_blocks_2_attn_proj_bias);  view_112 = p_pretrained_model_blocks_2_attn_proj_weight = p_pretrained_model_blocks_2_attn_proj_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_8: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_9);  linear_9 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:205 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
       "                    add_1408: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1347, clone_8);  add_1347 = clone_8 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_5: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1408, [768], p_pretrained_model_blocks_2_norm2_weight, p_pretrained_model_blocks_2_norm2_bias, 1e-06);  p_pretrained_model_blocks_2_norm2_weight = p_pretrained_model_blocks_2_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_10: \"f16[s77, 577, 3072]\" = torch.ops.aten.linear.default(layer_norm_5, p_pretrained_model_blocks_2_mlp_fc1_weight, p_pretrained_model_blocks_2_mlp_fc1_bias);  layer_norm_5 = p_pretrained_model_blocks_2_mlp_fc1_weight = p_pretrained_model_blocks_2_mlp_fc1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:815 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_2: \"f16[s77, 577, 3072]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_9: \"f16[s77, 577, 3072]\" = torch.ops.aten.clone.default(gelu_2);  gelu_2 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_11: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(clone_9, p_pretrained_model_blocks_2_mlp_fc2_weight, p_pretrained_model_blocks_2_mlp_fc2_bias);  clone_9 = p_pretrained_model_blocks_2_mlp_fc2_weight = p_pretrained_model_blocks_2_mlp_fc2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_10: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_11);  linear_11 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:206 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_1437: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1408, clone_10);  add_1408 = clone_10 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_6: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1437, [768], p_pretrained_model_blocks_3_norm1_weight, p_pretrained_model_blocks_3_norm1_bias, 1e-06);  p_pretrained_model_blocks_3_norm1_weight = p_pretrained_model_blocks_3_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_12: \"f16[s77, 577, 2304]\" = torch.ops.aten.linear.default(layer_norm_6, p_pretrained_model_blocks_3_attn_qkv_weight, p_pretrained_model_blocks_3_attn_qkv_bias);  layer_norm_6 = p_pretrained_model_blocks_3_attn_qkv_weight = p_pretrained_model_blocks_3_attn_qkv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:89 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
       "                    view_113: \"f16[s77, 577, 3, 12, 64]\" = torch.ops.aten.view.default(linear_12, [sym_size_int_7, 577, 3, 12, 64]);  linear_12 = None\n",
       "                    permute_5: \"f16[3, s77, 12, 577, 64]\" = torch.ops.aten.permute.default(view_113, [2, 0, 3, 1, 4]);  view_113 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:90 in forward, code: q, k, v = qkv.unbind(0)\n",
       "                    unbind_3 = torch.ops.aten.unbind.int(permute_5);  permute_5 = None\n",
       "                    getitem_165: \"f16[s77, 12, 577, 64]\" = unbind_3[0]\n",
       "                    getitem_166: \"f16[s77, 12, 577, 64]\" = unbind_3[1]\n",
       "                    getitem_167: \"f16[s77, 12, 577, 64]\" = unbind_3[2];  unbind_3 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:94 in forward, code: x = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_3: \"f16[s77, 12, 577, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_165, getitem_166, getitem_167);  getitem_165 = getitem_166 = getitem_167 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:107 in forward, code: x = x.transpose(1, 2).reshape(B, N, self.attn_dim)\n",
       "                    transpose_4: \"f16[s77, 577, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_3, 1, 2);  scaled_dot_product_attention_3 = None\n",
       "                    view_114: \"f16[s77, 577, 768]\" = torch.ops.aten.view.default(transpose_4, [sym_size_int_7, 577, 768]);  transpose_4 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_13: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(view_114, p_pretrained_model_blocks_3_attn_proj_weight, p_pretrained_model_blocks_3_attn_proj_bias);  view_114 = p_pretrained_model_blocks_3_attn_proj_weight = p_pretrained_model_blocks_3_attn_proj_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_11: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_13);  linear_13 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:205 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
       "                    add_1498: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1437, clone_11);  add_1437 = clone_11 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_7: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1498, [768], p_pretrained_model_blocks_3_norm2_weight, p_pretrained_model_blocks_3_norm2_bias, 1e-06);  p_pretrained_model_blocks_3_norm2_weight = p_pretrained_model_blocks_3_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_14: \"f16[s77, 577, 3072]\" = torch.ops.aten.linear.default(layer_norm_7, p_pretrained_model_blocks_3_mlp_fc1_weight, p_pretrained_model_blocks_3_mlp_fc1_bias);  layer_norm_7 = p_pretrained_model_blocks_3_mlp_fc1_weight = p_pretrained_model_blocks_3_mlp_fc1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:815 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_3: \"f16[s77, 577, 3072]\" = torch.ops.aten.gelu.default(linear_14);  linear_14 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_12: \"f16[s77, 577, 3072]\" = torch.ops.aten.clone.default(gelu_3);  gelu_3 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_15: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(clone_12, p_pretrained_model_blocks_3_mlp_fc2_weight, p_pretrained_model_blocks_3_mlp_fc2_bias);  clone_12 = p_pretrained_model_blocks_3_mlp_fc2_weight = p_pretrained_model_blocks_3_mlp_fc2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_13: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_15);  linear_15 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:206 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_1527: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1498, clone_13);  add_1498 = clone_13 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_8: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1527, [768], p_pretrained_model_blocks_4_norm1_weight, p_pretrained_model_blocks_4_norm1_bias, 1e-06);  p_pretrained_model_blocks_4_norm1_weight = p_pretrained_model_blocks_4_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_16: \"f16[s77, 577, 2304]\" = torch.ops.aten.linear.default(layer_norm_8, p_pretrained_model_blocks_4_attn_qkv_weight, p_pretrained_model_blocks_4_attn_qkv_bias);  layer_norm_8 = p_pretrained_model_blocks_4_attn_qkv_weight = p_pretrained_model_blocks_4_attn_qkv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:89 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
       "                    view_115: \"f16[s77, 577, 3, 12, 64]\" = torch.ops.aten.view.default(linear_16, [sym_size_int_7, 577, 3, 12, 64]);  linear_16 = None\n",
       "                    permute_6: \"f16[3, s77, 12, 577, 64]\" = torch.ops.aten.permute.default(view_115, [2, 0, 3, 1, 4]);  view_115 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:90 in forward, code: q, k, v = qkv.unbind(0)\n",
       "                    unbind_4 = torch.ops.aten.unbind.int(permute_6);  permute_6 = None\n",
       "                    getitem_168: \"f16[s77, 12, 577, 64]\" = unbind_4[0]\n",
       "                    getitem_169: \"f16[s77, 12, 577, 64]\" = unbind_4[1]\n",
       "                    getitem_170: \"f16[s77, 12, 577, 64]\" = unbind_4[2];  unbind_4 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:94 in forward, code: x = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_4: \"f16[s77, 12, 577, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_168, getitem_169, getitem_170);  getitem_168 = getitem_169 = getitem_170 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:107 in forward, code: x = x.transpose(1, 2).reshape(B, N, self.attn_dim)\n",
       "                    transpose_5: \"f16[s77, 577, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_4, 1, 2);  scaled_dot_product_attention_4 = None\n",
       "                    view_116: \"f16[s77, 577, 768]\" = torch.ops.aten.view.default(transpose_5, [sym_size_int_7, 577, 768]);  transpose_5 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_17: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(view_116, p_pretrained_model_blocks_4_attn_proj_weight, p_pretrained_model_blocks_4_attn_proj_bias);  view_116 = p_pretrained_model_blocks_4_attn_proj_weight = p_pretrained_model_blocks_4_attn_proj_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_14: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_17);  linear_17 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:205 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
       "                    add_1588: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1527, clone_14);  add_1527 = clone_14 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_9: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1588, [768], p_pretrained_model_blocks_4_norm2_weight, p_pretrained_model_blocks_4_norm2_bias, 1e-06);  p_pretrained_model_blocks_4_norm2_weight = p_pretrained_model_blocks_4_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_18: \"f16[s77, 577, 3072]\" = torch.ops.aten.linear.default(layer_norm_9, p_pretrained_model_blocks_4_mlp_fc1_weight, p_pretrained_model_blocks_4_mlp_fc1_bias);  layer_norm_9 = p_pretrained_model_blocks_4_mlp_fc1_weight = p_pretrained_model_blocks_4_mlp_fc1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:815 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_4: \"f16[s77, 577, 3072]\" = torch.ops.aten.gelu.default(linear_18);  linear_18 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_15: \"f16[s77, 577, 3072]\" = torch.ops.aten.clone.default(gelu_4);  gelu_4 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_19: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(clone_15, p_pretrained_model_blocks_4_mlp_fc2_weight, p_pretrained_model_blocks_4_mlp_fc2_bias);  clone_15 = p_pretrained_model_blocks_4_mlp_fc2_weight = p_pretrained_model_blocks_4_mlp_fc2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_16: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_19);  linear_19 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:206 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_1617: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1588, clone_16);  add_1588 = clone_16 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_10: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1617, [768], p_pretrained_model_blocks_5_norm1_weight, p_pretrained_model_blocks_5_norm1_bias, 1e-06);  p_pretrained_model_blocks_5_norm1_weight = p_pretrained_model_blocks_5_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_20: \"f16[s77, 577, 2304]\" = torch.ops.aten.linear.default(layer_norm_10, p_pretrained_model_blocks_5_attn_qkv_weight, p_pretrained_model_blocks_5_attn_qkv_bias);  layer_norm_10 = p_pretrained_model_blocks_5_attn_qkv_weight = p_pretrained_model_blocks_5_attn_qkv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:89 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
       "                    view_117: \"f16[s77, 577, 3, 12, 64]\" = torch.ops.aten.view.default(linear_20, [sym_size_int_7, 577, 3, 12, 64]);  linear_20 = None\n",
       "                    permute_7: \"f16[3, s77, 12, 577, 64]\" = torch.ops.aten.permute.default(view_117, [2, 0, 3, 1, 4]);  view_117 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:90 in forward, code: q, k, v = qkv.unbind(0)\n",
       "                    unbind_5 = torch.ops.aten.unbind.int(permute_7);  permute_7 = None\n",
       "                    getitem_171: \"f16[s77, 12, 577, 64]\" = unbind_5[0]\n",
       "                    getitem_172: \"f16[s77, 12, 577, 64]\" = unbind_5[1]\n",
       "                    getitem_173: \"f16[s77, 12, 577, 64]\" = unbind_5[2];  unbind_5 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:94 in forward, code: x = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_5: \"f16[s77, 12, 577, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_171, getitem_172, getitem_173);  getitem_171 = getitem_172 = getitem_173 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:107 in forward, code: x = x.transpose(1, 2).reshape(B, N, self.attn_dim)\n",
       "                    transpose_6: \"f16[s77, 577, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_5, 1, 2);  scaled_dot_product_attention_5 = None\n",
       "                    view_118: \"f16[s77, 577, 768]\" = torch.ops.aten.view.default(transpose_6, [sym_size_int_7, 577, 768]);  transpose_6 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_21: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(view_118, p_pretrained_model_blocks_5_attn_proj_weight, p_pretrained_model_blocks_5_attn_proj_bias);  view_118 = p_pretrained_model_blocks_5_attn_proj_weight = p_pretrained_model_blocks_5_attn_proj_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_17: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_21);  linear_21 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:205 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
       "                    add_1678: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1617, clone_17);  add_1617 = clone_17 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_11: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1678, [768], p_pretrained_model_blocks_5_norm2_weight, p_pretrained_model_blocks_5_norm2_bias, 1e-06);  p_pretrained_model_blocks_5_norm2_weight = p_pretrained_model_blocks_5_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_22: \"f16[s77, 577, 3072]\" = torch.ops.aten.linear.default(layer_norm_11, p_pretrained_model_blocks_5_mlp_fc1_weight, p_pretrained_model_blocks_5_mlp_fc1_bias);  layer_norm_11 = p_pretrained_model_blocks_5_mlp_fc1_weight = p_pretrained_model_blocks_5_mlp_fc1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:815 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_5: \"f16[s77, 577, 3072]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_18: \"f16[s77, 577, 3072]\" = torch.ops.aten.clone.default(gelu_5);  gelu_5 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_23: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(clone_18, p_pretrained_model_blocks_5_mlp_fc2_weight, p_pretrained_model_blocks_5_mlp_fc2_bias);  clone_18 = p_pretrained_model_blocks_5_mlp_fc2_weight = p_pretrained_model_blocks_5_mlp_fc2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_19: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_23);  linear_23 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:206 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_1707: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1678, clone_19);  add_1678 = clone_19 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_12: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1707, [768], p_pretrained_model_blocks_6_norm1_weight, p_pretrained_model_blocks_6_norm1_bias, 1e-06);  p_pretrained_model_blocks_6_norm1_weight = p_pretrained_model_blocks_6_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_24: \"f16[s77, 577, 2304]\" = torch.ops.aten.linear.default(layer_norm_12, p_pretrained_model_blocks_6_attn_qkv_weight, p_pretrained_model_blocks_6_attn_qkv_bias);  layer_norm_12 = p_pretrained_model_blocks_6_attn_qkv_weight = p_pretrained_model_blocks_6_attn_qkv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:89 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
       "                    view_119: \"f16[s77, 577, 3, 12, 64]\" = torch.ops.aten.view.default(linear_24, [sym_size_int_7, 577, 3, 12, 64]);  linear_24 = None\n",
       "                    permute_8: \"f16[3, s77, 12, 577, 64]\" = torch.ops.aten.permute.default(view_119, [2, 0, 3, 1, 4]);  view_119 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:90 in forward, code: q, k, v = qkv.unbind(0)\n",
       "                    unbind_6 = torch.ops.aten.unbind.int(permute_8);  permute_8 = None\n",
       "                    getitem_174: \"f16[s77, 12, 577, 64]\" = unbind_6[0]\n",
       "                    getitem_175: \"f16[s77, 12, 577, 64]\" = unbind_6[1]\n",
       "                    getitem_176: \"f16[s77, 12, 577, 64]\" = unbind_6[2];  unbind_6 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:94 in forward, code: x = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_6: \"f16[s77, 12, 577, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_174, getitem_175, getitem_176);  getitem_174 = getitem_175 = getitem_176 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:107 in forward, code: x = x.transpose(1, 2).reshape(B, N, self.attn_dim)\n",
       "                    transpose_7: \"f16[s77, 577, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_6, 1, 2);  scaled_dot_product_attention_6 = None\n",
       "                    view_120: \"f16[s77, 577, 768]\" = torch.ops.aten.view.default(transpose_7, [sym_size_int_7, 577, 768]);  transpose_7 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_25: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(view_120, p_pretrained_model_blocks_6_attn_proj_weight, p_pretrained_model_blocks_6_attn_proj_bias);  view_120 = p_pretrained_model_blocks_6_attn_proj_weight = p_pretrained_model_blocks_6_attn_proj_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_20: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_25);  linear_25 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:205 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
       "                    add_1768: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1707, clone_20);  add_1707 = clone_20 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_13: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1768, [768], p_pretrained_model_blocks_6_norm2_weight, p_pretrained_model_blocks_6_norm2_bias, 1e-06);  p_pretrained_model_blocks_6_norm2_weight = p_pretrained_model_blocks_6_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_26: \"f16[s77, 577, 3072]\" = torch.ops.aten.linear.default(layer_norm_13, p_pretrained_model_blocks_6_mlp_fc1_weight, p_pretrained_model_blocks_6_mlp_fc1_bias);  layer_norm_13 = p_pretrained_model_blocks_6_mlp_fc1_weight = p_pretrained_model_blocks_6_mlp_fc1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:815 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_6: \"f16[s77, 577, 3072]\" = torch.ops.aten.gelu.default(linear_26);  linear_26 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_21: \"f16[s77, 577, 3072]\" = torch.ops.aten.clone.default(gelu_6);  gelu_6 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_27: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(clone_21, p_pretrained_model_blocks_6_mlp_fc2_weight, p_pretrained_model_blocks_6_mlp_fc2_bias);  clone_21 = p_pretrained_model_blocks_6_mlp_fc2_weight = p_pretrained_model_blocks_6_mlp_fc2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_22: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_27);  linear_27 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:206 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_1797: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1768, clone_22);  add_1768 = clone_22 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_14: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1797, [768], p_pretrained_model_blocks_7_norm1_weight, p_pretrained_model_blocks_7_norm1_bias, 1e-06);  p_pretrained_model_blocks_7_norm1_weight = p_pretrained_model_blocks_7_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_28: \"f16[s77, 577, 2304]\" = torch.ops.aten.linear.default(layer_norm_14, p_pretrained_model_blocks_7_attn_qkv_weight, p_pretrained_model_blocks_7_attn_qkv_bias);  layer_norm_14 = p_pretrained_model_blocks_7_attn_qkv_weight = p_pretrained_model_blocks_7_attn_qkv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:89 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
       "                    view_121: \"f16[s77, 577, 3, 12, 64]\" = torch.ops.aten.view.default(linear_28, [sym_size_int_7, 577, 3, 12, 64]);  linear_28 = None\n",
       "                    permute_9: \"f16[3, s77, 12, 577, 64]\" = torch.ops.aten.permute.default(view_121, [2, 0, 3, 1, 4]);  view_121 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:90 in forward, code: q, k, v = qkv.unbind(0)\n",
       "                    unbind_7 = torch.ops.aten.unbind.int(permute_9);  permute_9 = None\n",
       "                    getitem_177: \"f16[s77, 12, 577, 64]\" = unbind_7[0]\n",
       "                    getitem_178: \"f16[s77, 12, 577, 64]\" = unbind_7[1]\n",
       "                    getitem_179: \"f16[s77, 12, 577, 64]\" = unbind_7[2];  unbind_7 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:94 in forward, code: x = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_7: \"f16[s77, 12, 577, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_177, getitem_178, getitem_179);  getitem_177 = getitem_178 = getitem_179 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:107 in forward, code: x = x.transpose(1, 2).reshape(B, N, self.attn_dim)\n",
       "                    transpose_8: \"f16[s77, 577, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_7, 1, 2);  scaled_dot_product_attention_7 = None\n",
       "                    view_122: \"f16[s77, 577, 768]\" = torch.ops.aten.view.default(transpose_8, [sym_size_int_7, 577, 768]);  transpose_8 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_29: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(view_122, p_pretrained_model_blocks_7_attn_proj_weight, p_pretrained_model_blocks_7_attn_proj_bias);  view_122 = p_pretrained_model_blocks_7_attn_proj_weight = p_pretrained_model_blocks_7_attn_proj_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_23: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_29);  linear_29 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:205 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
       "                    add_1858: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1797, clone_23);  add_1797 = clone_23 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_15: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1858, [768], p_pretrained_model_blocks_7_norm2_weight, p_pretrained_model_blocks_7_norm2_bias, 1e-06);  p_pretrained_model_blocks_7_norm2_weight = p_pretrained_model_blocks_7_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_30: \"f16[s77, 577, 3072]\" = torch.ops.aten.linear.default(layer_norm_15, p_pretrained_model_blocks_7_mlp_fc1_weight, p_pretrained_model_blocks_7_mlp_fc1_bias);  layer_norm_15 = p_pretrained_model_blocks_7_mlp_fc1_weight = p_pretrained_model_blocks_7_mlp_fc1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:815 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_7: \"f16[s77, 577, 3072]\" = torch.ops.aten.gelu.default(linear_30);  linear_30 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_24: \"f16[s77, 577, 3072]\" = torch.ops.aten.clone.default(gelu_7);  gelu_7 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_31: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(clone_24, p_pretrained_model_blocks_7_mlp_fc2_weight, p_pretrained_model_blocks_7_mlp_fc2_bias);  clone_24 = p_pretrained_model_blocks_7_mlp_fc2_weight = p_pretrained_model_blocks_7_mlp_fc2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_25: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_31);  linear_31 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:206 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_1887: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1858, clone_25);  add_1858 = clone_25 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_16: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1887, [768], p_pretrained_model_blocks_8_norm1_weight, p_pretrained_model_blocks_8_norm1_bias, 1e-06);  p_pretrained_model_blocks_8_norm1_weight = p_pretrained_model_blocks_8_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_32: \"f16[s77, 577, 2304]\" = torch.ops.aten.linear.default(layer_norm_16, p_pretrained_model_blocks_8_attn_qkv_weight, p_pretrained_model_blocks_8_attn_qkv_bias);  layer_norm_16 = p_pretrained_model_blocks_8_attn_qkv_weight = p_pretrained_model_blocks_8_attn_qkv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:89 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
       "                    view_123: \"f16[s77, 577, 3, 12, 64]\" = torch.ops.aten.view.default(linear_32, [sym_size_int_7, 577, 3, 12, 64]);  linear_32 = None\n",
       "                    permute_10: \"f16[3, s77, 12, 577, 64]\" = torch.ops.aten.permute.default(view_123, [2, 0, 3, 1, 4]);  view_123 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:90 in forward, code: q, k, v = qkv.unbind(0)\n",
       "                    unbind_8 = torch.ops.aten.unbind.int(permute_10);  permute_10 = None\n",
       "                    getitem_180: \"f16[s77, 12, 577, 64]\" = unbind_8[0]\n",
       "                    getitem_181: \"f16[s77, 12, 577, 64]\" = unbind_8[1]\n",
       "                    getitem_182: \"f16[s77, 12, 577, 64]\" = unbind_8[2];  unbind_8 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:94 in forward, code: x = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_8: \"f16[s77, 12, 577, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_180, getitem_181, getitem_182);  getitem_180 = getitem_181 = getitem_182 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:107 in forward, code: x = x.transpose(1, 2).reshape(B, N, self.attn_dim)\n",
       "                    transpose_9: \"f16[s77, 577, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_8, 1, 2);  scaled_dot_product_attention_8 = None\n",
       "                    view_124: \"f16[s77, 577, 768]\" = torch.ops.aten.view.default(transpose_9, [sym_size_int_7, 577, 768]);  transpose_9 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_33: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(view_124, p_pretrained_model_blocks_8_attn_proj_weight, p_pretrained_model_blocks_8_attn_proj_bias);  view_124 = p_pretrained_model_blocks_8_attn_proj_weight = p_pretrained_model_blocks_8_attn_proj_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_26: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_33);  linear_33 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:205 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
       "                    add_1948: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1887, clone_26);  add_1887 = clone_26 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_17: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1948, [768], p_pretrained_model_blocks_8_norm2_weight, p_pretrained_model_blocks_8_norm2_bias, 1e-06);  p_pretrained_model_blocks_8_norm2_weight = p_pretrained_model_blocks_8_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_34: \"f16[s77, 577, 3072]\" = torch.ops.aten.linear.default(layer_norm_17, p_pretrained_model_blocks_8_mlp_fc1_weight, p_pretrained_model_blocks_8_mlp_fc1_bias);  layer_norm_17 = p_pretrained_model_blocks_8_mlp_fc1_weight = p_pretrained_model_blocks_8_mlp_fc1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:815 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_8: \"f16[s77, 577, 3072]\" = torch.ops.aten.gelu.default(linear_34);  linear_34 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_27: \"f16[s77, 577, 3072]\" = torch.ops.aten.clone.default(gelu_8);  gelu_8 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_35: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(clone_27, p_pretrained_model_blocks_8_mlp_fc2_weight, p_pretrained_model_blocks_8_mlp_fc2_bias);  clone_27 = p_pretrained_model_blocks_8_mlp_fc2_weight = p_pretrained_model_blocks_8_mlp_fc2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_28: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_35);  linear_35 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:206 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_1977: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1948, clone_28);  add_1948 = clone_28 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_18: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_1977, [768], p_pretrained_model_blocks_9_norm1_weight, p_pretrained_model_blocks_9_norm1_bias, 1e-06);  p_pretrained_model_blocks_9_norm1_weight = p_pretrained_model_blocks_9_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_36: \"f16[s77, 577, 2304]\" = torch.ops.aten.linear.default(layer_norm_18, p_pretrained_model_blocks_9_attn_qkv_weight, p_pretrained_model_blocks_9_attn_qkv_bias);  layer_norm_18 = p_pretrained_model_blocks_9_attn_qkv_weight = p_pretrained_model_blocks_9_attn_qkv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:89 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
       "                    view_125: \"f16[s77, 577, 3, 12, 64]\" = torch.ops.aten.view.default(linear_36, [sym_size_int_7, 577, 3, 12, 64]);  linear_36 = None\n",
       "                    permute_11: \"f16[3, s77, 12, 577, 64]\" = torch.ops.aten.permute.default(view_125, [2, 0, 3, 1, 4]);  view_125 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:90 in forward, code: q, k, v = qkv.unbind(0)\n",
       "                    unbind_9 = torch.ops.aten.unbind.int(permute_11);  permute_11 = None\n",
       "                    getitem_183: \"f16[s77, 12, 577, 64]\" = unbind_9[0]\n",
       "                    getitem_184: \"f16[s77, 12, 577, 64]\" = unbind_9[1]\n",
       "                    getitem_185: \"f16[s77, 12, 577, 64]\" = unbind_9[2];  unbind_9 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:94 in forward, code: x = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_9: \"f16[s77, 12, 577, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_183, getitem_184, getitem_185);  getitem_183 = getitem_184 = getitem_185 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:107 in forward, code: x = x.transpose(1, 2).reshape(B, N, self.attn_dim)\n",
       "                    transpose_10: \"f16[s77, 577, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_9, 1, 2);  scaled_dot_product_attention_9 = None\n",
       "                    view_126: \"f16[s77, 577, 768]\" = torch.ops.aten.view.default(transpose_10, [sym_size_int_7, 577, 768]);  transpose_10 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_37: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(view_126, p_pretrained_model_blocks_9_attn_proj_weight, p_pretrained_model_blocks_9_attn_proj_bias);  view_126 = p_pretrained_model_blocks_9_attn_proj_weight = p_pretrained_model_blocks_9_attn_proj_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_29: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_37);  linear_37 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:205 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
       "                    add_2038: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_1977, clone_29);  clone_29 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_19: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_2038, [768], p_pretrained_model_blocks_9_norm2_weight, p_pretrained_model_blocks_9_norm2_bias, 1e-06);  p_pretrained_model_blocks_9_norm2_weight = p_pretrained_model_blocks_9_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_38: \"f16[s77, 577, 3072]\" = torch.ops.aten.linear.default(layer_norm_19, p_pretrained_model_blocks_9_mlp_fc1_weight, p_pretrained_model_blocks_9_mlp_fc1_bias);  layer_norm_19 = p_pretrained_model_blocks_9_mlp_fc1_weight = p_pretrained_model_blocks_9_mlp_fc1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:815 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_9: \"f16[s77, 577, 3072]\" = torch.ops.aten.gelu.default(linear_38);  linear_38 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_30: \"f16[s77, 577, 3072]\" = torch.ops.aten.clone.default(gelu_9);  gelu_9 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_39: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(clone_30, p_pretrained_model_blocks_9_mlp_fc2_weight, p_pretrained_model_blocks_9_mlp_fc2_bias);  clone_30 = p_pretrained_model_blocks_9_mlp_fc2_weight = p_pretrained_model_blocks_9_mlp_fc2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_31: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_39);  linear_39 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:206 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_2067: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_2038, clone_31);  add_2038 = clone_31 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_20: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_2067, [768], p_pretrained_model_blocks_10_norm1_weight, p_pretrained_model_blocks_10_norm1_bias, 1e-06);  p_pretrained_model_blocks_10_norm1_weight = p_pretrained_model_blocks_10_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_40: \"f16[s77, 577, 2304]\" = torch.ops.aten.linear.default(layer_norm_20, p_pretrained_model_blocks_10_attn_qkv_weight, p_pretrained_model_blocks_10_attn_qkv_bias);  layer_norm_20 = p_pretrained_model_blocks_10_attn_qkv_weight = p_pretrained_model_blocks_10_attn_qkv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:89 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
       "                    view_127: \"f16[s77, 577, 3, 12, 64]\" = torch.ops.aten.view.default(linear_40, [sym_size_int_7, 577, 3, 12, 64]);  linear_40 = None\n",
       "                    permute_12: \"f16[3, s77, 12, 577, 64]\" = torch.ops.aten.permute.default(view_127, [2, 0, 3, 1, 4]);  view_127 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:90 in forward, code: q, k, v = qkv.unbind(0)\n",
       "                    unbind_10 = torch.ops.aten.unbind.int(permute_12);  permute_12 = None\n",
       "                    getitem_186: \"f16[s77, 12, 577, 64]\" = unbind_10[0]\n",
       "                    getitem_187: \"f16[s77, 12, 577, 64]\" = unbind_10[1]\n",
       "                    getitem_188: \"f16[s77, 12, 577, 64]\" = unbind_10[2];  unbind_10 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:94 in forward, code: x = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_10: \"f16[s77, 12, 577, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_186, getitem_187, getitem_188);  getitem_186 = getitem_187 = getitem_188 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:107 in forward, code: x = x.transpose(1, 2).reshape(B, N, self.attn_dim)\n",
       "                    transpose_11: \"f16[s77, 577, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_10, 1, 2);  scaled_dot_product_attention_10 = None\n",
       "                    view_128: \"f16[s77, 577, 768]\" = torch.ops.aten.view.default(transpose_11, [sym_size_int_7, 577, 768]);  transpose_11 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_41: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(view_128, p_pretrained_model_blocks_10_attn_proj_weight, p_pretrained_model_blocks_10_attn_proj_bias);  view_128 = p_pretrained_model_blocks_10_attn_proj_weight = p_pretrained_model_blocks_10_attn_proj_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_32: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_41);  linear_41 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:205 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
       "                    add_2128: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_2067, clone_32);  add_2067 = clone_32 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_21: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_2128, [768], p_pretrained_model_blocks_10_norm2_weight, p_pretrained_model_blocks_10_norm2_bias, 1e-06);  p_pretrained_model_blocks_10_norm2_weight = p_pretrained_model_blocks_10_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_42: \"f16[s77, 577, 3072]\" = torch.ops.aten.linear.default(layer_norm_21, p_pretrained_model_blocks_10_mlp_fc1_weight, p_pretrained_model_blocks_10_mlp_fc1_bias);  layer_norm_21 = p_pretrained_model_blocks_10_mlp_fc1_weight = p_pretrained_model_blocks_10_mlp_fc1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:815 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_10: \"f16[s77, 577, 3072]\" = torch.ops.aten.gelu.default(linear_42);  linear_42 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_33: \"f16[s77, 577, 3072]\" = torch.ops.aten.clone.default(gelu_10);  gelu_10 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_43: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(clone_33, p_pretrained_model_blocks_10_mlp_fc2_weight, p_pretrained_model_blocks_10_mlp_fc2_bias);  clone_33 = p_pretrained_model_blocks_10_mlp_fc2_weight = p_pretrained_model_blocks_10_mlp_fc2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_34: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_43);  linear_43 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:206 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_2157: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_2128, clone_34);  add_2128 = clone_34 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_22: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_2157, [768], p_pretrained_model_blocks_11_norm1_weight, p_pretrained_model_blocks_11_norm1_bias, 1e-06);  p_pretrained_model_blocks_11_norm1_weight = p_pretrained_model_blocks_11_norm1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_44: \"f16[s77, 577, 2304]\" = torch.ops.aten.linear.default(layer_norm_22, p_pretrained_model_blocks_11_attn_qkv_weight, p_pretrained_model_blocks_11_attn_qkv_bias);  layer_norm_22 = p_pretrained_model_blocks_11_attn_qkv_weight = p_pretrained_model_blocks_11_attn_qkv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:89 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
       "                    view_129: \"f16[s77, 577, 3, 12, 64]\" = torch.ops.aten.view.default(linear_44, [sym_size_int_7, 577, 3, 12, 64]);  linear_44 = None\n",
       "                    permute_13: \"f16[3, s77, 12, 577, 64]\" = torch.ops.aten.permute.default(view_129, [2, 0, 3, 1, 4]);  view_129 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:90 in forward, code: q, k, v = qkv.unbind(0)\n",
       "                    unbind_11 = torch.ops.aten.unbind.int(permute_13);  permute_13 = None\n",
       "                    getitem_189: \"f16[s77, 12, 577, 64]\" = unbind_11[0]\n",
       "                    getitem_190: \"f16[s77, 12, 577, 64]\" = unbind_11[1]\n",
       "                    getitem_191: \"f16[s77, 12, 577, 64]\" = unbind_11[2];  unbind_11 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:94 in forward, code: x = F.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_11: \"f16[s77, 12, 577, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_189, getitem_190, getitem_191);  getitem_189 = getitem_190 = getitem_191 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/attention.py:107 in forward, code: x = x.transpose(1, 2).reshape(B, N, self.attn_dim)\n",
       "                    transpose_12: \"f16[s77, 577, 12, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_11, 1, 2);  scaled_dot_product_attention_11 = None\n",
       "                    view_130: \"f16[s77, 577, 768]\" = torch.ops.aten.view.default(transpose_12, [sym_size_int_7, 577, 768]);  transpose_12 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_45: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(view_130, p_pretrained_model_blocks_11_attn_proj_weight, p_pretrained_model_blocks_11_attn_proj_bias);  view_130 = p_pretrained_model_blocks_11_attn_proj_weight = p_pretrained_model_blocks_11_attn_proj_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_35: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_45);  linear_45 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:205 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
       "                    add_2218: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_2157, clone_35);  add_2157 = clone_35 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_23: \"f16[s77, 577, 768]\" = torch.ops.aten.layer_norm.default(add_2218, [768], p_pretrained_model_blocks_11_norm2_weight, p_pretrained_model_blocks_11_norm2_bias, 1e-06);  p_pretrained_model_blocks_11_norm2_weight = p_pretrained_model_blocks_11_norm2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_46: \"f16[s77, 577, 3072]\" = torch.ops.aten.linear.default(layer_norm_23, p_pretrained_model_blocks_11_mlp_fc1_weight, p_pretrained_model_blocks_11_mlp_fc1_bias);  layer_norm_23 = p_pretrained_model_blocks_11_mlp_fc1_weight = p_pretrained_model_blocks_11_mlp_fc1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:815 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_11: \"f16[s77, 577, 3072]\" = torch.ops.aten.gelu.default(linear_46);  linear_46 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_36: \"f16[s77, 577, 3072]\" = torch.ops.aten.clone.default(gelu_11);  gelu_11 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_47: \"f16[s77, 577, 768]\" = torch.ops.aten.linear.default(clone_36, p_pretrained_model_blocks_11_mlp_fc2_weight, p_pretrained_model_blocks_11_mlp_fc2_bias);  clone_36 = p_pretrained_model_blocks_11_mlp_fc2_weight = p_pretrained_model_blocks_11_mlp_fc2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_37: \"f16[s77, 577, 768]\" = torch.ops.aten.clone.default(linear_47);  linear_47 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/timm/models/vision_transformer.py:206 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_2247: \"f16[s77, 577, 768]\" = torch.ops.aten.add.Tensor(add_2218, clone_37);  add_2218 = clone_37 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/backbones/utils.py:36 in forward, code: readout = x[:, 0].unsqueeze(1).expand_as(x[:, self.start_index:])\n",
       "                    slice_3: \"f16[s77, 577, 768]\" = torch.ops.aten.slice.Tensor(add_1977, 0, 0, 9223372036854775807)\n",
       "                    select_1: \"f16[s77, 768]\" = torch.ops.aten.select.int(slice_3, 1, 0);  slice_3 = None\n",
       "                    unsqueeze: \"f16[s77, 1, 768]\" = torch.ops.aten.unsqueeze.default(select_1, 1);  select_1 = None\n",
       "                    slice_4: \"f16[s77, 577, 768]\" = torch.ops.aten.slice.Tensor(add_1977, 0, 0, 9223372036854775807);  slice_4 = None\n",
       "                    expand_1: \"f16[s77, 576, 768]\" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_7, 576, 768]);  unsqueeze = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/backbones/utils.py:37 in forward, code: features = torch.cat((x[:, self.start_index:], readout), -1)\n",
       "                    slice_6: \"f16[s77, 577, 768]\" = torch.ops.aten.slice.Tensor(add_1977, 0, 0, 9223372036854775807);  add_1977 = None\n",
       "                    slice_7: \"f16[s77, 576, 768]\" = torch.ops.aten.slice.Tensor(slice_6, 1, 1, 9223372036854775807);  slice_6 = None\n",
       "                    cat_2: \"f16[s77, 576, 1536]\" = torch.ops.aten.cat.default([slice_7, expand_1], -1);  slice_7 = expand_1 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_48: \"f16[s77, 576, 768]\" = torch.ops.aten.linear.default(cat_2, p_pretrained_act_postprocess3_0_project_0_weight, p_pretrained_act_postprocess3_0_project_0_bias);  cat_2 = p_pretrained_act_postprocess3_0_project_0_weight = p_pretrained_act_postprocess3_0_project_0_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:815 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_12: \"f16[s77, 576, 768]\" = torch.ops.aten.gelu.default(linear_48);  linear_48 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/backbones/utils.py:49 in forward, code: x = x.transpose(self.dim0, self.dim1)\n",
       "                    transpose_13: \"f16[s77, 768, 576]\" = torch.ops.aten.transpose.int(gelu_12, 1, 2);  gelu_12 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/backbones/utils.py:36 in forward, code: readout = x[:, 0].unsqueeze(1).expand_as(x[:, self.start_index:])\n",
       "                    slice_8: \"f16[s77, 577, 768]\" = torch.ops.aten.slice.Tensor(add_2247, 0, 0, 9223372036854775807)\n",
       "                    select_2: \"f16[s77, 768]\" = torch.ops.aten.select.int(slice_8, 1, 0);  slice_8 = None\n",
       "                    unsqueeze_1: \"f16[s77, 1, 768]\" = torch.ops.aten.unsqueeze.default(select_2, 1);  select_2 = None\n",
       "                    slice_9: \"f16[s77, 577, 768]\" = torch.ops.aten.slice.Tensor(add_2247, 0, 0, 9223372036854775807);  slice_9 = None\n",
       "                    expand_2: \"f16[s77, 576, 768]\" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_7, 576, 768]);  unsqueeze_1 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/backbones/utils.py:37 in forward, code: features = torch.cat((x[:, self.start_index:], readout), -1)\n",
       "                    slice_11: \"f16[s77, 577, 768]\" = torch.ops.aten.slice.Tensor(add_2247, 0, 0, 9223372036854775807);  add_2247 = None\n",
       "                    slice_12: \"f16[s77, 576, 768]\" = torch.ops.aten.slice.Tensor(slice_11, 1, 1, 9223372036854775807);  slice_11 = None\n",
       "                    cat_3: \"f16[s77, 576, 1536]\" = torch.ops.aten.cat.default([slice_12, expand_2], -1);  slice_12 = expand_2 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_49: \"f16[s77, 576, 768]\" = torch.ops.aten.linear.default(cat_3, p_pretrained_act_postprocess4_0_project_0_weight, p_pretrained_act_postprocess4_0_project_0_bias);  cat_3 = p_pretrained_act_postprocess4_0_project_0_weight = p_pretrained_act_postprocess4_0_project_0_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:815 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_13: \"f16[s77, 576, 768]\" = torch.ops.aten.gelu.default(linear_49);  linear_49 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/backbones/utils.py:49 in forward, code: x = x.transpose(self.dim0, self.dim1)\n",
       "                    transpose_14: \"f16[s77, 768, 576]\" = torch.ops.aten.transpose.int(gelu_13, 1, 2);  gelu_13 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/flatten.py:161 in forward, code: return input.unflatten(self.dim, self.unflattened_size)\n",
       "                    view_131: \"f16[s77, 768, 24, 24]\" = torch.ops.aten.view.default(transpose_13, [sym_size_int_7, 768, 24, 24]);  transpose_13 = None\n",
       "                    view_132: \"f16[s77, 768, 24, 24]\" = torch.ops.aten.view.default(transpose_14, [sym_size_int_7, 768, 24, 24]);  transpose_14 = sym_size_int_7 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_53: \"f16[s77, 768, 24, 24]\" = torch.ops.aten.conv2d.default(view_131, p_pretrained_act_postprocess3_3_weight, p_pretrained_act_postprocess3_3_bias);  view_131 = p_pretrained_act_postprocess3_3_weight = p_pretrained_act_postprocess3_3_bias = None\n",
       "                    conv2d_54: \"f16[s77, 768, 24, 24]\" = torch.ops.aten.conv2d.default(view_132, p_pretrained_act_postprocess4_3_weight, p_pretrained_act_postprocess4_3_bias);  view_132 = p_pretrained_act_postprocess4_3_weight = p_pretrained_act_postprocess4_3_bias = None\n",
       "                    conv2d_55: \"f16[s77, 768, 12, 12]\" = torch.ops.aten.conv2d.default(conv2d_54, p_pretrained_act_postprocess4_4_weight, p_pretrained_act_postprocess4_4_bias, [2, 2], [1, 1]);  conv2d_54 = p_pretrained_act_postprocess4_4_weight = p_pretrained_act_postprocess4_4_bias = None\n",
       "                    conv2d_56: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.conv2d.default(relu_9, p_scratch_layer1_rn_weight, None, [1, 1], [1, 1]);  relu_9 = p_scratch_layer1_rn_weight = None\n",
       "                    conv2d_57: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.conv2d.default(relu_21, p_scratch_layer2_rn_weight, None, [1, 1], [1, 1]);  relu_21 = p_scratch_layer2_rn_weight = None\n",
       "                    conv2d_58: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(conv2d_53, p_scratch_layer3_rn_weight, None, [1, 1], [1, 1]);  conv2d_53 = p_scratch_layer3_rn_weight = None\n",
       "                    conv2d_59: \"f16[s77, 256, 12, 12]\" = torch.ops.aten.conv2d.default(conv2d_55, p_scratch_layer4_rn_weight, None, [1, 1], [1, 1]);  conv2d_55 = p_scratch_layer4_rn_weight = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_49: \"f16[s77, 256, 12, 12]\" = torch.ops.aten.relu.default(conv2d_59)\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_60: \"f16[s77, 256, 12, 12]\" = torch.ops.aten.conv2d.default(relu_49, p_scratch_refinenet4_resconfunit2_conv1_weight, p_scratch_refinenet4_resconfunit2_conv1_bias, [1, 1], [1, 1]);  relu_49 = p_scratch_refinenet4_resconfunit2_conv1_weight = p_scratch_refinenet4_resconfunit2_conv1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_50: \"f16[s77, 256, 12, 12]\" = torch.ops.aten.relu.default(conv2d_60);  conv2d_60 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_61: \"f16[s77, 256, 12, 12]\" = torch.ops.aten.conv2d.default(relu_50, p_scratch_refinenet4_resconfunit2_conv2_weight, p_scratch_refinenet4_resconfunit2_conv2_bias, [1, 1], [1, 1]);  relu_50 = p_scratch_refinenet4_resconfunit2_conv2_weight = p_scratch_refinenet4_resconfunit2_conv2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:374 in forward, code: return self.skip_add.add(out, x)\n",
       "                    add_2415: \"f16[s77, 256, 12, 12]\" = torch.ops.aten.add.Tensor(conv2d_61, conv2d_59);  conv2d_61 = conv2d_59 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:432 in forward, code: output = nn.functional.interpolate(\n",
       "                    upsample_bilinear2d_1: \"f16[1, 256, 24, 24]\" = torch.ops.aten.upsample_bilinear2d.vec(add_2415, [24, 24], True, None);  add_2415 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_62: \"f16[1, 256, 24, 24]\" = torch.ops.aten.conv2d.default(upsample_bilinear2d_1, p_scratch_refinenet4_out_conv_weight, p_scratch_refinenet4_out_conv_bias);  upsample_bilinear2d_1 = p_scratch_refinenet4_out_conv_weight = p_scratch_refinenet4_out_conv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_51: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(conv2d_58)\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_63: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_51, p_scratch_refinenet3_resconfunit1_conv1_weight, p_scratch_refinenet3_resconfunit1_conv1_bias, [1, 1], [1, 1]);  relu_51 = p_scratch_refinenet3_resconfunit1_conv1_weight = p_scratch_refinenet3_resconfunit1_conv1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_52: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(conv2d_63);  conv2d_63 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_64: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_52, p_scratch_refinenet3_resconfunit1_conv2_weight, p_scratch_refinenet3_resconfunit1_conv2_bias, [1, 1], [1, 1]);  relu_52 = p_scratch_refinenet3_resconfunit1_conv2_weight = p_scratch_refinenet3_resconfunit1_conv2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:374 in forward, code: return self.skip_add.add(out, x)\n",
       "                    add_2441: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.add.Tensor(conv2d_64, conv2d_58);  conv2d_64 = conv2d_58 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:420 in forward, code: output = self.skip_add.add(output, res)\n",
       "                    add_2447: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.add.Tensor(conv2d_62, add_2441);  conv2d_62 = add_2441 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_53: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(add_2447)\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_65: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_53, p_scratch_refinenet3_resconfunit2_conv1_weight, p_scratch_refinenet3_resconfunit2_conv1_bias, [1, 1], [1, 1]);  relu_53 = p_scratch_refinenet3_resconfunit2_conv1_weight = p_scratch_refinenet3_resconfunit2_conv1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_54: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.relu.default(conv2d_65);  conv2d_65 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_66: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.conv2d.default(relu_54, p_scratch_refinenet3_resconfunit2_conv2_weight, p_scratch_refinenet3_resconfunit2_conv2_bias, [1, 1], [1, 1]);  relu_54 = p_scratch_refinenet3_resconfunit2_conv2_weight = p_scratch_refinenet3_resconfunit2_conv2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:374 in forward, code: return self.skip_add.add(out, x)\n",
       "                    add_2473: \"f16[s77, 256, 24, 24]\" = torch.ops.aten.add.Tensor(conv2d_66, add_2447);  conv2d_66 = add_2447 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:432 in forward, code: output = nn.functional.interpolate(\n",
       "                    upsample_bilinear2d_2: \"f16[1, 256, 48, 48]\" = torch.ops.aten.upsample_bilinear2d.vec(add_2473, [48, 48], True, None);  add_2473 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_67: \"f16[1, 256, 48, 48]\" = torch.ops.aten.conv2d.default(upsample_bilinear2d_2, p_scratch_refinenet3_out_conv_weight, p_scratch_refinenet3_out_conv_bias);  upsample_bilinear2d_2 = p_scratch_refinenet3_out_conv_weight = p_scratch_refinenet3_out_conv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_55: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.relu.default(conv2d_57)\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_68: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.conv2d.default(relu_55, p_scratch_refinenet2_resconfunit1_conv1_weight, p_scratch_refinenet2_resconfunit1_conv1_bias, [1, 1], [1, 1]);  relu_55 = p_scratch_refinenet2_resconfunit1_conv1_weight = p_scratch_refinenet2_resconfunit1_conv1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_56: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.relu.default(conv2d_68);  conv2d_68 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_69: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.conv2d.default(relu_56, p_scratch_refinenet2_resconfunit1_conv2_weight, p_scratch_refinenet2_resconfunit1_conv2_bias, [1, 1], [1, 1]);  relu_56 = p_scratch_refinenet2_resconfunit1_conv2_weight = p_scratch_refinenet2_resconfunit1_conv2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:374 in forward, code: return self.skip_add.add(out, x)\n",
       "                    add_2499: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.add.Tensor(conv2d_69, conv2d_57);  conv2d_69 = conv2d_57 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:420 in forward, code: output = self.skip_add.add(output, res)\n",
       "                    add_2505: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.add.Tensor(conv2d_67, add_2499);  conv2d_67 = add_2499 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_57: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.relu.default(add_2505)\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_70: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.conv2d.default(relu_57, p_scratch_refinenet2_resconfunit2_conv1_weight, p_scratch_refinenet2_resconfunit2_conv1_bias, [1, 1], [1, 1]);  relu_57 = p_scratch_refinenet2_resconfunit2_conv1_weight = p_scratch_refinenet2_resconfunit2_conv1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_58: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.relu.default(conv2d_70);  conv2d_70 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_71: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.conv2d.default(relu_58, p_scratch_refinenet2_resconfunit2_conv2_weight, p_scratch_refinenet2_resconfunit2_conv2_bias, [1, 1], [1, 1]);  relu_58 = p_scratch_refinenet2_resconfunit2_conv2_weight = p_scratch_refinenet2_resconfunit2_conv2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:374 in forward, code: return self.skip_add.add(out, x)\n",
       "                    add_2531: \"f16[s77, 256, 48, 48]\" = torch.ops.aten.add.Tensor(conv2d_71, add_2505);  conv2d_71 = add_2505 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:432 in forward, code: output = nn.functional.interpolate(\n",
       "                    upsample_bilinear2d_3: \"f16[1, 256, 96, 96]\" = torch.ops.aten.upsample_bilinear2d.vec(add_2531, [96, 96], True, None);  add_2531 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_72: \"f16[1, 256, 96, 96]\" = torch.ops.aten.conv2d.default(upsample_bilinear2d_3, p_scratch_refinenet2_out_conv_weight, p_scratch_refinenet2_out_conv_bias);  upsample_bilinear2d_3 = p_scratch_refinenet2_out_conv_weight = p_scratch_refinenet2_out_conv_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_59: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.relu.default(conv2d_56)\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_73: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.conv2d.default(relu_59, p_scratch_refinenet1_resconfunit1_conv1_weight, p_scratch_refinenet1_resconfunit1_conv1_bias, [1, 1], [1, 1]);  relu_59 = p_scratch_refinenet1_resconfunit1_conv1_weight = p_scratch_refinenet1_resconfunit1_conv1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_60: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.relu.default(conv2d_73);  conv2d_73 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_74: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.conv2d.default(relu_60, p_scratch_refinenet1_resconfunit1_conv2_weight, p_scratch_refinenet1_resconfunit1_conv2_bias, [1, 1], [1, 1]);  relu_60 = p_scratch_refinenet1_resconfunit1_conv2_weight = p_scratch_refinenet1_resconfunit1_conv2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:374 in forward, code: return self.skip_add.add(out, x)\n",
       "                    add_2557: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.add.Tensor(conv2d_74, conv2d_56);  conv2d_74 = conv2d_56 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:420 in forward, code: output = self.skip_add.add(output, res)\n",
       "                    add_2563: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.add.Tensor(conv2d_72, add_2557);  conv2d_72 = add_2557 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_61: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.relu.default(add_2563)\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_75: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.conv2d.default(relu_61, p_scratch_refinenet1_resconfunit2_conv1_weight, p_scratch_refinenet1_resconfunit2_conv1_bias, [1, 1], [1, 1]);  relu_61 = p_scratch_refinenet1_resconfunit2_conv1_weight = p_scratch_refinenet1_resconfunit2_conv1_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_62: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.relu.default(conv2d_75);  conv2d_75 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_76: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.conv2d.default(relu_62, p_scratch_refinenet1_resconfunit2_conv2_weight, p_scratch_refinenet1_resconfunit2_conv2_bias, [1, 1], [1, 1]);  relu_62 = p_scratch_refinenet1_resconfunit2_conv2_weight = p_scratch_refinenet1_resconfunit2_conv2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:374 in forward, code: return self.skip_add.add(out, x)\n",
       "                    add_2589: \"f16[s77, 256, 96, 96]\" = torch.ops.aten.add.Tensor(conv2d_76, add_2563);  conv2d_76 = add_2563 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:432 in forward, code: output = nn.functional.interpolate(\n",
       "                    upsample_bilinear2d_4: \"f16[1, 256, 192, 192]\" = torch.ops.aten.upsample_bilinear2d.vec(add_2589, None, True, [2.0, 2.0]);  add_2589 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_77: \"f16[1, 256, 192, 192]\" = torch.ops.aten.conv2d.default(upsample_bilinear2d_4, p_scratch_refinenet1_out_conv_weight, p_scratch_refinenet1_out_conv_bias);  upsample_bilinear2d_4 = p_scratch_refinenet1_out_conv_weight = p_scratch_refinenet1_out_conv_bias = None\n",
       "                    conv2d_78: \"f16[1, 128, 192, 192]\" = torch.ops.aten.conv2d.default(conv2d_77, p_scratch_output_conv_0_weight, p_scratch_output_conv_0_bias, [1, 1], [1, 1]);  conv2d_77 = p_scratch_output_conv_0_weight = p_scratch_output_conv_0_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/.cache/torch/hub/intel-isl_MiDaS_master/midas/blocks.py:236 in forward, code: x = self.interp(\n",
       "                    upsample_bilinear2d_5: \"f16[1, 128, 384, 384]\" = torch.ops.aten.upsample_bilinear2d.vec(conv2d_78, None, True, [2.0, 2.0]);  conv2d_78 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_79: \"f16[1, 32, 384, 384]\" = torch.ops.aten.conv2d.default(upsample_bilinear2d_5, p_scratch_output_conv_2_weight, p_scratch_output_conv_2_bias, [1, 1], [1, 1]);  upsample_bilinear2d_5 = p_scratch_output_conv_2_weight = p_scratch_output_conv_2_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_63: \"f16[1, 32, 384, 384]\" = torch.ops.aten.relu.default(conv2d_79);  conv2d_79 = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_80: \"f16[1, 1, 384, 384]\" = torch.ops.aten.conv2d.default(relu_63, p_scratch_output_conv_4_weight, p_scratch_output_conv_4_bias);  relu_63 = p_scratch_output_conv_4_weight = p_scratch_output_conv_4_bias = None\n",
       "            \n",
       "                    # File: /home/RUS_CIP/st189432/MasterThesis/ddacs/Monocular-Depth-ViT-Optimization/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_64: \"f16[1, 1, 384, 384]\" = torch.ops.aten.relu.default(conv2d_80);  conv2d_80 = None\n",
       "            \n",
       "                    # No stacktrace found for following nodes\n",
       "                    squeeze_1: \"f16[1, 384, 384]\" = torch.ops.aten.squeeze.dim(relu_64, 1);  relu_64 = None\n",
       "                    return (squeeze_1,)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_pretrained_model_cls_token: PARAMETER target='pretrained.model.cls_token'\n",
       "            p_pretrained_model_pos_embed: PARAMETER target='pretrained.model.pos_embed'\n",
       "            p_pretrained_model_patch_embed_backbone_stem_conv_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stem.conv.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stem_norm_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stem.norm.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stem_norm_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stem.norm.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_downsample_conv_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.0.downsample.conv.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_downsample_norm_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.0.downsample.norm.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_downsample_norm_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.0.downsample.norm.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.0.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.0.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.0.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.0.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.0.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.0.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.0.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.0.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_0_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.0.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.1.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.1.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.1.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.1.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.1.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.1.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.1.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.1.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_1_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.1.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.2.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.2.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.2.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.2.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.2.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.2.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.2.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.2.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_0_blocks_2_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.0.blocks.2.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_downsample_conv_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.0.downsample.conv.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_downsample_norm_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.0.downsample.norm.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_downsample_norm_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.0.downsample.norm.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.0.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.0.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.0.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.0.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.0.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.0.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.0.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.0.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_0_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.0.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.1.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.1.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.1.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.1.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.1.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.1.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.1.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.1.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_1_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.1.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.2.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.2.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.2.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.2.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.2.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.2.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.2.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.2.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_2_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.2.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.3.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.3.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.3.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.3.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.3.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.3.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.3.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.3.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_1_blocks_3_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.1.blocks.3.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_downsample_conv_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.0.downsample.conv.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_downsample_norm_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.0.downsample.norm.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_downsample_norm_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.0.downsample.norm.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.0.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.0.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.0.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.0.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.0.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.0.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.0.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.0.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_0_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.0.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.1.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.1.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.1.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.1.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.1.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.1.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.1.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.1.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_1_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.1.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.2.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.2.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.2.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.2.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.2.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.2.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.2.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.2.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_2_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.2.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.3.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.3.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.3.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.3.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.3.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.3.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.3.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.3.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_3_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.3.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.4.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.4.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.4.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.4.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.4.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.4.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.4.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.4.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_4_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.4.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.5.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.5.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.5.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.5.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.5.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.5.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.5.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.5.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_5_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.5.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.6.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.6.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.6.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.6.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.6.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.6.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.6.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.6.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_6_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.6.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.7.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.7.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.7.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.7.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.7.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.7.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.7.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.7.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_7_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.7.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_conv1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.8.conv1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm1_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.8.norm1.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm1_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.8.norm1.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_conv2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.8.conv2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm2_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.8.norm2.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm2_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.8.norm2.bias'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_conv3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.8.conv3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm3_weight: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.8.norm3.weight'\n",
       "            p_pretrained_model_patch_embed_backbone_stages_2_blocks_8_norm3_bias: PARAMETER target='pretrained.model.patch_embed.backbone.stages.2.blocks.8.norm3.bias'\n",
       "            p_pretrained_model_patch_embed_proj_weight: PARAMETER target='pretrained.model.patch_embed.proj.weight'\n",
       "            p_pretrained_model_patch_embed_proj_bias: PARAMETER target='pretrained.model.patch_embed.proj.bias'\n",
       "            p_pretrained_model_blocks_0_norm1_weight: PARAMETER target='pretrained.model.blocks.0.norm1.weight'\n",
       "            p_pretrained_model_blocks_0_norm1_bias: PARAMETER target='pretrained.model.blocks.0.norm1.bias'\n",
       "            p_pretrained_model_blocks_0_attn_qkv_weight: PARAMETER target='pretrained.model.blocks.0.attn.qkv.weight'\n",
       "            p_pretrained_model_blocks_0_attn_qkv_bias: PARAMETER target='pretrained.model.blocks.0.attn.qkv.bias'\n",
       "            p_pretrained_model_blocks_0_attn_proj_weight: PARAMETER target='pretrained.model.blocks.0.attn.proj.weight'\n",
       "            p_pretrained_model_blocks_0_attn_proj_bias: PARAMETER target='pretrained.model.blocks.0.attn.proj.bias'\n",
       "            p_pretrained_model_blocks_0_norm2_weight: PARAMETER target='pretrained.model.blocks.0.norm2.weight'\n",
       "            p_pretrained_model_blocks_0_norm2_bias: PARAMETER target='pretrained.model.blocks.0.norm2.bias'\n",
       "            p_pretrained_model_blocks_0_mlp_fc1_weight: PARAMETER target='pretrained.model.blocks.0.mlp.fc1.weight'\n",
       "            p_pretrained_model_blocks_0_mlp_fc1_bias: PARAMETER target='pretrained.model.blocks.0.mlp.fc1.bias'\n",
       "            p_pretrained_model_blocks_0_mlp_fc2_weight: PARAMETER target='pretrained.model.blocks.0.mlp.fc2.weight'\n",
       "            p_pretrained_model_blocks_0_mlp_fc2_bias: PARAMETER target='pretrained.model.blocks.0.mlp.fc2.bias'\n",
       "            p_pretrained_model_blocks_1_norm1_weight: PARAMETER target='pretrained.model.blocks.1.norm1.weight'\n",
       "            p_pretrained_model_blocks_1_norm1_bias: PARAMETER target='pretrained.model.blocks.1.norm1.bias'\n",
       "            p_pretrained_model_blocks_1_attn_qkv_weight: PARAMETER target='pretrained.model.blocks.1.attn.qkv.weight'\n",
       "            p_pretrained_model_blocks_1_attn_qkv_bias: PARAMETER target='pretrained.model.blocks.1.attn.qkv.bias'\n",
       "            p_pretrained_model_blocks_1_attn_proj_weight: PARAMETER target='pretrained.model.blocks.1.attn.proj.weight'\n",
       "            p_pretrained_model_blocks_1_attn_proj_bias: PARAMETER target='pretrained.model.blocks.1.attn.proj.bias'\n",
       "            p_pretrained_model_blocks_1_norm2_weight: PARAMETER target='pretrained.model.blocks.1.norm2.weight'\n",
       "            p_pretrained_model_blocks_1_norm2_bias: PARAMETER target='pretrained.model.blocks.1.norm2.bias'\n",
       "            p_pretrained_model_blocks_1_mlp_fc1_weight: PARAMETER target='pretrained.model.blocks.1.mlp.fc1.weight'\n",
       "            p_pretrained_model_blocks_1_mlp_fc1_bias: PARAMETER target='pretrained.model.blocks.1.mlp.fc1.bias'\n",
       "            p_pretrained_model_blocks_1_mlp_fc2_weight: PARAMETER target='pretrained.model.blocks.1.mlp.fc2.weight'\n",
       "            p_pretrained_model_blocks_1_mlp_fc2_bias: PARAMETER target='pretrained.model.blocks.1.mlp.fc2.bias'\n",
       "            p_pretrained_model_blocks_2_norm1_weight: PARAMETER target='pretrained.model.blocks.2.norm1.weight'\n",
       "            p_pretrained_model_blocks_2_norm1_bias: PARAMETER target='pretrained.model.blocks.2.norm1.bias'\n",
       "            p_pretrained_model_blocks_2_attn_qkv_weight: PARAMETER target='pretrained.model.blocks.2.attn.qkv.weight'\n",
       "            p_pretrained_model_blocks_2_attn_qkv_bias: PARAMETER target='pretrained.model.blocks.2.attn.qkv.bias'\n",
       "            p_pretrained_model_blocks_2_attn_proj_weight: PARAMETER target='pretrained.model.blocks.2.attn.proj.weight'\n",
       "            p_pretrained_model_blocks_2_attn_proj_bias: PARAMETER target='pretrained.model.blocks.2.attn.proj.bias'\n",
       "            p_pretrained_model_blocks_2_norm2_weight: PARAMETER target='pretrained.model.blocks.2.norm2.weight'\n",
       "            p_pretrained_model_blocks_2_norm2_bias: PARAMETER target='pretrained.model.blocks.2.norm2.bias'\n",
       "            p_pretrained_model_blocks_2_mlp_fc1_weight: PARAMETER target='pretrained.model.blocks.2.mlp.fc1.weight'\n",
       "            p_pretrained_model_blocks_2_mlp_fc1_bias: PARAMETER target='pretrained.model.blocks.2.mlp.fc1.bias'\n",
       "            p_pretrained_model_blocks_2_mlp_fc2_weight: PARAMETER target='pretrained.model.blocks.2.mlp.fc2.weight'\n",
       "            p_pretrained_model_blocks_2_mlp_fc2_bias: PARAMETER target='pretrained.model.blocks.2.mlp.fc2.bias'\n",
       "            p_pretrained_model_blocks_3_norm1_weight: PARAMETER target='pretrained.model.blocks.3.norm1.weight'\n",
       "            p_pretrained_model_blocks_3_norm1_bias: PARAMETER target='pretrained.model.blocks.3.norm1.bias'\n",
       "            p_pretrained_model_blocks_3_attn_qkv_weight: PARAMETER target='pretrained.model.blocks.3.attn.qkv.weight'\n",
       "            p_pretrained_model_blocks_3_attn_qkv_bias: PARAMETER target='pretrained.model.blocks.3.attn.qkv.bias'\n",
       "            p_pretrained_model_blocks_3_attn_proj_weight: PARAMETER target='pretrained.model.blocks.3.attn.proj.weight'\n",
       "            p_pretrained_model_blocks_3_attn_proj_bias: PARAMETER target='pretrained.model.blocks.3.attn.proj.bias'\n",
       "            p_pretrained_model_blocks_3_norm2_weight: PARAMETER target='pretrained.model.blocks.3.norm2.weight'\n",
       "            p_pretrained_model_blocks_3_norm2_bias: PARAMETER target='pretrained.model.blocks.3.norm2.bias'\n",
       "            p_pretrained_model_blocks_3_mlp_fc1_weight: PARAMETER target='pretrained.model.blocks.3.mlp.fc1.weight'\n",
       "            p_pretrained_model_blocks_3_mlp_fc1_bias: PARAMETER target='pretrained.model.blocks.3.mlp.fc1.bias'\n",
       "            p_pretrained_model_blocks_3_mlp_fc2_weight: PARAMETER target='pretrained.model.blocks.3.mlp.fc2.weight'\n",
       "            p_pretrained_model_blocks_3_mlp_fc2_bias: PARAMETER target='pretrained.model.blocks.3.mlp.fc2.bias'\n",
       "            p_pretrained_model_blocks_4_norm1_weight: PARAMETER target='pretrained.model.blocks.4.norm1.weight'\n",
       "            p_pretrained_model_blocks_4_norm1_bias: PARAMETER target='pretrained.model.blocks.4.norm1.bias'\n",
       "            p_pretrained_model_blocks_4_attn_qkv_weight: PARAMETER target='pretrained.model.blocks.4.attn.qkv.weight'\n",
       "            p_pretrained_model_blocks_4_attn_qkv_bias: PARAMETER target='pretrained.model.blocks.4.attn.qkv.bias'\n",
       "            p_pretrained_model_blocks_4_attn_proj_weight: PARAMETER target='pretrained.model.blocks.4.attn.proj.weight'\n",
       "            p_pretrained_model_blocks_4_attn_proj_bias: PARAMETER target='pretrained.model.blocks.4.attn.proj.bias'\n",
       "            p_pretrained_model_blocks_4_norm2_weight: PARAMETER target='pretrained.model.blocks.4.norm2.weight'\n",
       "            p_pretrained_model_blocks_4_norm2_bias: PARAMETER target='pretrained.model.blocks.4.norm2.bias'\n",
       "            p_pretrained_model_blocks_4_mlp_fc1_weight: PARAMETER target='pretrained.model.blocks.4.mlp.fc1.weight'\n",
       "            p_pretrained_model_blocks_4_mlp_fc1_bias: PARAMETER target='pretrained.model.blocks.4.mlp.fc1.bias'\n",
       "            p_pretrained_model_blocks_4_mlp_fc2_weight: PARAMETER target='pretrained.model.blocks.4.mlp.fc2.weight'\n",
       "            p_pretrained_model_blocks_4_mlp_fc2_bias: PARAMETER target='pretrained.model.blocks.4.mlp.fc2.bias'\n",
       "            p_pretrained_model_blocks_5_norm1_weight: PARAMETER target='pretrained.model.blocks.5.norm1.weight'\n",
       "            p_pretrained_model_blocks_5_norm1_bias: PARAMETER target='pretrained.model.blocks.5.norm1.bias'\n",
       "            p_pretrained_model_blocks_5_attn_qkv_weight: PARAMETER target='pretrained.model.blocks.5.attn.qkv.weight'\n",
       "            p_pretrained_model_blocks_5_attn_qkv_bias: PARAMETER target='pretrained.model.blocks.5.attn.qkv.bias'\n",
       "            p_pretrained_model_blocks_5_attn_proj_weight: PARAMETER target='pretrained.model.blocks.5.attn.proj.weight'\n",
       "            p_pretrained_model_blocks_5_attn_proj_bias: PARAMETER target='pretrained.model.blocks.5.attn.proj.bias'\n",
       "            p_pretrained_model_blocks_5_norm2_weight: PARAMETER target='pretrained.model.blocks.5.norm2.weight'\n",
       "            p_pretrained_model_blocks_5_norm2_bias: PARAMETER target='pretrained.model.blocks.5.norm2.bias'\n",
       "            p_pretrained_model_blocks_5_mlp_fc1_weight: PARAMETER target='pretrained.model.blocks.5.mlp.fc1.weight'\n",
       "            p_pretrained_model_blocks_5_mlp_fc1_bias: PARAMETER target='pretrained.model.blocks.5.mlp.fc1.bias'\n",
       "            p_pretrained_model_blocks_5_mlp_fc2_weight: PARAMETER target='pretrained.model.blocks.5.mlp.fc2.weight'\n",
       "            p_pretrained_model_blocks_5_mlp_fc2_bias: PARAMETER target='pretrained.model.blocks.5.mlp.fc2.bias'\n",
       "            p_pretrained_model_blocks_6_norm1_weight: PARAMETER target='pretrained.model.blocks.6.norm1.weight'\n",
       "            p_pretrained_model_blocks_6_norm1_bias: PARAMETER target='pretrained.model.blocks.6.norm1.bias'\n",
       "            p_pretrained_model_blocks_6_attn_qkv_weight: PARAMETER target='pretrained.model.blocks.6.attn.qkv.weight'\n",
       "            p_pretrained_model_blocks_6_attn_qkv_bias: PARAMETER target='pretrained.model.blocks.6.attn.qkv.bias'\n",
       "            p_pretrained_model_blocks_6_attn_proj_weight: PARAMETER target='pretrained.model.blocks.6.attn.proj.weight'\n",
       "            p_pretrained_model_blocks_6_attn_proj_bias: PARAMETER target='pretrained.model.blocks.6.attn.proj.bias'\n",
       "            p_pretrained_model_blocks_6_norm2_weight: PARAMETER target='pretrained.model.blocks.6.norm2.weight'\n",
       "            p_pretrained_model_blocks_6_norm2_bias: PARAMETER target='pretrained.model.blocks.6.norm2.bias'\n",
       "            p_pretrained_model_blocks_6_mlp_fc1_weight: PARAMETER target='pretrained.model.blocks.6.mlp.fc1.weight'\n",
       "            p_pretrained_model_blocks_6_mlp_fc1_bias: PARAMETER target='pretrained.model.blocks.6.mlp.fc1.bias'\n",
       "            p_pretrained_model_blocks_6_mlp_fc2_weight: PARAMETER target='pretrained.model.blocks.6.mlp.fc2.weight'\n",
       "            p_pretrained_model_blocks_6_mlp_fc2_bias: PARAMETER target='pretrained.model.blocks.6.mlp.fc2.bias'\n",
       "            p_pretrained_model_blocks_7_norm1_weight: PARAMETER target='pretrained.model.blocks.7.norm1.weight'\n",
       "            p_pretrained_model_blocks_7_norm1_bias: PARAMETER target='pretrained.model.blocks.7.norm1.bias'\n",
       "            p_pretrained_model_blocks_7_attn_qkv_weight: PARAMETER target='pretrained.model.blocks.7.attn.qkv.weight'\n",
       "            p_pretrained_model_blocks_7_attn_qkv_bias: PARAMETER target='pretrained.model.blocks.7.attn.qkv.bias'\n",
       "            p_pretrained_model_blocks_7_attn_proj_weight: PARAMETER target='pretrained.model.blocks.7.attn.proj.weight'\n",
       "            p_pretrained_model_blocks_7_attn_proj_bias: PARAMETER target='pretrained.model.blocks.7.attn.proj.bias'\n",
       "            p_pretrained_model_blocks_7_norm2_weight: PARAMETER target='pretrained.model.blocks.7.norm2.weight'\n",
       "            p_pretrained_model_blocks_7_norm2_bias: PARAMETER target='pretrained.model.blocks.7.norm2.bias'\n",
       "            p_pretrained_model_blocks_7_mlp_fc1_weight: PARAMETER target='pretrained.model.blocks.7.mlp.fc1.weight'\n",
       "            p_pretrained_model_blocks_7_mlp_fc1_bias: PARAMETER target='pretrained.model.blocks.7.mlp.fc1.bias'\n",
       "            p_pretrained_model_blocks_7_mlp_fc2_weight: PARAMETER target='pretrained.model.blocks.7.mlp.fc2.weight'\n",
       "            p_pretrained_model_blocks_7_mlp_fc2_bias: PARAMETER target='pretrained.model.blocks.7.mlp.fc2.bias'\n",
       "            p_pretrained_model_blocks_8_norm1_weight: PARAMETER target='pretrained.model.blocks.8.norm1.weight'\n",
       "            p_pretrained_model_blocks_8_norm1_bias: PARAMETER target='pretrained.model.blocks.8.norm1.bias'\n",
       "            p_pretrained_model_blocks_8_attn_qkv_weight: PARAMETER target='pretrained.model.blocks.8.attn.qkv.weight'\n",
       "            p_pretrained_model_blocks_8_attn_qkv_bias: PARAMETER target='pretrained.model.blocks.8.attn.qkv.bias'\n",
       "            p_pretrained_model_blocks_8_attn_proj_weight: PARAMETER target='pretrained.model.blocks.8.attn.proj.weight'\n",
       "            p_pretrained_model_blocks_8_attn_proj_bias: PARAMETER target='pretrained.model.blocks.8.attn.proj.bias'\n",
       "            p_pretrained_model_blocks_8_norm2_weight: PARAMETER target='pretrained.model.blocks.8.norm2.weight'\n",
       "            p_pretrained_model_blocks_8_norm2_bias: PARAMETER target='pretrained.model.blocks.8.norm2.bias'\n",
       "            p_pretrained_model_blocks_8_mlp_fc1_weight: PARAMETER target='pretrained.model.blocks.8.mlp.fc1.weight'\n",
       "            p_pretrained_model_blocks_8_mlp_fc1_bias: PARAMETER target='pretrained.model.blocks.8.mlp.fc1.bias'\n",
       "            p_pretrained_model_blocks_8_mlp_fc2_weight: PARAMETER target='pretrained.model.blocks.8.mlp.fc2.weight'\n",
       "            p_pretrained_model_blocks_8_mlp_fc2_bias: PARAMETER target='pretrained.model.blocks.8.mlp.fc2.bias'\n",
       "            p_pretrained_model_blocks_9_norm1_weight: PARAMETER target='pretrained.model.blocks.9.norm1.weight'\n",
       "            p_pretrained_model_blocks_9_norm1_bias: PARAMETER target='pretrained.model.blocks.9.norm1.bias'\n",
       "            p_pretrained_model_blocks_9_attn_qkv_weight: PARAMETER target='pretrained.model.blocks.9.attn.qkv.weight'\n",
       "            p_pretrained_model_blocks_9_attn_qkv_bias: PARAMETER target='pretrained.model.blocks.9.attn.qkv.bias'\n",
       "            p_pretrained_model_blocks_9_attn_proj_weight: PARAMETER target='pretrained.model.blocks.9.attn.proj.weight'\n",
       "            p_pretrained_model_blocks_9_attn_proj_bias: PARAMETER target='pretrained.model.blocks.9.attn.proj.bias'\n",
       "            p_pretrained_model_blocks_9_norm2_weight: PARAMETER target='pretrained.model.blocks.9.norm2.weight'\n",
       "            p_pretrained_model_blocks_9_norm2_bias: PARAMETER target='pretrained.model.blocks.9.norm2.bias'\n",
       "            p_pretrained_model_blocks_9_mlp_fc1_weight: PARAMETER target='pretrained.model.blocks.9.mlp.fc1.weight'\n",
       "            p_pretrained_model_blocks_9_mlp_fc1_bias: PARAMETER target='pretrained.model.blocks.9.mlp.fc1.bias'\n",
       "            p_pretrained_model_blocks_9_mlp_fc2_weight: PARAMETER target='pretrained.model.blocks.9.mlp.fc2.weight'\n",
       "            p_pretrained_model_blocks_9_mlp_fc2_bias: PARAMETER target='pretrained.model.blocks.9.mlp.fc2.bias'\n",
       "            p_pretrained_model_blocks_10_norm1_weight: PARAMETER target='pretrained.model.blocks.10.norm1.weight'\n",
       "            p_pretrained_model_blocks_10_norm1_bias: PARAMETER target='pretrained.model.blocks.10.norm1.bias'\n",
       "            p_pretrained_model_blocks_10_attn_qkv_weight: PARAMETER target='pretrained.model.blocks.10.attn.qkv.weight'\n",
       "            p_pretrained_model_blocks_10_attn_qkv_bias: PARAMETER target='pretrained.model.blocks.10.attn.qkv.bias'\n",
       "            p_pretrained_model_blocks_10_attn_proj_weight: PARAMETER target='pretrained.model.blocks.10.attn.proj.weight'\n",
       "            p_pretrained_model_blocks_10_attn_proj_bias: PARAMETER target='pretrained.model.blocks.10.attn.proj.bias'\n",
       "            p_pretrained_model_blocks_10_norm2_weight: PARAMETER target='pretrained.model.blocks.10.norm2.weight'\n",
       "            p_pretrained_model_blocks_10_norm2_bias: PARAMETER target='pretrained.model.blocks.10.norm2.bias'\n",
       "            p_pretrained_model_blocks_10_mlp_fc1_weight: PARAMETER target='pretrained.model.blocks.10.mlp.fc1.weight'\n",
       "            p_pretrained_model_blocks_10_mlp_fc1_bias: PARAMETER target='pretrained.model.blocks.10.mlp.fc1.bias'\n",
       "            p_pretrained_model_blocks_10_mlp_fc2_weight: PARAMETER target='pretrained.model.blocks.10.mlp.fc2.weight'\n",
       "            p_pretrained_model_blocks_10_mlp_fc2_bias: PARAMETER target='pretrained.model.blocks.10.mlp.fc2.bias'\n",
       "            p_pretrained_model_blocks_11_norm1_weight: PARAMETER target='pretrained.model.blocks.11.norm1.weight'\n",
       "            p_pretrained_model_blocks_11_norm1_bias: PARAMETER target='pretrained.model.blocks.11.norm1.bias'\n",
       "            p_pretrained_model_blocks_11_attn_qkv_weight: PARAMETER target='pretrained.model.blocks.11.attn.qkv.weight'\n",
       "            p_pretrained_model_blocks_11_attn_qkv_bias: PARAMETER target='pretrained.model.blocks.11.attn.qkv.bias'\n",
       "            p_pretrained_model_blocks_11_attn_proj_weight: PARAMETER target='pretrained.model.blocks.11.attn.proj.weight'\n",
       "            p_pretrained_model_blocks_11_attn_proj_bias: PARAMETER target='pretrained.model.blocks.11.attn.proj.bias'\n",
       "            p_pretrained_model_blocks_11_norm2_weight: PARAMETER target='pretrained.model.blocks.11.norm2.weight'\n",
       "            p_pretrained_model_blocks_11_norm2_bias: PARAMETER target='pretrained.model.blocks.11.norm2.bias'\n",
       "            p_pretrained_model_blocks_11_mlp_fc1_weight: PARAMETER target='pretrained.model.blocks.11.mlp.fc1.weight'\n",
       "            p_pretrained_model_blocks_11_mlp_fc1_bias: PARAMETER target='pretrained.model.blocks.11.mlp.fc1.bias'\n",
       "            p_pretrained_model_blocks_11_mlp_fc2_weight: PARAMETER target='pretrained.model.blocks.11.mlp.fc2.weight'\n",
       "            p_pretrained_model_blocks_11_mlp_fc2_bias: PARAMETER target='pretrained.model.blocks.11.mlp.fc2.bias'\n",
       "            p_pretrained_model_norm_weight: PARAMETER target='pretrained.model.norm.weight'\n",
       "            p_pretrained_model_norm_bias: PARAMETER target='pretrained.model.norm.bias'\n",
       "            p_pretrained_model_head_weight: PARAMETER target='pretrained.model.head.weight'\n",
       "            p_pretrained_model_head_bias: PARAMETER target='pretrained.model.head.bias'\n",
       "            p_pretrained_act_postprocess3_0_project_0_weight: PARAMETER target='pretrained.act_postprocess3.0.project.0.weight'\n",
       "            p_pretrained_act_postprocess3_0_project_0_bias: PARAMETER target='pretrained.act_postprocess3.0.project.0.bias'\n",
       "            p_pretrained_act_postprocess3_3_weight: PARAMETER target='pretrained.act_postprocess3.3.weight'\n",
       "            p_pretrained_act_postprocess3_3_bias: PARAMETER target='pretrained.act_postprocess3.3.bias'\n",
       "            p_pretrained_act_postprocess4_0_project_0_weight: PARAMETER target='pretrained.act_postprocess4.0.project.0.weight'\n",
       "            p_pretrained_act_postprocess4_0_project_0_bias: PARAMETER target='pretrained.act_postprocess4.0.project.0.bias'\n",
       "            p_pretrained_act_postprocess4_3_weight: PARAMETER target='pretrained.act_postprocess4.3.weight'\n",
       "            p_pretrained_act_postprocess4_3_bias: PARAMETER target='pretrained.act_postprocess4.3.bias'\n",
       "            p_pretrained_act_postprocess4_4_weight: PARAMETER target='pretrained.act_postprocess4.4.weight'\n",
       "            p_pretrained_act_postprocess4_4_bias: PARAMETER target='pretrained.act_postprocess4.4.bias'\n",
       "            p_scratch_layer1_rn_weight: PARAMETER target='scratch.layer1_rn.weight'\n",
       "            p_scratch_layer2_rn_weight: PARAMETER target='scratch.layer2_rn.weight'\n",
       "            p_scratch_layer3_rn_weight: PARAMETER target='scratch.layer3_rn.weight'\n",
       "            p_scratch_layer4_rn_weight: PARAMETER target='scratch.layer4_rn.weight'\n",
       "            p_scratch_refinenet1_out_conv_weight: PARAMETER target='scratch.refinenet1.out_conv.weight'\n",
       "            p_scratch_refinenet1_out_conv_bias: PARAMETER target='scratch.refinenet1.out_conv.bias'\n",
       "            p_scratch_refinenet1_resconfunit1_conv1_weight: PARAMETER target='scratch.refinenet1.resConfUnit1.conv1.weight'\n",
       "            p_scratch_refinenet1_resconfunit1_conv1_bias: PARAMETER target='scratch.refinenet1.resConfUnit1.conv1.bias'\n",
       "            p_scratch_refinenet1_resconfunit1_conv2_weight: PARAMETER target='scratch.refinenet1.resConfUnit1.conv2.weight'\n",
       "            p_scratch_refinenet1_resconfunit1_conv2_bias: PARAMETER target='scratch.refinenet1.resConfUnit1.conv2.bias'\n",
       "            p_scratch_refinenet1_resconfunit2_conv1_weight: PARAMETER target='scratch.refinenet1.resConfUnit2.conv1.weight'\n",
       "            p_scratch_refinenet1_resconfunit2_conv1_bias: PARAMETER target='scratch.refinenet1.resConfUnit2.conv1.bias'\n",
       "            p_scratch_refinenet1_resconfunit2_conv2_weight: PARAMETER target='scratch.refinenet1.resConfUnit2.conv2.weight'\n",
       "            p_scratch_refinenet1_resconfunit2_conv2_bias: PARAMETER target='scratch.refinenet1.resConfUnit2.conv2.bias'\n",
       "            p_scratch_refinenet2_out_conv_weight: PARAMETER target='scratch.refinenet2.out_conv.weight'\n",
       "            p_scratch_refinenet2_out_conv_bias: PARAMETER target='scratch.refinenet2.out_conv.bias'\n",
       "            p_scratch_refinenet2_resconfunit1_conv1_weight: PARAMETER target='scratch.refinenet2.resConfUnit1.conv1.weight'\n",
       "            p_scratch_refinenet2_resconfunit1_conv1_bias: PARAMETER target='scratch.refinenet2.resConfUnit1.conv1.bias'\n",
       "            p_scratch_refinenet2_resconfunit1_conv2_weight: PARAMETER target='scratch.refinenet2.resConfUnit1.conv2.weight'\n",
       "            p_scratch_refinenet2_resconfunit1_conv2_bias: PARAMETER target='scratch.refinenet2.resConfUnit1.conv2.bias'\n",
       "            p_scratch_refinenet2_resconfunit2_conv1_weight: PARAMETER target='scratch.refinenet2.resConfUnit2.conv1.weight'\n",
       "            p_scratch_refinenet2_resconfunit2_conv1_bias: PARAMETER target='scratch.refinenet2.resConfUnit2.conv1.bias'\n",
       "            p_scratch_refinenet2_resconfunit2_conv2_weight: PARAMETER target='scratch.refinenet2.resConfUnit2.conv2.weight'\n",
       "            p_scratch_refinenet2_resconfunit2_conv2_bias: PARAMETER target='scratch.refinenet2.resConfUnit2.conv2.bias'\n",
       "            p_scratch_refinenet3_out_conv_weight: PARAMETER target='scratch.refinenet3.out_conv.weight'\n",
       "            p_scratch_refinenet3_out_conv_bias: PARAMETER target='scratch.refinenet3.out_conv.bias'\n",
       "            p_scratch_refinenet3_resconfunit1_conv1_weight: PARAMETER target='scratch.refinenet3.resConfUnit1.conv1.weight'\n",
       "            p_scratch_refinenet3_resconfunit1_conv1_bias: PARAMETER target='scratch.refinenet3.resConfUnit1.conv1.bias'\n",
       "            p_scratch_refinenet3_resconfunit1_conv2_weight: PARAMETER target='scratch.refinenet3.resConfUnit1.conv2.weight'\n",
       "            p_scratch_refinenet3_resconfunit1_conv2_bias: PARAMETER target='scratch.refinenet3.resConfUnit1.conv2.bias'\n",
       "            p_scratch_refinenet3_resconfunit2_conv1_weight: PARAMETER target='scratch.refinenet3.resConfUnit2.conv1.weight'\n",
       "            p_scratch_refinenet3_resconfunit2_conv1_bias: PARAMETER target='scratch.refinenet3.resConfUnit2.conv1.bias'\n",
       "            p_scratch_refinenet3_resconfunit2_conv2_weight: PARAMETER target='scratch.refinenet3.resConfUnit2.conv2.weight'\n",
       "            p_scratch_refinenet3_resconfunit2_conv2_bias: PARAMETER target='scratch.refinenet3.resConfUnit2.conv2.bias'\n",
       "            p_scratch_refinenet4_out_conv_weight: PARAMETER target='scratch.refinenet4.out_conv.weight'\n",
       "            p_scratch_refinenet4_out_conv_bias: PARAMETER target='scratch.refinenet4.out_conv.bias'\n",
       "            p_scratch_refinenet4_resconfunit1_conv1_weight: PARAMETER target='scratch.refinenet4.resConfUnit1.conv1.weight'\n",
       "            p_scratch_refinenet4_resconfunit1_conv1_bias: PARAMETER target='scratch.refinenet4.resConfUnit1.conv1.bias'\n",
       "            p_scratch_refinenet4_resconfunit1_conv2_weight: PARAMETER target='scratch.refinenet4.resConfUnit1.conv2.weight'\n",
       "            p_scratch_refinenet4_resconfunit1_conv2_bias: PARAMETER target='scratch.refinenet4.resConfUnit1.conv2.bias'\n",
       "            p_scratch_refinenet4_resconfunit2_conv1_weight: PARAMETER target='scratch.refinenet4.resConfUnit2.conv1.weight'\n",
       "            p_scratch_refinenet4_resconfunit2_conv1_bias: PARAMETER target='scratch.refinenet4.resConfUnit2.conv1.bias'\n",
       "            p_scratch_refinenet4_resconfunit2_conv2_weight: PARAMETER target='scratch.refinenet4.resConfUnit2.conv2.weight'\n",
       "            p_scratch_refinenet4_resconfunit2_conv2_bias: PARAMETER target='scratch.refinenet4.resConfUnit2.conv2.bias'\n",
       "            p_scratch_output_conv_0_weight: PARAMETER target='scratch.output_conv.0.weight'\n",
       "            p_scratch_output_conv_0_bias: PARAMETER target='scratch.output_conv.0.bias'\n",
       "            p_scratch_output_conv_2_weight: PARAMETER target='scratch.output_conv.2.weight'\n",
       "            p_scratch_output_conv_2_bias: PARAMETER target='scratch.output_conv.2.bias'\n",
       "            p_scratch_output_conv_4_weight: PARAMETER target='scratch.output_conv.4.weight'\n",
       "            p_scratch_output_conv_4_bias: PARAMETER target='scratch.output_conv.4.bias'\n",
       "            x: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            squeeze_1: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {s77: VR[0, 2]}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"midas.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"depth\"],\n",
    "    opset_version=17,\n",
    "    dynamic_axes={\"input\": {0: \"batch\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model, device=\"cuda\", precision=\"fp32\"):\n",
    "    model = model.to(device)\n",
    "\n",
    "    if precision == \"fp16\":\n",
    "        model = model.half()\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "def build_infer_fn(model, device=\"cuda\", precision=\"fp32\"):\n",
    "\n",
    "    if precision == \"fp32\":\n",
    "        def infer(input_tensor):\n",
    "            with torch.no_grad():\n",
    "                return model(input_tensor)\n",
    "\n",
    "    elif precision == \"amp\":\n",
    "        def infer(input_tensor):\n",
    "            with torch.no_grad():\n",
    "                with torch.amp.autocast('cuda',dtype=torch.float16):\n",
    "                    return model(input_tensor)\n",
    "\n",
    "    elif precision == \"fp16\":\n",
    "        def infer(input_tensor):\n",
    "            with torch.no_grad():\n",
    "                return model(input_tensor.half())\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported precision\")\n",
    "\n",
    "    return infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f889735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiler run complete. Printing summary for Compiled Model...\n",
      "--------------------------------------------------\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "## Call CompiledFxGraph f44qjcsmxtrjesnygsobp7jlvtrk...         0.00%       0.000us         0.00%       0.000us       0.000us       7.570ms       101.67%       7.570ms       7.570ms             1  \n",
      "                             Torch-Compiled Region: 0/3         0.07%      20.970us        91.32%      28.030ms      28.030ms       0.000us         0.00%       7.446ms       7.446ms             1  \n",
      "                             Torch-Compiled Region: 1/3         0.08%      25.006us        90.89%      27.896ms      27.896ms       0.000us         0.00%       7.446ms       7.446ms             1  \n",
      "                             Torch-Compiled Region: 2/3         0.03%       8.116us        73.18%      22.461ms      22.461ms       0.000us         0.00%       4.820ms       4.820ms             1  \n",
      "                             Torch-Compiled Region: 3/3         0.89%     273.455us        73.13%      22.446ms      22.446ms       0.000us         0.00%       4.820ms       4.820ms             1  \n",
      "                             Torch-Compiled Region: 4/4         2.69%     825.846us        58.02%      17.807ms      17.807ms       0.000us         0.00%       4.537ms       4.537ms             1  \n",
      "## Call CompiledFxGraph f44qjcsmxtrjesnygsobp7jlvtrk...        27.40%       8.411ms        55.08%      16.905ms      16.905ms       0.000us         0.00%       4.537ms       4.537ms             1  \n",
      "## Call CompiledFxGraph fl7h4yny5hkzzpabrwty5sw5dz7v...         0.00%       0.000us         0.00%       0.000us       0.000us       3.527ms        47.37%       3.527ms       3.527ms             1  \n",
      "                                      aten::convolution         0.61%     185.753us         9.69%       2.974ms      36.710us       0.000us         0.00%       2.804ms      34.612us            81  \n",
      "                                     aten::_convolution         0.89%     272.965us         9.08%       2.788ms      34.417us       0.000us         0.00%       2.804ms      34.612us            81  \n",
      "                                aten::cudnn_convolution         5.92%       1.818ms         7.97%       2.446ms      30.198us       2.778ms        37.31%       2.778ms      34.295us            81  \n",
      "                             Torch-Compiled Region: 6/1         0.14%      42.100us        17.33%       5.318ms       5.318ms       0.000us         0.00%       2.626ms       2.626ms             1  \n",
      "## Call CompiledFxGraph fl7h4yny5hkzzpabrwty5sw5dz7v...        11.36%       3.488ms        17.14%       5.260ms       5.260ms       0.000us         0.00%       2.626ms       2.626ms             1  \n",
      "                                               aten::mm         1.62%     497.695us         2.37%     728.141us      19.162us       1.838ms        24.68%       1.838ms      48.363us            38  \n",
      "sm75_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwc...         0.00%       0.000us         0.00%       0.000us       0.000us       1.028ms        13.81%       1.028ms     171.375us             6  \n",
      "turing_fp16_s1688gemm_fp16_256x128_ldg8_f2f_stages_3...         0.00%       0.000us         0.00%       0.000us       0.000us     833.882us        11.20%     833.882us      69.490us            12  \n",
      "          aten::_scaled_dot_product_efficient_attention         0.28%      85.926us         1.71%     523.836us      43.653us       0.000us         0.00%     790.811us      65.901us            12  \n",
      "                     aten::_efficient_attention_forward         0.34%     104.014us         0.97%     298.224us      24.852us     790.811us        10.62%     790.811us      65.901us            12  \n",
      "fmha_cutlassF_f16_aligned_64x64_rf_sm75(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us     790.811us        10.62%     790.811us      65.901us            12  \n",
      "         turing_fp16_s1688gemm_fp16_256x128_ldg8_f2f_tn         0.00%       0.000us         0.00%       0.000us       0.000us     643.130us         8.64%     643.130us      53.594us            12  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 30.693ms\n",
      "Self CUDA time total: 7.446ms\n",
      "\n",
      "Profiler run complete. Printing summary for Eager model...\n",
      "--------------------------------------------------\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::conv2d         0.53%      99.264us        19.88%       3.758ms      46.392us       0.000us         0.00%       4.454ms      54.992us            81  \n",
      "                                      aten::convolution         0.92%     174.378us        19.35%       3.658ms      45.166us       0.000us         0.00%       4.454ms      54.992us            81  \n",
      "                                     aten::_convolution         1.96%     371.207us        18.43%       3.484ms      43.014us       0.000us         0.00%       4.454ms      54.992us            81  \n",
      "                                aten::cudnn_convolution         9.44%       1.784ms        14.56%       2.753ms      33.984us       4.027ms        40.58%       4.027ms      49.715us            81  \n",
      "                                           aten::linear         1.06%     200.917us         9.84%       1.860ms      37.193us       0.000us         0.00%       2.404ms      48.087us            50  \n",
      "                                            aten::addmm         5.08%     959.463us         7.35%       1.389ms      27.786us       2.404ms        24.23%       2.404ms      48.087us            50  \n",
      "    turing_fp16_s1688gemm_fp16_256x128_ldg8_relu_f2f_tn         0.00%       0.000us         0.00%       0.000us       0.000us       1.309ms        13.19%       1.309ms      54.531us            24  \n",
      "    turing_fp16_s1688gemm_fp16_128x128_ldg8_relu_f2f_tn         0.00%       0.000us         0.00%       0.000us       0.000us       1.083ms        10.91%       1.083ms      41.649us            26  \n",
      "sm75_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwc...         0.00%       0.000us         0.00%       0.000us       0.000us       1.032ms        10.40%       1.032ms     172.074us             6  \n",
      "void cudnn::engines_precompiled::nchwToNhwcKernel<__...         0.00%       0.000us         0.00%       0.000us       0.000us     974.072us         9.82%     974.072us      15.968us            61  \n",
      "                     aten::scaled_dot_product_attention         0.40%      76.420us         2.94%     555.971us      46.331us       0.000us         0.00%     798.238us      66.520us            12  \n",
      "          aten::_scaled_dot_product_efficient_attention         0.67%     126.475us         2.54%     479.551us      39.963us       0.000us         0.00%     798.238us      66.520us            12  \n",
      "                     aten::_efficient_attention_forward         0.53%      99.530us         1.56%     294.756us      24.563us     798.238us         8.04%     798.238us      66.520us            12  \n",
      "fmha_cutlassF_f16_aligned_64x64_rf_sm75(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us     798.238us         8.04%     798.238us      66.520us            12  \n",
      "                                       aten::batch_norm         0.57%     107.460us        13.49%       2.550ms      49.042us       0.000us         0.00%     537.336us      10.333us            52  \n",
      "                           aten::_batch_norm_impl_index         0.78%     147.906us        12.92%       2.443ms      46.976us       0.000us         0.00%     537.336us      10.333us            52  \n",
      "                                aten::native_batch_norm         3.97%     750.847us        11.21%       2.119ms      40.741us     537.336us         5.41%     537.336us      10.333us            52  \n",
      "                              aten::upsample_bilinear2d         0.35%      65.504us         0.67%     127.540us      21.257us     480.635us         4.84%     480.635us      80.106us             6  \n",
      "void at::native::(anonymous namespace)::upsample_bil...         0.00%       0.000us         0.00%       0.000us       0.000us     480.635us         4.84%     480.635us      96.127us             5  \n",
      "                                       aten::group_norm         0.71%     134.127us        13.41%       2.535ms      48.753us       0.000us         0.00%     461.822us       8.881us            52  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 18.906ms\n",
      "Self CUDA time total: 9.923ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_16 = build_infer_fn(model, precision=\"fp16\")\n",
    "dummy_input = torch.randn(1, 3, RESOLUTION, RESOLUTION).to(DEVICE)\n",
    "\n",
    "for _ in range(5):\n",
    "    model_16(dummy_input.half())\n",
    "    compiled_model(dummy_input.half())\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA, # Only include if CUDA is available\n",
    "    ],\n",
    ") as prof:\n",
    "    with torch.no_grad():\n",
    "        compiled_model(dummy_input.half())\n",
    "print(\"Profiler run complete. Printing summary for Compiled Model...\")\n",
    "print(\"-\" * 50)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA, # Only include if CUDA is available\n",
    "    ],\n",
    ") as prof:\n",
    "    with torch.no_grad():\n",
    "        model_16(dummy_input.half())\n",
    "print(\"Profiler run complete. Printing summary for Eager model...\")\n",
    "print(\"-\" * 50)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27ce63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Monocular-Depth-ViT-Optimization (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
